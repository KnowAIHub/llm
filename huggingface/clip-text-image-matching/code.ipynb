{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLIP文本匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: ftfy in e:\\miniconda3\\lib\\site-packages (6.2.0)\n",
      "Requirement already satisfied: regex in e:\\miniconda3\\lib\\site-packages (2023.10.3)\n",
      "Requirement already satisfied: tqdm in e:\\miniconda3\\lib\\site-packages (4.65.0)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in e:\\miniconda3\\lib\\site-packages (from ftfy) (0.2.13)\n",
      "Requirement already satisfied: colorama in e:\\miniconda3\\lib\\site-packages (from tqdm) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at e:\\miniconda3\\lib\\site-packages\\whisper_live-0.0.11-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\wangrs\\appdata\\local\\temp\\pip-req-build-sjlmhcq0\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: ftfy in e:\\miniconda3\\lib\\site-packages (from clip==1.0) (6.2.0)\n",
      "Requirement already satisfied: packaging in e:\\miniconda3\\lib\\site-packages (from clip==1.0) (23.2)\n",
      "Requirement already satisfied: regex in e:\\miniconda3\\lib\\site-packages (from clip==1.0) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in e:\\miniconda3\\lib\\site-packages (from clip==1.0) (4.65.0)\n",
      "Requirement already satisfied: torch in e:\\miniconda3\\lib\\site-packages (from clip==1.0) (2.1.1+cu118)\n",
      "Requirement already satisfied: torchvision in e:\\miniconda3\\lib\\site-packages (from clip==1.0) (0.16.1)\n",
      "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in e:\\miniconda3\\lib\\site-packages (from ftfy->clip==1.0) (0.2.13)\n",
      "Requirement already satisfied: filelock in e:\\miniconda3\\lib\\site-packages (from torch->clip==1.0) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in e:\\miniconda3\\lib\\site-packages (from torch->clip==1.0) (4.12.2)\n",
      "Requirement already satisfied: sympy in e:\\miniconda3\\lib\\site-packages (from torch->clip==1.0) (1.12)\n",
      "Requirement already satisfied: networkx in e:\\miniconda3\\lib\\site-packages (from torch->clip==1.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in e:\\miniconda3\\lib\\site-packages (from torch->clip==1.0) (3.1.2)\n",
      "Requirement already satisfied: fsspec in e:\\miniconda3\\lib\\site-packages (from torch->clip==1.0) (2023.10.0)\n",
      "Requirement already satisfied: numpy in e:\\miniconda3\\lib\\site-packages (from torchvision->clip==1.0) (1.26.2)\n",
      "Requirement already satisfied: requests in e:\\miniconda3\\lib\\site-packages (from torchvision->clip==1.0) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in e:\\miniconda3\\lib\\site-packages (from torchvision->clip==1.0) (10.3.0)\n",
      "Requirement already satisfied: colorama in e:\\miniconda3\\lib\\site-packages (from tqdm->clip==1.0) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\miniconda3\\lib\\site-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\miniconda3\\lib\\site-packages (from requests->torchvision->clip==1.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\miniconda3\\lib\\site-packages (from requests->torchvision->clip==1.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\miniconda3\\lib\\site-packages (from requests->torchvision->clip==1.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\miniconda3\\lib\\site-packages (from requests->torchvision->clip==1.0) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in e:\\miniconda3\\lib\\site-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
      "Building wheels for collected packages: clip\n",
      "  Building wheel for clip (setup.py): started\n",
      "  Building wheel for clip (setup.py): finished with status 'done'\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369571 sha256=2fdc6b164bf9b9f2ae4c9e66a9667f753fdadc4a1c04579507a61b8eb0a0da12\n",
      "  Stored in directory: C:\\Users\\wangrs\\AppData\\Local\\Temp\\pip-ephem-wheel-cache-g5mvetbv\\wheels\\3f\\7c\\a4\\9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
      "Successfully built clip\n",
      "Installing collected packages: clip\n",
      "Successfully installed clip-1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at e:\\miniconda3\\lib\\site-packages\\whisper_live-0.0.11-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\wangrs\\AppData\\Local\\Temp\\pip-req-build-sjlmhcq0'\n"
     ]
    }
   ],
   "source": [
    "!pip install ftfy regex tqdm\n",
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [00:28<00:00, 12.5MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label probs: [[7.588e-05 8.225e-06 1.867e-03 9.980e-01]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "image = preprocess(Image.open(\"data/cat.png\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"a diagram\", \"a dog\", \"a cat\", \"five cats\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    \n",
    "    logits_per_image, logits_per_text = model(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：https://github.com/openai/CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e62f782569734b9aa2afea498d22d897",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/4.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\wangrs\\.cache\\huggingface\\hub\\models--openai--clip-vit-base-patch32. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66135c6967c4947a9a2fdb117c2512f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\miniconda3\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1761b1516040389b7747e5b535d37c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fe6c4658040436d83d8762837941e7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0babf00a7efb4db8882a140afcbb779e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/862k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82e351ca57144722aae048ce121147ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83f8a5ec86fe4d26b9cad8742765a3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.22M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b81ad33c29014a0e9a8172f072cd3214",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# URLs\n",
    "# url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Local\n",
    "image = Image.open(\"data/cat.png\")\n",
    "\n",
    "inputs = processor(text=[\"a photo of a cat\", \"a photo of a dog\", \"five cats\"], images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.6054e-02, 3.3200e-05, 9.8391e-01]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：https://huggingface.co/docs/transformers/model_doc/clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 中文CLIP文本匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: cn_clip in e:\\miniconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy in e:\\miniconda3\\lib\\site-packages (from cn_clip) (1.26.2)\n",
      "Requirement already satisfied: tqdm in e:\\miniconda3\\lib\\site-packages (from cn_clip) (4.65.0)\n",
      "Requirement already satisfied: six in e:\\miniconda3\\lib\\site-packages (from cn_clip) (1.16.0)\n",
      "Requirement already satisfied: timm in e:\\miniconda3\\lib\\site-packages (from cn_clip) (0.9.16)\n",
      "Requirement already satisfied: lmdb==1.3.0 in e:\\miniconda3\\lib\\site-packages (from cn_clip) (1.3.0)\n",
      "Requirement already satisfied: torch>=1.7.1 in e:\\miniconda3\\lib\\site-packages (from cn_clip) (2.1.1+cu118)\n",
      "Requirement already satisfied: torchvision in e:\\miniconda3\\lib\\site-packages (from cn_clip) (0.16.1)\n",
      "Requirement already satisfied: filelock in e:\\miniconda3\\lib\\site-packages (from torch>=1.7.1->cn_clip) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in e:\\miniconda3\\lib\\site-packages (from torch>=1.7.1->cn_clip) (4.12.2)\n",
      "Requirement already satisfied: sympy in e:\\miniconda3\\lib\\site-packages (from torch>=1.7.1->cn_clip) (1.12)\n",
      "Requirement already satisfied: networkx in e:\\miniconda3\\lib\\site-packages (from torch>=1.7.1->cn_clip) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in e:\\miniconda3\\lib\\site-packages (from torch>=1.7.1->cn_clip) (3.1.2)\n",
      "Requirement already satisfied: fsspec in e:\\miniconda3\\lib\\site-packages (from torch>=1.7.1->cn_clip) (2023.10.0)\n",
      "Requirement already satisfied: pyyaml in e:\\miniconda3\\lib\\site-packages (from timm->cn_clip) (6.0.1)\n",
      "Requirement already satisfied: huggingface_hub in e:\\miniconda3\\lib\\site-packages (from timm->cn_clip) (0.23.0)\n",
      "Requirement already satisfied: safetensors in e:\\miniconda3\\lib\\site-packages (from timm->cn_clip) (0.4.1)\n",
      "Requirement already satisfied: requests in e:\\miniconda3\\lib\\site-packages (from torchvision->cn_clip) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in e:\\miniconda3\\lib\\site-packages (from torchvision->cn_clip) (10.3.0)\n",
      "Requirement already satisfied: colorama in e:\\miniconda3\\lib\\site-packages (from tqdm->cn_clip) (0.4.6)\n",
      "Requirement already satisfied: packaging>=20.9 in e:\\miniconda3\\lib\\site-packages (from huggingface_hub->timm->cn_clip) (23.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\miniconda3\\lib\\site-packages (from jinja2->torch>=1.7.1->cn_clip) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in e:\\miniconda3\\lib\\site-packages (from requests->torchvision->cn_clip) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\miniconda3\\lib\\site-packages (from requests->torchvision->cn_clip) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\miniconda3\\lib\\site-packages (from requests->torchvision->cn_clip) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\miniconda3\\lib\\site-packages (from requests->torchvision->cn_clip) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in e:\\miniconda3\\lib\\site-packages (from sympy->torch>=1.7.1->cn_clip) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at e:\\miniconda3\\lib\\site-packages\\whisper_live-0.0.11-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!pip install cn_clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models: ['ViT-B-16', 'ViT-L-14', 'ViT-L-14-336', 'ViT-H-14', 'RN50']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 718M/718M [01:12<00:00, 10.4MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vision model config from e:\\miniconda3\\Lib\\site-packages\\cn_clip\\clip\\model_configs\\ViT-B-16.json\n",
      "Loading text model config from e:\\miniconda3\\Lib\\site-packages\\cn_clip\\clip\\model_configs\\RoBERTa-wwm-ext-base-chinese.json\n",
      "Model info {'embed_dim': 512, 'image_resolution': 224, 'vision_layers': 12, 'vision_width': 768, 'vision_patch_size': 16, 'vocab_size': 21128, 'text_attention_probs_dropout_prob': 0.1, 'text_hidden_act': 'gelu', 'text_hidden_dropout_prob': 0.1, 'text_hidden_size': 768, 'text_initializer_range': 0.02, 'text_intermediate_size': 3072, 'text_max_position_embeddings': 512, 'text_num_attention_heads': 12, 'text_num_hidden_layers': 12, 'text_type_vocab_size': 2}\n",
      "Label probs: [[7.272e-06 8.750e-05 1.967e-06 1.488e-04 9.995e-01]]\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "from PIL import Image\n",
    "\n",
    "import cn_clip.clip as clip\n",
    "from cn_clip.clip import load_from_name, available_models\n",
    "print(\"Available models:\", available_models())  \n",
    "# Available models: ['ViT-B-16', 'ViT-L-14', 'ViT-L-14-336', 'ViT-H-14', 'RN50']\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = load_from_name(\"ViT-B-16\", device=device, download_root='./')\n",
    "model.eval()\n",
    "image = preprocess(Image.open(\"data/cat.png\")).unsqueeze(0).to(device)\n",
    "text = clip.tokenize([\"杰尼龟\", \"妙蛙种子\", \"小火龙\", \"皮卡丘\", \"五只小猫\"]).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(text)\n",
    "    # 对特征进行归一化，请使用归一化后的图文特征用于下游任务\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True) \n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)    \n",
    "\n",
    "    logits_per_image, logits_per_text = model.get_similarity(image, text)\n",
    "    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "print(\"Label probs:\", probs)  # [[1.268734e-03 5.436878e-02 6.795761e-04 9.436829e-01]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：https://github.com/OFA-Sys/Chinese-CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a83c72c3172d4baeb628ee72d08e8454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/3.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\miniconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\wangrs\\.cache\\huggingface\\hub\\models--OFA-Sys--chinese-clip-vit-large-patch14. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c346fa2a5e742529b7d44f295910435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52d3ce2ca16c4b1992fa450f635fd23f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31bb9e4e518d460980125214e9fb2608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused or unrecognized kwargs: padding.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import ChineseCLIPProcessor, ChineseCLIPModel\n",
    "\n",
    "model = ChineseCLIPModel.from_pretrained(\"OFA-Sys/chinese-clip-vit-large-patch14\")\n",
    "processor = ChineseCLIPProcessor.from_pretrained(\"OFA-Sys/chinese-clip-vit-large-patch14\")\n",
    "\n",
    "# URL\n",
    "# url = \"https://clip-cn-beijing.oss-cn-beijing.aliyuncs.com/pokemon.jpeg\"\n",
    "# image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "# Local\n",
    "image = Image.open(\"data/cat.png\")\n",
    "# Squirtle, Bulbasaur, Charmander, Pikachu in English\n",
    "texts = [\"5只小猫\", \"杰尼龟\", \"妙蛙种子\", \"小火龙\", \"皮卡丘\"]\n",
    "\n",
    "# compute image feature\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "image_features = model.get_image_features(**inputs)\n",
    "image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)  # normalize\n",
    "\n",
    "# compute text features\n",
    "inputs = processor(text=texts, padding=True, return_tensors=\"pt\")\n",
    "text_features = model.get_text_features(**inputs)\n",
    "text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)  # normalize\n",
    "\n",
    "# compute image-text similarity scores\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n",
    "probs = logits_per_image.softmax(dim=1)  # probs: [[0.0066, 0.0211, 0.0031, 0.9692]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[9.9999e-01, 5.9417e-08, 1.4565e-06, 1.1806e-07, 1.0535e-05]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：https://huggingface.co/OFA-Sys/chinese-clip-vit-large-patch14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
