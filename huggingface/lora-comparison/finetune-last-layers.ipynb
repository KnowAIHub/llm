{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aa5ed53-af4d-41ee-8930-e08cd9f17734",
   "metadata": {},
   "source": [
    "### Loading the dataset into DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1851e942-ae94-4c11-8123-08ec5c051410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from local_dataset_utilities import download_dataset, load_dataset_into_to_dataframe, partition_dataset\n",
    "from local_dataset_utilities import IMDBDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c44ebad8-5e35-4a1b-bb15-4e632e105f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    print(\"Please switch to a GPU machine before running this notebook.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01b9be1f-c9e1-487c-ab61-2996e39b1dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = (\"test.csv\", \"train.csv\", \"val.csv\")\n",
    "download = True\n",
    "\n",
    "for f in files:\n",
    "    if not os.path.exists(os.path.join(\"data\", f)):\n",
    "        download = False\n",
    "\n",
    "if download is False:\n",
    "    download_dataset()\n",
    "    df = load_dataset_into_to_dataframe()\n",
    "    partition_dataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f22333db-90f9-4e15-8bba-7b30b9c8bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(\"data\", \"train.csv\"))\n",
    "df_val = pd.read_csv(os.path.join(\"data\", \"val.csv\"))\n",
    "df_test = pd.read_csv(os.path.join(\"data\", \"test.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5317aff6-d84e-4602-b485-518d2b94fcd7",
   "metadata": {},
   "source": [
    "### Tokenization and Numericalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "213c2b1a-9627-4269-84f8-a7a240063a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 35000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 5000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['index', 'text', 'label'],\n",
      "        num_rows: 10000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "imdb_dataset = load_dataset(\n",
    "    \"csv\",\n",
    "    data_files={\n",
    "        \"train\": os.path.join(\"data\", \"train.csv\"),\n",
    "        \"validation\": os.path.join(\"data\", \"val.csv\"),\n",
    "        \"test\": os.path.join(\"data\", \"test.csv\"),\n",
    "    },\n",
    ")\n",
    "\n",
    "print(imdb_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0febd78-a93c-488a-8092-a9681098ab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac8b2160-c78e-4153-a93c-82cf7327f0b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer input max length: 512\n",
      "Tokenizer vocabulary size: 30522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "print(\"Tokenizer input max length:\", tokenizer.model_max_length)\n",
    "print(\"Tokenizer vocabulary size:\", tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "429bacb8-b5a7-47b3-b074-9f39b67a3fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(batch):\n",
    "    return tokenizer(batch[\"text\"], truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f012eaa-52da-4247-ac81-8700b1df3c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681b748623894e2d8fba8eda976ba925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imdb_tokenized = imdb_dataset.map(tokenize_text, batched=True, batch_size=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beef12da-a008-441f-8c33-7bc2bd506fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "del imdb_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb17ae7e-c5fa-4373-9783-3606ad03c817",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_tokenized.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "761bd4d8-6fda-4658-ac46-e04e6cf76be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a26b0bd-d157-4ae5-b9da-33ae083d87c1",
   "metadata": {},
   "source": [
    "### Set Up DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74de21f9-96ca-4620-a299-236cf94cbf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, dataset_dict, partition_key=\"train\"):\n",
    "        self.partition = dataset_dict[partition_key]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.partition[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.partition.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dd38d9e-839a-4fce-8bc3-32a7e1423f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = IMDBDataset(imdb_tokenized, partition_key=\"train\")\n",
    "val_dataset = IMDBDataset(imdb_tokenized, partition_key=\"validation\")\n",
    "test_dataset = IMDBDataset(imdb_tokenized, partition_key=\"test\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=12,\n",
    "    shuffle=True, \n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=12,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=12,\n",
    "    num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60719ce4-f573-4bad-b2a7-f686c0e2381e",
   "metadata": {},
   "source": [
    "### Initializing DistilBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9ee2d8b-1680-4daf-aae3-e91d4fc28e1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7face7e-7af4-42ac-9f3d-d153bab027d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 66955010\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"Total number of trainable parameters:\", count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "646fd4a9-e29f-4353-960f-3425995257a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310482bb-a9e1-497a-85e2-24ffba7eb778",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter: {name}, requires_grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d830754f-55cc-4754-bcba-049908cc2240",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab36be-5a36-464e-9673-1c38f627e62f",
   "metadata": {},
   "source": [
    "Unfreeze last layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58d52db9-5121-4218-87e7-a47a187ec8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43b6ddc7-023f-42e3-b1da-3b5099815f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: distilbert.embeddings.word_embeddings.weight, requires_grad: False\n",
      "Parameter: distilbert.embeddings.position_embeddings.weight, requires_grad: False\n",
      "Parameter: distilbert.embeddings.LayerNorm.weight, requires_grad: False\n",
      "Parameter: distilbert.embeddings.LayerNorm.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.0.attention.q_lin.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.0.attention.q_lin.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.0.attention.k_lin.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.0.attention.k_lin.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.0.attention.v_lin.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.0.attention.v_lin.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.0.attention.out_lin.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.0.attention.out_lin.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.0.sa_layer_norm.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.0.sa_layer_norm.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.0.ffn.lin1.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.0.ffn.lin1.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.0.ffn.lin2.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.0.ffn.lin2.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.0.output_layer_norm.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.0.output_layer_norm.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.1.attention.q_lin.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.1.attention.q_lin.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.1.attention.k_lin.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.1.attention.k_lin.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.1.attention.v_lin.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.1.attention.v_lin.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.1.attention.out_lin.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.1.attention.out_lin.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.1.sa_layer_norm.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.1.sa_layer_norm.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.1.ffn.lin1.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.1.ffn.lin1.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.1.ffn.lin2.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.1.ffn.lin2.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.1.output_layer_norm.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.1.output_layer_norm.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.2.attention.q_lin.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.2.attention.q_lin.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.2.attention.k_lin.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.2.attention.k_lin.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.2.attention.v_lin.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.2.attention.v_lin.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.2.attention.out_lin.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.2.attention.out_lin.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.2.sa_layer_norm.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.2.sa_layer_norm.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.2.ffn.lin1.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.2.ffn.lin1.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.2.ffn.lin2.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.2.ffn.lin2.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.2.output_layer_norm.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.2.output_layer_norm.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.3.attention.q_lin.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.3.attention.q_lin.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.3.attention.k_lin.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.3.attention.k_lin.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.3.attention.v_lin.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.3.attention.v_lin.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.3.attention.out_lin.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.3.attention.out_lin.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.3.sa_layer_norm.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.3.sa_layer_norm.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.3.ffn.lin1.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.3.ffn.lin1.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.3.ffn.lin2.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.3.ffn.lin2.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.3.output_layer_norm.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.3.output_layer_norm.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.4.attention.q_lin.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.4.attention.q_lin.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.4.attention.k_lin.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.4.attention.k_lin.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.4.attention.v_lin.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.4.attention.v_lin.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.4.attention.out_lin.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.4.attention.out_lin.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.4.sa_layer_norm.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.4.sa_layer_norm.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.4.ffn.lin1.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.4.ffn.lin1.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.4.ffn.lin2.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.4.ffn.lin2.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.4.output_layer_norm.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.4.output_layer_norm.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.5.attention.q_lin.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.5.attention.q_lin.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.5.attention.k_lin.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.5.attention.k_lin.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.5.attention.v_lin.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.5.attention.v_lin.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.5.attention.out_lin.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.5.attention.out_lin.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.5.sa_layer_norm.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.5.sa_layer_norm.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.5.ffn.lin1.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.5.ffn.lin1.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.5.ffn.lin2.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.5.ffn.lin2.bias, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.5.output_layer_norm.weight, requires_grad: False\n",
      "Parameter: distilbert.transformer.layer.5.output_layer_norm.bias, requires_grad: False\n",
      "Parameter: pre_classifier.weight, requires_grad: False\n",
      "Parameter: pre_classifier.bias, requires_grad: False\n",
      "Parameter: classifier.weight, requires_grad: False\n",
      "Parameter: classifier.bias, requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Parameter: {name}, requires_grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5e54a3a-2391-491c-b2a1-f621d1caa0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.pre_classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "for param in model.classifier.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a0d2b66-b79a-4132-8f06-2afc594167b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of trainable parameters: 592130\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of trainable parameters:\", count_parameters(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ab943f48-88dc-4d3d-898e-3952447af17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter: pre_classifier.weight, requires_grad: True\n",
      "Parameter: pre_classifier.bias, requires_grad: True\n",
      "Parameter: classifier.weight, requires_grad: True\n",
      "Parameter: classifier.bias, requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if name in [\"pre_classifier.weight\", \"pre_classifier.bias\", \"classifier.weight\", \"classifier.bias\"]:\n",
    "        print(f\"Parameter: {name}, requires_grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfc314c-1eac-4ccf-b58a-a396309e2efb",
   "metadata": {},
   "source": [
    "### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c011a97-ce04-4851-9305-2113de7f0e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_model_utilities import CustomLightningModule\n",
    "\n",
    "lightning_model = CustomLightningModule(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "707e2c05-ca65-431d-9bdb-48f1a75712d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        save_top_k=1, mode=\"max\", monitor=\"val_acc\"\n",
    "    )  # save top 1 model\n",
    "]\n",
    "logger = CSVLogger(save_dir=\"logs/\", name=\"my-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95641825-02a2-4232-a41e-368ba78f58c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=3,\n",
    "    callbacks=callbacks,\n",
    "    accelerator=\"gpu\",\n",
    "    precision=\"16-mixed\",\n",
    "    devices=1,\n",
    "    logger=logger,\n",
    "    log_every_n_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f24ab5e3-af10-40dd-a7cd-950244d6b85f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type                                | Params | Mode \n",
      "-------------------------------------------------------------------------\n",
      "0 | model    | DistilBertForSequenceClassification | 67.0 M | eval \n",
      "1 | val_acc  | MulticlassAccuracy                  | 0      | train\n",
      "2 | test_acc | MulticlassAccuracy                  | 0      | train\n",
      "-------------------------------------------------------------------------\n",
      "592 K     Trainable params\n",
      "66.4 M    Non-trainable params\n",
      "67.0 M    Total params\n",
      "267.820   Total estimated model params size (MB)\n",
      "2         Modules in train mode\n",
      "96        Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4df68296c4c24a8fb4ef7856aaf32a43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed 4.20 min\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "trainer.fit(model=lightning_model,\n",
    "            train_dataloaders=train_loader,\n",
    "            val_dataloaders=val_loader)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "print(f\"Time elapsed {elapsed/60:.2f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b7dcc65-accf-4e5e-91d6-4150d92d5671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs/my-model/version_0/checkpoints/epoch=2-step=8751.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at logs/my-model/version_0/checkpoints/epoch=2-step=8751.ckpt\n",
      "/root/miniconda3/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:475: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d6e6d72708a43fd8179f5c288b376d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs/my-model/version_0/checkpoints/epoch=2-step=8751.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at logs/my-model/version_0/checkpoints/epoch=2-step=8751.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1790347f40f48cf93a6e64fe4a910aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Restoring states from the checkpoint path at logs/my-model/version_0/checkpoints/epoch=2-step=8751.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loaded model weights from the checkpoint at logs/my-model/version_0/checkpoints/epoch=2-step=8751.ckpt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27613f918ff94ab3a8c151122e5393a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_acc = trainer.test(lightning_model, dataloaders=train_loader, ckpt_path=\"best\", verbose=False)\n",
    "test_acc = trainer.test(lightning_model, dataloaders=test_loader, ckpt_path=\"best\", verbose=False)\n",
    "val_acc = trainer.test(lightning_model, dataloaders=val_loader, ckpt_path=\"best\", verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae1b59ae-1d02-446d-9c64-7dc7b5aa425a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 87.16%\n",
      "Val acc:   87.30%\n",
      "Test acc:  86.70%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train acc: {train_acc[0]['accuracy']*100:2.2f}%\")\n",
    "print(f\"Val acc:   {val_acc[0]['accuracy']*100:2.2f}%\")\n",
    "print(f\"Test acc:  {test_acc[0]['accuracy']*100:2.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5b0b9cf-7840-4b12-a8da-5ced1104ecdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Cleanup checkpoint files as we don't need them later\n",
    "log_dir = f\"logs/my-model\"\n",
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70d8ba8-ac14-4a79-a2c8-a08c970d2742",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
