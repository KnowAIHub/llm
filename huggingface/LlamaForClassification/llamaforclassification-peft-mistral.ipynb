{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"MAX_LEN = 512\nmistral_checkpoint = \"mistralai/Mistral-7B-v0.1\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-15T09:43:57.387440Z","iopub.execute_input":"2024-08-15T09:43:57.387810Z","iopub.status.idle":"2024-08-15T09:43:57.398951Z","shell.execute_reply.started":"2024-08-15T09:43:57.387780Z","shell.execute_reply":"2024-08-15T09:43:57.398137Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"wangrongsheng/twitter_disaster\")","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:44:21.797267Z","iopub.execute_input":"2024-08-15T09:44:21.797641Z","iopub.status.idle":"2024-08-15T09:44:24.488908Z","shell.execute_reply.started":"2024-08-15T09:44:21.797588Z","shell.execute_reply":"2024-08-15T09:44:24.488061Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/84.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79099bceec1d43a2a4c5cdc5aa8c0061"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/988k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9a842e418ca490da3e4f4a90a0d2fba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/427k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d961282917d54c20845e8166f71a590c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/7613 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"903bcfd7196340c0ab9dacc29fdc58dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3263 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1d952e33f5440789f9eab16c751abb4"}},"metadata":{}}]},{"cell_type":"code","source":"from datasets import Dataset\n# Split the dataset into training and validation datasets\ndata = dataset['train'].train_test_split(train_size=0.8, seed=42)\n# Rename the default \"test\" split to \"validation\"\ndata['val'] = data.pop(\"test\")\n# Convert the test dataframe to HuggingFace dataset and add it into the first dataset\ndata['test'] = dataset['test']","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:44:24.653378Z","iopub.execute_input":"2024-08-15T09:44:24.653727Z","iopub.status.idle":"2024-08-15T09:44:24.671526Z","shell.execute_reply.started":"2024-08-15T09:44:24.653700Z","shell.execute_reply":"2024-08-15T09:44:24.670747Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndata['train'].to_pandas().info()\ndata['test'].to_pandas().info()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:44:25.065399Z","iopub.execute_input":"2024-08-15T09:44:25.065792Z","iopub.status.idle":"2024-08-15T09:44:25.141756Z","shell.execute_reply.started":"2024-08-15T09:44:25.065762Z","shell.execute_reply":"2024-08-15T09:44:25.140791Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 6090 entries, 0 to 6089\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        6090 non-null   int64 \n 1   keyword   6037 non-null   object\n 2   location  4064 non-null   object\n 3   text      6090 non-null   object\n 4   target    6090 non-null   int64 \ndtypes: int64(2), object(3)\nmemory usage: 238.0+ KB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3263 entries, 0 to 3262\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        3263 non-null   int64 \n 1   keyword   3237 non-null   object\n 2   location  2158 non-null   object\n 3   text      3263 non-null   object\n 4   target    3263 non-null   int64 \ndtypes: int64(2), object(3)\nmemory usage: 127.6+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"pos_weights = len(data['train'].to_pandas()) / (2 * data['train'].to_pandas().target.value_counts()[1])\nneg_weights = len(data['train'].to_pandas()) / (2 * data['train'].to_pandas().target.value_counts()[0])","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:44:25.468887Z","iopub.execute_input":"2024-08-15T09:44:25.469584Z","iopub.status.idle":"2024-08-15T09:44:25.613925Z","shell.execute_reply.started":"2024-08-15T09:44:25.469555Z","shell.execute_reply":"2024-08-15T09:44:25.613086Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Number of Characters\nmax_char = data['train'].to_pandas()['text'].str.len().max()\n# Number of Words\nmax_words = data['train'].to_pandas()['text'].str.split().str.len().max()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:44:33.533236Z","iopub.execute_input":"2024-08-15T09:44:33.533937Z","iopub.status.idle":"2024-08-15T09:44:33.634612Z","shell.execute_reply.started":"2024-08-15T09:44:33.533907Z","shell.execute_reply":"2024-08-15T09:44:33.633805Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"!huggingface-cli login --token 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","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:48:38.167656Z","iopub.execute_input":"2024-08-15T09:48:38.168506Z","iopub.status.idle":"2024-08-15T09:48:39.872216Z","shell.execute_reply.started":"2024-08-15T09:48:38.168472Z","shell.execute_reply":"2024-08-15T09:48:39.871039Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n","output_type":"stream"},{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load Mistral 7B Tokenizer\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\nmistral_tokenizer = AutoTokenizer.from_pretrained(mistral_checkpoint, add_prefix_space=True)\nmistral_tokenizer.pad_token_id = mistral_tokenizer.eos_token_id\nmistral_tokenizer.pad_token = mistral_tokenizer.eos_token\n\ncol_to_delete = ['id', 'keyword','location', 'text']\n\ndef mistral_preprocessing_function(examples):\n    return mistral_tokenizer(examples['text'], truncation=True, max_length=MAX_LEN)\n\nmistral_tokenized_datasets = data.map(mistral_preprocessing_function, batched=True, remove_columns=col_to_delete)\nmistral_tokenized_datasets = mistral_tokenized_datasets.rename_column(\"target\", \"label\")\nmistral_tokenized_datasets.set_format(\"torch\")\n\n# Data collator for padding a batch of examples to the maximum length seen in the batch\nmistral_data_collator = DataCollatorWithPadding(tokenizer=mistral_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:48:42.729230Z","iopub.execute_input":"2024-08-15T09:48:42.730130Z","iopub.status.idle":"2024-08-15T09:48:45.198718Z","shell.execute_reply.started":"2024-08-15T09:48:42.730084Z","shell.execute_reply":"2024-08-15T09:48:45.197739Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/996 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"430a7faed0fb42e684fcbb81c18f98b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b5edab7aa4a4c6799136dde44f75f2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ac9e47f305d4a64a2ec572327a9818a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d574dce47b9348249d818ddb064a190f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6090 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e6023c6e64f42a4b4471b5621f25304"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1523 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a5ee145ddf849718beeb7ab91fef4c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3263 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4dd0ea0ad24473b9816fb727992deb1"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nimport torch\nmistral_model =  AutoModelForSequenceClassification.from_pretrained(\n  pretrained_model_name_or_path=mistral_checkpoint,\n  num_labels=2,\n  device_map=\"auto\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:48:46.993217Z","iopub.execute_input":"2024-08-15T09:48:46.993575Z","iopub.status.idle":"2024-08-15T09:50:13.658959Z","shell.execute_reply.started":"2024-08-15T09:48:46.993546Z","shell.execute_reply":"2024-08-15T09:50:13.658160Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa6fdb8f975a49ff979de643f08b3744"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc21a84563284551949ef2d614a98547"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a226b8b4119442618b8494dcc17f6532"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b3a2fa57ab8447ba94e6eb7edcb81e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5e61a1879f14d8fa9691e75525dd052"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54f63d17fc824230af178beebb9b2786"}},"metadata":{}},{"name":"stderr","text":"Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at mistralai/Mistral-7B-v0.1 and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"对于 Mistral 7B，我们必须添加填充令牌 id，因为默认情况下未定义它。","metadata":{}},{"cell_type":"code","source":"mistral_model.config.pad_token_id = mistral_model.config.eos_token_id","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:50:21.504337Z","iopub.execute_input":"2024-08-15T09:50:21.505508Z","iopub.status.idle":"2024-08-15T09:50:21.509459Z","shell.execute_reply.started":"2024-08-15T09:50:21.505474Z","shell.execute_reply":"2024-08-15T09:50:21.508613Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"!pip install evaluate peft -q","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:50:28.732460Z","iopub.execute_input":"2024-08-15T09:50:28.732836Z","iopub.status.idle":"2024-08-15T09:50:44.577418Z","shell.execute_reply.started":"2024-08-15T09:50:28.732806Z","shell.execute_reply":"2024-08-15T09:50:44.576194Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"from peft import get_peft_model, LoraConfig, TaskType\n\nmistral_peft_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS, r=2, lora_alpha=16, lora_dropout=0.1, bias=\"none\", \n    target_modules=[\n        \"q_proj\",\n        \"v_proj\",\n    ],\n)\n\nmistral_model = get_peft_model(mistral_model, mistral_peft_config)\nmistral_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:50:51.458909Z","iopub.execute_input":"2024-08-15T09:50:51.459538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import evaluate\nimport numpy as np\n\ndef compute_metrics(eval_pred):\n    # All metrics are already predefined in the HF `evaluate` package\n    precision_metric = evaluate.load(\"precision\")\n    recall_metric = evaluate.load(\"recall\")\n    f1_metric= evaluate.load(\"f1\")\n    accuracy_metric = evaluate.load(\"accuracy\")\n\n    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n    predictions = np.argmax(logits, axis=-1)\n    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n    # The trainer is expecting a dictionary where the keys are the metrics names and the values are the scores. \n    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\n\nclass WeightedCELossTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        # Get model's predictions\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        # Compute custom loss\n        loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor([neg_weights, pos_weights], device=model.device, dtype=logits.dtype))\n        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\nmistral_model = mistral_model.cuda()\n\nlr = 1e-4\nbatch_size = 8\nnum_epochs = 5\n\ntraining_args = TrainingArguments(\n    output_dir=\"mistral-lora-token-classification\",\n    learning_rate=lr,\n    lr_scheduler_type= \"constant\",\n    warmup_ratio= 0.1,\n    max_grad_norm= 0.3,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=num_epochs,\n    weight_decay=0.001,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    report_to=\"wandb\",\n    fp16=True,\n    gradient_checkpointing=True,\n)\n\n\nmistral_trainer = WeightedCELossTrainer(\n    model=mistral_model,\n    args=training_args,\n    train_dataset=mistral_tokenized_datasets['train'],\n    eval_dataset=mistral_tokenized_datasets[\"val\"],\n    data_collator=mistral_data_collator,\n    compute_metrics=compute_metrics\n)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mistral_trainer.train()","metadata":{},"execution_count":null,"outputs":[]}]}