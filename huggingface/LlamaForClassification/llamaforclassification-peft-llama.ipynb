{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"MAX_LEN = 512\nllama_checkpoint = \"NousResearch/Llama-2-7b-hf\" # \"meta-llama/Llama-2-7b-hf\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-15T09:37:48.510476Z","iopub.execute_input":"2024-08-15T09:37:48.511102Z","iopub.status.idle":"2024-08-15T09:37:48.524418Z","shell.execute_reply.started":"2024-08-15T09:37:48.511024Z","shell.execute_reply":"2024-08-15T09:37:48.523107Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"wangrongsheng/twitter_disaster\")","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:37:48.529751Z","iopub.execute_input":"2024-08-15T09:37:48.536752Z","iopub.status.idle":"2024-08-15T09:37:53.434401Z","shell.execute_reply.started":"2024-08-15T09:37:48.536703Z","shell.execute_reply":"2024-08-15T09:37:53.433155Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/84.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f2fa464feda4d24a9fdb4d0374234b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/988k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43c30a9c27f8494b8f8007201dbec7bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/427k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5cb173485e54402b6d7a92fe62c6c9d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/7613 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0777e9b46db74695a1003c1bae354b17"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3263 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42ba4304a1d44ed9914852bb4167adb5"}},"metadata":{}}]},{"cell_type":"code","source":"from datasets import Dataset\n# Split the dataset into training and validation datasets\ndata = dataset['train'].train_test_split(train_size=0.8, seed=42)\n# Rename the default \"test\" split to \"validation\"\ndata['val'] = data.pop(\"test\")\n# Convert the test dataframe to HuggingFace dataset and add it into the first dataset\ndata['test'] = dataset['test']","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:37:53.435971Z","iopub.execute_input":"2024-08-15T09:37:53.436768Z","iopub.status.idle":"2024-08-15T09:37:53.456790Z","shell.execute_reply.started":"2024-08-15T09:37:53.436733Z","shell.execute_reply":"2024-08-15T09:37:53.455727Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ndata['train'].to_pandas().info()\ndata['test'].to_pandas().info()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:37:53.458286Z","iopub.execute_input":"2024-08-15T09:37:53.458912Z","iopub.status.idle":"2024-08-15T09:37:53.542789Z","shell.execute_reply.started":"2024-08-15T09:37:53.458880Z","shell.execute_reply":"2024-08-15T09:37:53.541854Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 6090 entries, 0 to 6089\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        6090 non-null   int64 \n 1   keyword   6037 non-null   object\n 2   location  4064 non-null   object\n 3   text      6090 non-null   object\n 4   target    6090 non-null   int64 \ndtypes: int64(2), object(3)\nmemory usage: 238.0+ KB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3263 entries, 0 to 3262\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        3263 non-null   int64 \n 1   keyword   3237 non-null   object\n 2   location  2158 non-null   object\n 3   text      3263 non-null   object\n 4   target    3263 non-null   int64 \ndtypes: int64(2), object(3)\nmemory usage: 127.6+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"pos_weights = len(data['train'].to_pandas()) / (2 * data['train'].to_pandas().target.value_counts()[1])\nneg_weights = len(data['train'].to_pandas()) / (2 * data['train'].to_pandas().target.value_counts()[0])","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:37:53.545886Z","iopub.execute_input":"2024-08-15T09:37:53.546271Z","iopub.status.idle":"2024-08-15T09:37:53.691468Z","shell.execute_reply.started":"2024-08-15T09:37:53.546244Z","shell.execute_reply":"2024-08-15T09:37:53.690657Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Number of Characters\nmax_char = data['train'].to_pandas()['text'].str.len().max()\n# Number of Words\nmax_words = data['train'].to_pandas()['text'].str.split().str.len().max()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:37:53.692539Z","iopub.execute_input":"2024-08-15T09:37:53.692843Z","iopub.status.idle":"2024-08-15T09:37:53.784338Z","shell.execute_reply.started":"2024-08-15T09:37:53.692817Z","shell.execute_reply":"2024-08-15T09:37:53.783584Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"data['train'][0]","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:37:53.785296Z","iopub.execute_input":"2024-08-15T09:37:53.785549Z","iopub.status.idle":"2024-08-15T09:37:53.794203Z","shell.execute_reply.started":"2024-08-15T09:37:53.785527Z","shell.execute_reply":"2024-08-15T09:37:53.793284Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"{'id': 5285,\n 'keyword': 'fear',\n 'location': 'Thibodaux, LA',\n 'text': 'my worst fear. https://t.co/iH8UDz8mq3',\n 'target': 0}"},"metadata":{}}]},{"cell_type":"code","source":"# Load Llama 2 Tokenizer\nfrom transformers import AutoTokenizer, DataCollatorWithPadding\nllama_tokenizer = AutoTokenizer.from_pretrained(llama_checkpoint, add_prefix_space=True)\nllama_tokenizer.pad_token_id = llama_tokenizer.eos_token_id\nllama_tokenizer.pad_token = llama_tokenizer.eos_token\n\ncol_to_delete = ['id', 'keyword','location', 'text']\n\ndef llama_preprocessing_function(examples):\n    return llama_tokenizer(examples['text'], truncation=True, max_length=MAX_LEN)\n\nllama_tokenized_datasets = data.map(llama_preprocessing_function, batched=True, remove_columns=col_to_delete)\nllama_tokenized_datasets = llama_tokenized_datasets.rename_column(\"target\", \"label\")\nllama_tokenized_datasets.set_format(\"torch\")\n\n# Data collator for padding a batch of examples to the maximum length seen in the batch\nllama_data_collator = DataCollatorWithPadding(tokenizer=llama_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:37:53.795345Z","iopub.execute_input":"2024-08-15T09:37:53.795658Z","iopub.status.idle":"2024-08-15T09:38:12.175560Z","shell.execute_reply.started":"2024-08-15T09:37:53.795635Z","shell.execute_reply":"2024-08-15T09:38:12.174618Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"2024-08-15 09:38:00.009777: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-15 09:38:00.009879: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-15 09:38:00.147380: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/746 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a742ff68ed3e4e43a0be051c04b889e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"882199994f474e5083b371c1a8dd8ba2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7aed5fc3fff4126ba8a8eba61a6b464"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af6623a540084c22b2ff151aed84fef8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6090 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7a82faa64f14b5a82b3e3fb12a77db2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1523 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac42442068cc4fca8712ffde1aac139a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3263 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"357142f0b61f47398b234ee608082081"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\nimport torch\nllama_model =  AutoModelForSequenceClassification.from_pretrained(\n  pretrained_model_name_or_path=llama_checkpoint,\n  num_labels=2,\n  device_map=\"auto\",\n  offload_folder=\"offload\",\n  trust_remote_code=True,\n  #torch_dtype=torch.float16\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:38:12.176859Z","iopub.execute_input":"2024-08-15T09:38:12.177445Z","iopub.status.idle":"2024-08-15T09:39:11.986657Z","shell.execute_reply.started":"2024-08-15T09:38:12.177407Z","shell.execute_reply":"2024-08-15T09:39:11.985694Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/583 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ea01267d0c944fdb398ba6a9532228c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fd540bfe1b448ee957d74cd39cb653b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f4c2b1fd98b4cd6a4726c8c652cea0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d307185b327a4164bf83fa52486b7daf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c676ce01c2924dac88858c825f9a3421"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a1adb7babe94918b3f90be31ff98a01"}},"metadata":{}},{"name":"stderr","text":"Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at NousResearch/Llama-2-7b-hf and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"对于 Llama 2，我们必须添加填充标记 id，因为默认情况下未定义它：","metadata":{}},{"cell_type":"code","source":"llama_model.config.pad_token_id = llama_model.config.eos_token_id","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:39:11.987861Z","iopub.execute_input":"2024-08-15T09:39:11.988201Z","iopub.status.idle":"2024-08-15T09:39:11.994455Z","shell.execute_reply.started":"2024-08-15T09:39:11.988177Z","shell.execute_reply":"2024-08-15T09:39:11.993616Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"!pip install peft -q","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:39:11.995588Z","iopub.execute_input":"2024-08-15T09:39:11.995923Z","iopub.status.idle":"2024-08-15T09:39:39.245520Z","shell.execute_reply.started":"2024-08-15T09:39:11.995891Z","shell.execute_reply":"2024-08-15T09:39:39.244361Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"from peft import get_peft_model, LoraConfig, TaskType\n\nllama_peft_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS, r=16, lora_alpha=16, lora_dropout=0.05, bias=\"none\", \n    target_modules=[\n        \"q_proj\",\n        \"v_proj\",  \n    ],\n)\n\nllama_model = get_peft_model(llama_model, llama_peft_config)\nllama_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:39:39.247695Z","iopub.execute_input":"2024-08-15T09:39:39.248112Z","iopub.status.idle":"2024-08-15T09:39:39.592845Z","shell.execute_reply.started":"2024-08-15T09:39:39.248073Z","shell.execute_reply":"2024-08-15T09:39:39.591863Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"trainable params: 8,396,800 || all params: 6,615,748,608 || trainable%: 0.1269\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install evaluate -q","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:39:39.594223Z","iopub.execute_input":"2024-08-15T09:39:39.594624Z","iopub.status.idle":"2024-08-15T09:39:52.614132Z","shell.execute_reply.started":"2024-08-15T09:39:39.594591Z","shell.execute_reply":"2024-08-15T09:39:52.612873Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\nimport numpy as np\n\ndef compute_metrics(eval_pred):\n    # All metrics are already predefined in the HF `evaluate` package\n    precision_metric = evaluate.load(\"precision\")\n    recall_metric = evaluate.load(\"recall\")\n    f1_metric= evaluate.load(\"f1\")\n    accuracy_metric = evaluate.load(\"accuracy\")\n\n    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n    predictions = np.argmax(logits, axis=-1)\n    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n    # The trainer is expecting a dictionary where the keys are the metrics names and the values are the scores. \n    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:39:52.618278Z","iopub.execute_input":"2024-08-15T09:39:52.618635Z","iopub.status.idle":"2024-08-15T09:39:53.653736Z","shell.execute_reply.started":"2024-08-15T09:39:52.618605Z","shell.execute_reply":"2024-08-15T09:39:53.652916Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from transformers import Trainer\n\nclass WeightedCELossTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        # Get model's predictions\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        # Compute custom loss\n        loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor([neg_weights, pos_weights], device=model.device, dtype=logits.dtype))\n        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:39:53.654841Z","iopub.execute_input":"2024-08-15T09:39:53.655157Z","iopub.status.idle":"2024-08-15T09:39:54.209202Z","shell.execute_reply.started":"2024-08-15T09:39:53.655131Z","shell.execute_reply":"2024-08-15T09:39:54.208371Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer\n\nllama_model = llama_model.cuda()\n\nlr = 1e-4\nbatch_size = 1\nnum_epochs = 3\n\ntraining_args = TrainingArguments(\n    output_dir=\"llama-lora-token-classification\",\n    learning_rate=lr,\n    lr_scheduler_type= \"constant\",\n    warmup_ratio= 0.1,\n    max_grad_norm= 0.3,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=num_epochs,\n    weight_decay=0.001,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    report_to=\"wandb\",\n    fp16=True,\n    gradient_checkpointing=True,\n)\n\n\n\nllama_trainer = WeightedCELossTrainer(\n    model=llama_model,\n    args=training_args,\n    train_dataset=llama_tokenized_datasets['train'],\n    eval_dataset=llama_tokenized_datasets[\"val\"],\n    data_collator=llama_data_collator,\n    compute_metrics=compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:39:54.210394Z","iopub.execute_input":"2024-08-15T09:39:54.210739Z","iopub.status.idle":"2024-08-15T09:39:54.967894Z","shell.execute_reply.started":"2024-08-15T09:39:54.210709Z","shell.execute_reply":"2024-08-15T09:39:54.967077Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"llama_trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T09:39:54.968987Z","iopub.execute_input":"2024-08-15T09:39:54.969288Z","iopub.status.idle":"2024-08-15T09:40:24.931678Z","shell.execute_reply.started":"2024-08-15T09:39:54.969264Z","shell.execute_reply":"2024-08-15T09:40:24.930086Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.17.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.17.4"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240815_094005-h4la56zq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/wangrongsheng/huggingface/runs/h4la56zq' target=\"_blank\">llama-lora-token-classification</a></strong> to <a href='https://wandb.ai/wangrongsheng/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/wangrongsheng/huggingface' target=\"_blank\">https://wandb.ai/wangrongsheng/huggingface</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/wangrongsheng/huggingface/runs/h4la56zq' target=\"_blank\">https://wandb.ai/wangrongsheng/huggingface/runs/h4la56zq</a>"},"metadata":{}},{"name":"stderr","text":"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllama_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2314\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2309\u001b[0m     _grad_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[1;32m   2310\u001b[0m         amp\u001b[38;5;241m.\u001b[39mmaster_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer),\n\u001b[1;32m   2311\u001b[0m         args\u001b[38;5;241m.\u001b[39mmax_grad_norm,\n\u001b[1;32m   2312\u001b[0m     )\n\u001b[1;32m   2313\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2314\u001b[0m     _grad_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2315\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2316\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2317\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2320\u001b[0m     is_accelerate_available()\n\u001b[1;32m   2321\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[1;32m   2322\u001b[0m ):\n\u001b[1;32m   2323\u001b[0m     grad_norm \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_global_grad_norm()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2295\u001b[0m, in \u001b[0;36mAccelerator.clip_grad_norm_\u001b[0;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[1;32m   2293\u001b[0m             \u001b[38;5;66;03m# Set is_xla_gradients_synced to True to avoid all-reduce twice in the AcceleratedOptimizer step.\u001b[39;00m\n\u001b[1;32m   2294\u001b[0m             acc_opt\u001b[38;5;241m.\u001b[39mgradient_state\u001b[38;5;241m.\u001b[39mis_xla_gradients_synced \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m-> 2295\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(parameters, max_norm, norm_type\u001b[38;5;241m=\u001b[39mnorm_type)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:2245\u001b[0m, in \u001b[0;36mAccelerator.unscale_gradients\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m   2243\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(opt, AcceleratedOptimizer):\n\u001b[1;32m   2244\u001b[0m     opt \u001b[38;5;241m=\u001b[39m opt\u001b[38;5;241m.\u001b[39moptimizer\n\u001b[0;32m-> 2245\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munscale_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:307\u001b[0m, in \u001b[0;36mGradScaler.unscale_\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    304\u001b[0m inv_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdouble()\u001b[38;5;241m.\u001b[39mreciprocal()\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m    305\u001b[0m found_inf \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfull((), \u001b[38;5;241m0.0\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 307\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf_per_device\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_unscale_grads_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minv_scale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    309\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m optimizer_state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m OptState\u001b[38;5;241m.\u001b[39mUNSCALED\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/cuda/amp/grad_scaler.py:229\u001b[0m, in \u001b[0;36mGradScaler._unscale_grads_\u001b[0;34m(self, optimizer, inv_scale, found_inf, allow_fp16)\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m allow_fp16) \u001b[38;5;129;01mand\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[0;32m--> 229\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to unscale FP16 gradients.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mis_sparse:\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;66;03m# is_coalesced() == False means the sparse grad has values with duplicate indices.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m     \u001b[38;5;66;03m# coalesce() deduplicates indices and adds all values that have the same index.\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# For scaled fp16 values, there's a good chance coalescing will cause overflow,\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# so we should check the coalesced _values().\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m param\u001b[38;5;241m.\u001b[39mgrad\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfloat16:\n","\u001b[0;31mValueError\u001b[0m: Attempting to unscale FP16 gradients."],"ename":"ValueError","evalue":"Attempting to unscale FP16 gradients.","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}