{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!nvidia-smi","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-15T04:36:43.921238Z","iopub.execute_input":"2024-08-15T04:36:43.922046Z","iopub.status.idle":"2024-08-15T04:36:44.982790Z","shell.execute_reply.started":"2024-08-15T04:36:43.922014Z","shell.execute_reply":"2024-08-15T04:36:44.981660Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Thu Aug 15 04:36:44 2024       \n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 550.90.07              Driver Version: 550.90.07      CUDA Version: 12.4     |\n|-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  Tesla P100-PCIE-16GB           Off |   00000000:00:04.0 Off |                    0 |\n| N/A   37C    P0             27W /  250W |       0MiB /  16384MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n                                                                                         \n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"markdown","source":"大型语言模型以其文本生成能力而闻名。在预训练期间，它们接受了数百万个tokens的训练。这将有助于大型语言模型理解英文文本并在生成期间生成有意义的完整标记。自然语言处理中的另一个常见任务是序列分类任务。在此，我们将给定的序列分类为不同的类别。这可以通过 prompt 使用大型语言模型简单地完成，但这可能只是有时有效。我们可以调整大型语言模型以针对给定的输入为每个类别输出一组概率。本指南将展示如何训练此类 LLM 并使用微调的 Llama 3 模型。","metadata":{}},{"cell_type":"markdown","source":"下载必要的库，我们需要这些库来微调Llama 3进行序列分类。让我们运行以下代码：","metadata":{}},{"cell_type":"code","source":"!pip install -q transformers accelerate trl bitsandbytes datasets evaluate\n!pip install -q peft scikit-learn","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:37:08.427169Z","iopub.execute_input":"2024-08-15T04:37:08.427603Z","iopub.status.idle":"2024-08-15T04:37:39.610478Z","shell.execute_reply.started":"2024-08-15T04:37:08.427575Z","shell.execute_reply":"2024-08-15T04:37:39.609263Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!huggingface-cli login --token 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","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:37:43.794918Z","iopub.execute_input":"2024-08-15T04:37:43.795677Z","iopub.status.idle":"2024-08-15T04:37:45.483510Z","shell.execute_reply.started":"2024-08-15T04:37:43.795642Z","shell.execute_reply":"2024-08-15T04:37:45.482581Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}]},{"cell_type":"markdown","source":"接下来，我们加载数据集进行训练。为此，我们使用以下代码：","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\n# 下载数据：https://huggingface.co/datasets/fancyzhx/ag_news\ndataset = load_dataset(\"ag_news\")","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:37:56.422970Z","iopub.execute_input":"2024-08-15T04:37:56.423460Z","iopub.status.idle":"2024-08-15T04:38:05.744122Z","shell.execute_reply.started":"2024-08-15T04:37:56.423421Z","shell.execute_reply":"2024-08-15T04:38:05.743302Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/8.07k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7d35cd102014a4f9f1027c270b51afe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/18.6M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b96bdaa437ad442eac9c061074127114"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.23M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"851a018aaf61490ebfc9f1bff859a408"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/120000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"733dc81d353a476691e1c7be78e72935"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/7600 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0bf7811e2f574b2f9503b1dc0ceefd59"}},"metadata":{}}]},{"cell_type":"markdown","source":"这是一个新闻分类数据集。新闻分为不同的类别，例如世界、体育、商业和科学/技术。现在，让我们看看每个类别的示例数量是否相等，或者是否存在类别不平衡。","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.DataFrame(dataset['train'])\n\ndf.label.value_counts(normalize=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:38:17.014621Z","iopub.execute_input":"2024-08-15T04:38:17.015185Z","iopub.status.idle":"2024-08-15T04:38:21.392143Z","shell.execute_reply.started":"2024-08-15T04:38:17.015154Z","shell.execute_reply":"2024-08-15T04:38:21.391093Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"label\n2    0.25\n3    0.25\n1    0.25\n0    0.25\nName: proportion, dtype: float64"},"metadata":{}}]},{"cell_type":"markdown","source":"运行此代码会产生以下输出。我们可以检查所有 4 个标签是否具有相同的比例，这意味着每个类别在数据集中都有相同数量的示例。数据集很大，所以我们只需要其中的一部分。因此，我们使用以下代码从此数据框中抽取一些数据：","metadata":{}},{"cell_type":"code","source":"# Splitting the dataframe into 4 separate dataframes based on the labels\nlabel_1_df = df[df['label'] == 0]\nlabel_2_df = df[df['label'] == 1]\nlabel_3_df = df[df['label'] == 2]\nlabel_4_df = df[df['label'] == 3]\n\n# Shuffle each label dataframe\nlabel_1_df = label_1_df.sample(frac=1).reset_index(drop=True)\nlabel_2_df = label_2_df.sample(frac=1).reset_index(drop=True)\nlabel_3_df = label_3_df.sample(frac=1).reset_index(drop=True)\nlabel_4_df = label_4_df.sample(frac=1).reset_index(drop=True)\n\n# Splitting each label dataframe into train, test, and validation sets\nlabel_1_train = label_1_df.iloc[:2000]\nlabel_1_test = label_1_df.iloc[2000:2500]\nlabel_1_val = label_1_df.iloc[2500:3000]\n\nlabel_2_train = label_2_df.iloc[:2000]\nlabel_2_test = label_2_df.iloc[2000:2500]\nlabel_2_val = label_2_df.iloc[2500:3000]\n\nlabel_3_train = label_3_df.iloc[:2000]\nlabel_3_test = label_3_df.iloc[2000:2500]\nlabel_3_val = label_3_df.iloc[2500:3000]\n\nlabel_4_train = label_4_df.iloc[:2000]\nlabel_4_test = label_4_df.iloc[2000:2500]\nlabel_4_val = label_4_df.iloc[2500:3000]\n\n# Concatenating the splits back together\ntrain_df = pd.concat([label_1_train, label_2_train, label_3_train, label_4_train])\ntest_df = pd.concat([label_1_test, label_2_test, label_3_test, label_4_test])\nval_df = pd.concat([label_1_val, label_2_val, label_3_val, label_4_val])\n\n# Shuffle the dataframes to ensure randomness\ntrain_df = train_df.sample(frac=1).reset_index(drop=True)\ntest_df = test_df.sample(frac=1).reset_index(drop=True)\nval_df = val_df.sample(frac=1).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:38:36.746366Z","iopub.execute_input":"2024-08-15T04:38:36.746751Z","iopub.status.idle":"2024-08-15T04:38:36.793756Z","shell.execute_reply.started":"2024-08-15T04:38:36.746724Z","shell.execute_reply":"2024-08-15T04:38:36.792650Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"我们的数据包含 4 个标签。因此，我们创建 4 个数据框，每个数据框由一个标签组成。然后，我们通过调用 sample 函数来打乱每个标签，然后，我们将每个标签数据框分成 3 个部分，分别称为训练、测试和有效数据框。对于训练，我们提供 2000 个标签，对于测试和验证，我们各提供 500 个标签。现在，我们通过pd.concat() 函数将所有标签的训练数据框合并为一个训练数据框，我们对测试和验证数据框也做了同样的事情。最后，我们再次对 train_df、test_df 和 valid_df 进行混洗，以确保它们的随机性。","metadata":{}},{"cell_type":"markdown","source":"为了确认，让我们检查训练数据框中每个标签的值计数。代码如下：","metadata":{}},{"cell_type":"code","source":"train_df.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:38:48.673728Z","iopub.execute_input":"2024-08-15T04:38:48.674742Z","iopub.status.idle":"2024-08-15T04:38:48.682123Z","shell.execute_reply.started":"2024-08-15T04:38:48.674705Z","shell.execute_reply":"2024-08-15T04:38:48.681115Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"label\n2    2000\n0    2000\n3    2000\n1    2000\nName: count, dtype: int64"},"metadata":{}}]},{"cell_type":"markdown","source":"我们可以检查训练数据框是否对四个标签都具有相同的示例。在将它们发送到训练之前，我们需要将这些 Pandas DataFrames 转换为 HuggingFace 训练库接受的 DatasetDict。为此，我们使用以下代码：","metadata":{}},{"cell_type":"code","source":"from datasets import DatasetDict, Dataset\n\n# Converting pandas DataFrames into Hugging Face Dataset objects:\ndataset_train = Dataset.from_pandas(train_df)\ndataset_val = Dataset.from_pandas(val_df)\ndataset_test = Dataset.from_pandas(test_df)\n\n# Combine them into a single DatasetDict\ndataset = DatasetDict({\n    'train': dataset_train,\n    'val': dataset_val,\n    'test': dataset_test\n})\n\ndataset","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:39:03.523602Z","iopub.execute_input":"2024-08-15T04:39:03.523997Z","iopub.status.idle":"2024-08-15T04:39:03.567833Z","shell.execute_reply.started":"2024-08-15T04:39:03.523967Z","shell.execute_reply":"2024-08-15T04:39:03.566881Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 8000\n    })\n    val: Dataset({\n        features: ['text', 'label'],\n        num_rows: 2000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 2000\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"dataset['train'][0]","metadata":{"execution":{"iopub.status.busy":"2024-08-15T06:55:40.897914Z","iopub.execute_input":"2024-08-15T06:55:40.898734Z","iopub.status.idle":"2024-08-15T06:55:40.905257Z","shell.execute_reply.started":"2024-08-15T06:55:40.898702Z","shell.execute_reply":"2024-08-15T06:55:40.904376Z"},"trusted":true},"execution_count":40,"outputs":[{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"{'text': 'A Windfall for a Student Loan Program The student loan industry is collecting a record amount of the old subsidies Congress thought it had retired.',\n 'label': 2}"},"metadata":{}}]},{"cell_type":"markdown","source":"从输出中我们可以看到，DatasetDict 包含 3 个数据集，分别是训练、测试和验证数据集。其中每个数据集仅包含 2 列：一列是文本，另一列是标签。\n\n在这里，在我们的数据集中，每个类别的比例是相同的。在实际情况中，这可能只是有时是正确的。因此，当类别不平衡时，我们需要采取适当的措施，以便 LLM 不会更加重视包含更多示例的标签。为此，我们计算类别权重。\n\n类别权重告诉我们必须赋予每个类别多大的重要性；类别权重越大，该类别的重要性就越大。如果我们的数据集不平衡，我们可以为标签提供更多的类别权重，使用更少的示例，从而赋予它更大的重要性。为了获得这些类别权重，我们可以取数据集中类别标签（值计数）比例的倒数。此代码如下：","metadata":{}},{"cell_type":"code","source":"import torch\n\nclass_weights=(1/train_df.label.value_counts(normalize=True).sort_index()).tolist()\nclass_weights=torch.tensor(class_weights)\nclass_weights=class_weights/class_weights.sum()\n\nclass_weights","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:39:17.231665Z","iopub.execute_input":"2024-08-15T04:39:17.232040Z","iopub.status.idle":"2024-08-15T04:39:19.229060Z","shell.execute_reply.started":"2024-08-15T04:39:17.232011Z","shell.execute_reply":"2024-08-15T04:39:19.227893Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"tensor([0.2500, 0.2500, 0.2500, 0.2500])"},"metadata":{}}]},{"cell_type":"markdown","source":"从输出中我们可以看到，所有类别的类别权重都是相等的；这是因为所有类别都有相同数量的示例。","metadata":{}},{"cell_type":"markdown","source":"我们将下载并准备模型进行训练。首先是下载模型。我们无法使用完整模型，因为我们正在处理一个小型 GPU；因此，我们将对其进行量化。此代码如下：","metadata":{}},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig, AutoModelForSequenceClassification\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit = True, \n    bnb_4bit_quant_type = 'nf4',\n    bnb_4bit_use_double_quant = True, \n    bnb_4bit_compute_dtype = torch.bfloat16 \n)\n\nmodel_name = \"NousResearch/Meta-Llama-3-8B\" # \"meta-llama/Meta-Llama-3-8B\"\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    quantization_config=quantization_config,\n    num_labels=4,\n    device_map='auto'\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:41:37.052936Z","iopub.execute_input":"2024-08-15T04:41:37.053726Z","iopub.status.idle":"2024-08-15T04:49:24.323599Z","shell.execute_reply.started":"2024-08-15T04:41:37.053692Z","shell.execute_reply":"2024-08-15T04:49:24.322556Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c0ed71da52c46abaad40c60057cd030"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dd445078e2241249a454fdd4de47308"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e6d9a16ad1440cfbc8fb7a54ae7506a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6277a33cd7fc4b15992735938d9f41b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42df290bc7f8473c894ae969a1ef0813"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d505c647732944c4b602ad568f90d152"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"667df59beb134e9dbe89f1d6b364d73f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df3676e1fd2748d4b8d5322e05af6ab8"}},"metadata":{}},{"name":"stderr","text":"Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at NousResearch/Meta-Llama-3-8B and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"因此，运行上述代码将从 HuggingFace 中心下载 Llama 3 8B 大型语言模型，根据我们为其提供的 quantization_config 对其进行量化，然后将 LLM 的输出头替换为具有 4 个神经元的线性头作为输出，并将模型推送到 GPU。接下来，我们将为模型创建一个 LoRA 配置，以仅训练一部分参数。此代码如下：","metadata":{}},{"cell_type":"code","source":"from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n\nlora_config = LoraConfig(\n    r = 16, \n    lora_alpha = 8,\n    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n    lora_dropout = 0.05, \n    bias = 'none',\n    task_type = 'SEQ_CLS'\n)\n\n# 选择合适的target_modules：https://github.com/huggingface/peft/blob/main/src/peft/utils/constants.py#L78\n\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:55:14.425343Z","iopub.execute_input":"2024-08-15T04:55:14.426410Z","iopub.status.idle":"2024-08-15T04:55:14.900412Z","shell.execute_reply.started":"2024-08-15T04:55:14.426362Z","shell.execute_reply":"2024-08-15T04:55:14.899509Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"运行此程序后，get_peft_model 将采用模型并通过包装模型和 LoRA 配置来准备使用PEFT 方法（如本例中的LoRA）进行训练。","metadata":{}},{"cell_type":"markdown","source":"我们将在模型训练之前在测试数据上测试 Llama 3 模型。为此，我们将首先下载标记器。此代码如下：","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_name = \"NousResearch/Meta-Llama-3-8B\" # \"meta-llama/Meta-Llama-3-8B\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n\ntokenizer.pad_token_id = tokenizer.eos_token_id\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:55:56.284803Z","iopub.execute_input":"2024-08-15T04:55:56.285533Z","iopub.status.idle":"2024-08-15T04:55:58.189671Z","shell.execute_reply.started":"2024-08-15T04:55:56.285499Z","shell.execute_reply":"2024-08-15T04:55:58.188596Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/50.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"494b758f15954ca18a33c87e2d617996"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9c12e4effed4998bd31e2e6b3191c42"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/73.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"942b576376c44b87bf4b63ad364a1c99"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"model.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:56:47.014748Z","iopub.execute_input":"2024-08-15T04:56:47.015545Z","iopub.status.idle":"2024-08-15T04:56:47.020508Z","shell.execute_reply.started":"2024-08-15T04:56:47.015508Z","shell.execute_reply":"2024-08-15T04:56:47.019510Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"接下来，我们甚至通过将模型的 pad token ID 设置为 tokenizer 的 pad token ID 来编辑模型配置，并且不使用缓存。现在，我们将测试数据提供给模型并收集输出：","metadata":{}},{"cell_type":"code","source":"sentences = test_df.text.tolist()\nbatch_size = 16  \nall_outputs = []\n\nfor i in range(0, len(sentences), batch_size):\n    batch_sentences = sentences[i:i + batch_size]\n\n    inputs = tokenizer(batch_sentences, return_tensors=\"pt\", \n    padding=True, truncation=True, max_length=512)\n\n    inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n        all_outputs.append(outputs['logits'])\n        \nfinal_outputs = torch.cat(all_outputs, dim=0)\ntest_df['predictions']=final_outputs.argmax(axis=1).cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T04:57:05.911352Z","iopub.execute_input":"2024-08-15T04:57:05.911715Z","iopub.status.idle":"2024-08-15T05:03:22.346163Z","shell.execute_reply.started":"2024-08-15T04:57:05.911686Z","shell.execute_reply":"2024-08-15T05:03:22.345224Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"2024-08-15 04:57:11.476773: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-15 04:57:11.476896: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-15 04:57:11.603893: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"运行此代码会将模型结果存储在一个变量中。我们将这些预测添加到新列中的测试 DataFrame 中。我们取每个输出的 argmax；这为我们提供了 final_outputs 列表中每个输出概率最高的标签。现在，我们需要评估 LLM 生成的输出，我们可以通过以下代码进行评估：","metadata":{}},{"cell_type":"markdown","source":"运行该程序产生了以下结果。我们可以看到，我们的准确率为 0.21，这非常低。该模型的准确率、召回率和 f1 分数也非常低；它们甚至没有达到 50% 以上的百分比。在训练模型后对它们进行测试将让我们了解模型的训练效果。","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import balanced_accuracy_score, classification_report\n\ndef get_metrics_result(test_df):\n    y_test = test_df.label\n    y_pred = test_df.predictions\n\n    print(\"Classification Report:\")\n    print(classification_report(y_test, y_pred))\n\n    print(\"Balanced Accuracy Score:\", balanced_accuracy_score(y_test, y_pred))\n    print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))\n\nget_metrics_result(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T05:03:56.641658Z","iopub.execute_input":"2024-08-15T05:03:56.642403Z","iopub.status.idle":"2024-08-15T05:03:57.349544Z","shell.execute_reply.started":"2024-08-15T05:03:56.642368Z","shell.execute_reply":"2024-08-15T05:03:57.348487Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.37      0.24      0.29       500\n           1       0.38      0.07      0.12       500\n           2       0.18      0.31      0.22       500\n           3       0.18      0.26      0.21       500\n\n    accuracy                           0.22      2000\n   macro avg       0.28      0.22      0.21      2000\nweighted avg       0.28      0.22      0.21      2000\n\nBalanced Accuracy Score: 0.2185\nAccuracy Score: 0.2185\n","output_type":"stream"}]},{"cell_type":"markdown","source":"在开始训练之前，我们需要对数据进行预处理，然后再将其发送到模型。为此，我们使用以下代码：","metadata":{}},{"cell_type":"code","source":"def data_preprocesing(row):\n    return tokenizer(row['text'], truncation=True, max_length=512)\n\ntokenized_data = dataset.map(data_preprocesing, batched=True, \nremove_columns=['text'])\ntokenized_data.set_format(\"torch\")","metadata":{"execution":{"iopub.status.busy":"2024-08-15T05:04:51.762590Z","iopub.execute_input":"2024-08-15T05:04:51.763267Z","iopub.status.idle":"2024-08-15T05:04:53.778695Z","shell.execute_reply.started":"2024-08-15T05:04:51.763238Z","shell.execute_reply":"2024-08-15T05:04:53.777692Z"},"trusted":true},"execution_count":18,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/8000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ce9904e3f0c472a88677c17684b53cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce967de7135149cd8f0898737121d248"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6fcb4b9cbbca4cf6a7c9d0507d704c3b"}},"metadata":{}}]},{"cell_type":"markdown","source":"现在，datasetdict 中的每个数据集都包含三个特征/列，即标签、input_id 和 Attention_masks。使用上述预处理函数为每个文本生成 input_ids 和 Attention_masks。在训练时，我们需要一个数据整理器来批量处理数据。为此，我们使用以下代码：","metadata":{}},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\ncollate_fn = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T05:05:03.784210Z","iopub.execute_input":"2024-08-15T05:05:03.784597Z","iopub.status.idle":"2024-08-15T05:05:03.809108Z","shell.execute_reply.started":"2024-08-15T05:05:03.784568Z","shell.execute_reply":"2024-08-15T05:05:03.808319Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"这将确保批处理中的所有输入具有相同的长度，这对于加快训练速度至关重要。因此，我们使用特殊标记（如填充标记）将输入统一填充到最长序列长度，从而允许同时进行批处理。","metadata":{}},{"cell_type":"markdown","source":"在开始训练之前，我们需要一个误差度量来评估它。大型语言模型的默认误差度量是负对数似然损失。但在这里，因为我们正在修改 LLM 以使其成为序列分类工具，所以我们需要重新定义在训练时测试模型所需的误差度量：","metadata":{}},{"cell_type":"code","source":"def compute_metrics(evaluations):\n    predictions, labels = evaluations\n    predictions = np.argmax(predictions, axis=1)\n    return {'balanced_accuracy' : balanced_accuracy_score(predictions, labels),\n    'accuracy':accuracy_score(predictions,labels)}","metadata":{"execution":{"iopub.status.busy":"2024-08-15T05:05:17.336515Z","iopub.execute_input":"2024-08-15T05:05:17.337134Z","iopub.status.idle":"2024-08-15T05:05:17.342267Z","shell.execute_reply.started":"2024-08-15T05:05:17.337095Z","shell.execute_reply":"2024-08-15T05:05:17.341358Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"因为我们要使用自定义指标，所以我们甚至定义了一个自定义训练器来训练我们的 LLM，这是必要的，因为我们在这里使用类权重。为此，代码将是","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\n\nclass CustomTrainer(Trainer):\n    def __init__(self, *args, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        if class_weights is not None:\n            self.class_weights = torch.tensor(class_weights, \n            dtype=torch.float32).to(self.args.device)\n        else:\n            self.class_weights = None\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\").long()\n\n        outputs = model(**inputs)\n\n        logits = outputs.get('logits')\n\n        if self.class_weights is not None:\n            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n        else:\n            loss = F.cross_entropy(logits, labels)\n\n        return (loss, outputs) if return_outputs else loss","metadata":{"execution":{"iopub.status.busy":"2024-08-15T05:06:13.633247Z","iopub.execute_input":"2024-08-15T05:06:13.634207Z","iopub.status.idle":"2024-08-15T05:06:14.178482Z","shell.execute_reply.started":"2024-08-15T05:06:13.634152Z","shell.execute_reply":"2024-08-15T05:06:14.177209Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"现在，我们将定义我们的训练参数。代码如下：","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\ntraining_args = TrainingArguments(\n    output_dir = 'sentiment_classification',\n    learning_rate = 1e-4,\n    per_device_train_batch_size = 8,\n    per_device_eval_batch_size = 8,\n    num_train_epochs = 1,\n    logging_steps=1,\n    weight_decay = 0.01,\n    evaluation_strategy = 'epoch',\n    save_strategy = 'epoch',\n    load_best_model_at_end = True,\n    report_to=\"none\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T05:08:09.058337Z","iopub.execute_input":"2024-08-15T05:08:09.059202Z","iopub.status.idle":"2024-08-15T05:08:09.094738Z","shell.execute_reply.started":"2024-08-15T05:08:09.059172Z","shell.execute_reply":"2024-08-15T05:08:09.093791Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"这将创建我们的 TrainingArguments 对象。现在，我们准备将其传递给我们创建的 Trainer。此代码如下：","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\nimport numpy as np\n\ntrainer = CustomTrainer(\n    model = model,\n    args = training_args,\n    train_dataset = tokenized_data['train'],\n    eval_dataset = tokenized_data['val'],\n    tokenizer = tokenizer,\n    data_collator = collate_fn,\n    compute_metrics = compute_metrics,\n    class_weights=class_weights,\n)\n\ntrain_result = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T05:10:37.016600Z","iopub.execute_input":"2024-08-15T05:10:37.016998Z","iopub.status.idle":"2024-08-15T06:36:58.085674Z","shell.execute_reply.started":"2024-08-15T05:10:37.016970Z","shell.execute_reply":"2024-08-15T06:36:58.084078Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/4108302796.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  self.class_weights = torch.tensor(class_weights,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1001' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 1:19:57, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [250/250 06:15]\n    </div>\n    "},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[29], line 14\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[1;32m      3\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[1;32m      4\u001b[0m     model \u001b[38;5;241m=\u001b[39m model,\n\u001b[1;32m      5\u001b[0m     args \u001b[38;5;241m=\u001b[39m training_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     class_weights\u001b[38;5;241m=\u001b[39mclass_weights,\n\u001b[1;32m     12\u001b[0m )\n\u001b[0;32m---> 14\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1932\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1930\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1932\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1933\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1934\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1935\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1936\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1937\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2365\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2362\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_training_stop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2364\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_epoch_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m-> 2365\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DebugOption\u001b[38;5;241m.\u001b[39mTPU_METRICS_DEBUG \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdebug:\n\u001b[1;32m   2368\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   2369\u001b[0m         \u001b[38;5;66;03m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2793\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2791\u001b[0m metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_evaluate:\n\u001b[0;32m-> 2793\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2795\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m   2796\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_checkpoint(model, trial, metrics\u001b[38;5;241m=\u001b[39mmetrics)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2750\u001b[0m, in \u001b[0;36mTrainer._evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   2749\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_evaluate\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, ignore_keys_for_eval, skip_scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m-> 2750\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2751\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[1;32m   2753\u001b[0m     \u001b[38;5;66;03m# Run delayed LR scheduler now that metrics are populated\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3641\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3638\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m   3640\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[0;32m-> 3641\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3642\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3643\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3644\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[1;32m   3645\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[1;32m   3646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   3647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3649\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3651\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3923\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3919\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_metrics(\n\u001b[1;32m   3920\u001b[0m             EvalPrediction(predictions\u001b[38;5;241m=\u001b[39mall_preds, label_ids\u001b[38;5;241m=\u001b[39mall_labels, inputs\u001b[38;5;241m=\u001b[39mall_inputs)\n\u001b[1;32m   3921\u001b[0m         )\n\u001b[1;32m   3922\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3923\u001b[0m         metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEvalPrediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_preds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3924\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m metrics \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3925\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m {}\n","Cell \u001b[0;32mIn[20], line 3\u001b[0m, in \u001b[0;36mcompute_metrics\u001b[0;34m(evaluations)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_metrics\u001b[39m(evaluations):\n\u001b[1;32m      2\u001b[0m     predictions, labels \u001b[38;5;241m=\u001b[39m evaluations\n\u001b[0;32m----> 3\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39margmax(predictions, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m : balanced_accuracy_score(predictions, labels),\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m:accuracy_score(predictions,labels)}\n","\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"],"ename":"NameError","evalue":"name 'np' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"现在，让我们尝试进行评估，在测试数据上测试新训练的模型：","metadata":{}},{"cell_type":"code","source":"def generate_predictions(model,df_test):\n    sentences = df_test.text.tolist()\n    batch_size = 32  \n    all_outputs = []\n\n    for i in range(0, len(sentences), batch_size):\n\n        batch_sentences = sentences[i:i + batch_size]\n\n        inputs = tokenizer(batch_sentences, return_tensors=\"pt\", \n        padding=True, truncation=True, max_length=512)\n\n        inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') \n        for k, v in inputs.items()}\n\n        with torch.no_grad():\n            outputs = model(**inputs)\n            all_outputs.append(outputs['logits'])\n        \n    final_outputs = torch.cat(all_outputs, dim=0)\n    df_test['predictions']=final_outputs.argmax(axis=1).cpu().numpy()\n\ngenerate_predictions(model,test_df)\n#get_performance_metrics(test_df)\nget_metrics_result(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T06:46:01.496237Z","iopub.execute_input":"2024-08-15T06:46:01.497251Z","iopub.status.idle":"2024-08-15T06:52:23.734085Z","shell.execute_reply.started":"2024-08-15T06:46:01.497215Z","shell.execute_reply":"2024-08-15T06:52:23.733091Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.97      0.92      0.94       500\n           1       0.98      1.00      0.99       500\n           2       0.92      0.89      0.90       500\n           3       0.88      0.94      0.91       500\n\n    accuracy                           0.94      2000\n   macro avg       0.94      0.94      0.94      2000\nweighted avg       0.94      0.94      0.94      2000\n\nBalanced Accuracy Score: 0.935\nAccuracy Score: 0.935\n","output_type":"stream"}]},{"cell_type":"code","source":"ls","metadata":{"execution":{"iopub.status.busy":"2024-08-15T06:52:43.882469Z","iopub.execute_input":"2024-08-15T06:52:43.883295Z","iopub.status.idle":"2024-08-15T06:52:44.933615Z","shell.execute_reply.started":"2024-08-15T06:52:43.883262Z","shell.execute_reply":"2024-08-15T06:52:44.932575Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[0m\u001b[01;34msentiment_classification\u001b[0m/\n","output_type":"stream"}]},{"cell_type":"code","source":"cd sentiment_classification/","metadata":{"execution":{"iopub.status.busy":"2024-08-15T06:52:55.796586Z","iopub.execute_input":"2024-08-15T06:52:55.797346Z","iopub.status.idle":"2024-08-15T06:52:55.803478Z","shell.execute_reply.started":"2024-08-15T06:52:55.797289Z","shell.execute_reply":"2024-08-15T06:52:55.802525Z"},"trusted":true},"execution_count":33,"outputs":[{"name":"stdout","text":"/kaggle/working/sentiment_classification\n","output_type":"stream"}]},{"cell_type":"code","source":"ls","metadata":{"execution":{"iopub.status.busy":"2024-08-15T06:52:58.113077Z","iopub.execute_input":"2024-08-15T06:52:58.113483Z","iopub.status.idle":"2024-08-15T06:52:59.149697Z","shell.execute_reply.started":"2024-08-15T06:52:58.113450Z","shell.execute_reply":"2024-08-15T06:52:59.148568Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[0m\u001b[01;34mcheckpoint-1000\u001b[0m/\n","output_type":"stream"}]},{"cell_type":"code","source":"cd checkpoint-1000","metadata":{"execution":{"iopub.status.busy":"2024-08-15T06:53:13.444138Z","iopub.execute_input":"2024-08-15T06:53:13.444888Z","iopub.status.idle":"2024-08-15T06:53:13.452619Z","shell.execute_reply.started":"2024-08-15T06:53:13.444852Z","shell.execute_reply":"2024-08-15T06:53:13.451674Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"/kaggle/working/sentiment_classification/checkpoint-1000\n","output_type":"stream"}]},{"cell_type":"code","source":"ls","metadata":{"execution":{"iopub.status.busy":"2024-08-15T06:53:15.596471Z","iopub.execute_input":"2024-08-15T06:53:15.597250Z","iopub.status.idle":"2024-08-15T06:53:16.632326Z","shell.execute_reply.started":"2024-08-15T06:53:15.597217Z","shell.execute_reply":"2024-08-15T06:53:16.631197Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"README.md                  rng_state.pth            tokenizer_config.json\nadapter_config.json        scheduler.pt             trainer_state.json\nadapter_model.safetensors  special_tokens_map.json  training_args.bin\noptimizer.pt               tokenizer.json\n","output_type":"stream"}]},{"cell_type":"code","source":"ll","metadata":{"execution":{"iopub.status.busy":"2024-08-15T06:53:38.131635Z","iopub.execute_input":"2024-08-15T06:53:38.132580Z","iopub.status.idle":"2024-08-15T06:53:39.161063Z","shell.execute_reply.started":"2024-08-15T06:53:38.132536Z","shell.execute_reply":"2024-08-15T06:53:39.160025Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"total 169308\n-rw-r--r-- 1 root      5102 Aug 15 06:30 README.md\n-rw-r--r-- 1 root       710 Aug 15 06:30 adapter_config.json\n-rw-r--r-- 1 root  54626008 Aug 15 06:30 adapter_model.safetensors\n-rw-r--r-- 1 root 109399354 Aug 15 06:30 optimizer.pt\n-rw-r--r-- 1 root     14244 Aug 15 06:30 rng_state.pth\n-rw-r--r-- 1 root      1064 Aug 15 06:30 scheduler.pt\n-rw-r--r-- 1 root       335 Aug 15 06:30 special_tokens_map.json\n-rw-r--r-- 1 root   9085796 Aug 15 06:30 tokenizer.json\n-rw-r--r-- 1 root     50628 Aug 15 06:30 tokenizer_config.json\n-rw-r--r-- 1 root    151864 Aug 15 06:30 trainer_state.json\n-rw-r--r-- 1 root      5112 Aug 15 06:30 training_args.bin\n","output_type":"stream"}]},{"cell_type":"markdown","source":"本文参考：https://www.analyticsvidhya.com/blog/2024/06/finetuning-llama-3-for-sequence-classification/","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}