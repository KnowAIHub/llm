{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"在快速发展的自然语言处理 (NLP) 领域，我们经常会比较不同的语言模型，看看哪一种最适合特定任务。这篇博文主要是关于比较三种模型：RoBERTa、Mistral-7b 和 Llama-2-7b。我们用它们来解决一个常见问题——对有关灾难的推文进行分类。值得注意的是，Mistral 和 Llama 2 是具有 70 亿个参数的大型模型。相比之下，RoBERTa-large（355M 参数）是一个相对较小的模型，用作比较研究的基线。\n\n在这篇博客中，我们使用 PEFT（参数高效微调）技术：LoRA（大型语言模型的低秩适应）来微调序列分类任务上的预训练模型。 LoRa 旨在显着减少可训练参数的数量，同时保持强大的下游任务性能。\n\nRoBERTa 的最大序列长度限制为 512：","metadata":{}},{"cell_type":"code","source":"MAX_LEN = 512 \nroberta_checkpoint = \"roberta-large\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-15T08:34:47.946908Z","iopub.execute_input":"2024-08-15T08:34:47.947598Z","iopub.status.idle":"2024-08-15T08:34:47.958407Z","shell.execute_reply.started":"2024-08-15T08:34:47.947565Z","shell.execute_reply":"2024-08-15T08:34:47.957108Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"我们将从 Hugging Face 加载数据集：","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset(\"wangrongsheng/twitter_disaster\")","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:35:00.242928Z","iopub.execute_input":"2024-08-15T08:35:00.243816Z","iopub.status.idle":"2024-08-15T08:35:04.838839Z","shell.execute_reply.started":"2024-08-15T08:35:00.243779Z","shell.execute_reply":"2024-08-15T08:35:04.838096Z"},"trusted":true},"execution_count":2,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/84.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4bab1c136c0456c87ff90d3eed2f505"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/988k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c89e0b6d7f794995b7245d6a69a96392"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/427k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f502d469b064e54afca294b3da6fee8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/7613 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeebdc6d41f64cb1b8dbb3d892e34f6b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3263 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4d48701afb349dfb91a073eb21bf716"}},"metadata":{}}]},{"cell_type":"markdown","source":"现在，让我们将数据集分为训练数据集和验证数据集。然后添加测试集：","metadata":{}},{"cell_type":"code","source":"from datasets import Dataset\n# Split the dataset into training and validation datasets\ndata = dataset['train'].train_test_split(train_size=0.8, seed=42)\n# Rename the default \"test\" split to \"validation\"\ndata['val'] = data.pop(\"test\")\n# Convert the test dataframe to HuggingFace dataset and add it into the first dataset\ndata['test'] = dataset['test']","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:35:18.021481Z","iopub.execute_input":"2024-08-15T08:35:18.022356Z","iopub.status.idle":"2024-08-15T08:35:18.041165Z","shell.execute_reply.started":"2024-08-15T08:35:18.022325Z","shell.execute_reply":"2024-08-15T08:35:18.040062Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"我们来检查一下数据分布：","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\ndata['train'].to_pandas().info()\ndata['test'].to_pandas().info()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:35:30.630999Z","iopub.execute_input":"2024-08-15T08:35:30.631970Z","iopub.status.idle":"2024-08-15T08:35:30.706849Z","shell.execute_reply.started":"2024-08-15T08:35:30.631935Z","shell.execute_reply":"2024-08-15T08:35:30.705888Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 6090 entries, 0 to 6089\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        6090 non-null   int64 \n 1   keyword   6037 non-null   object\n 2   location  4064 non-null   object\n 3   text      6090 non-null   object\n 4   target    6090 non-null   int64 \ndtypes: int64(2), object(3)\nmemory usage: 238.0+ KB\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 3263 entries, 0 to 3262\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype \n---  ------    --------------  ----- \n 0   id        3263 non-null   int64 \n 1   keyword   3237 non-null   object\n 2   location  2158 non-null   object\n 3   text      3263 non-null   object\n 4   target    3263 non-null   int64 \ndtypes: int64(2), object(3)\nmemory usage: 127.6+ KB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"由于类别不平衡，我们将计算正负权重，并将其用于稍后的损失计算：","metadata":{}},{"cell_type":"code","source":"pos_weights = len(data['train'].to_pandas()) / (2 * data['train'].to_pandas().target.value_counts()[1])\nneg_weights = len(data['train'].to_pandas()) / (2 * data['train'].to_pandas().target.value_counts()[0])","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:36:37.942909Z","iopub.execute_input":"2024-08-15T08:36:37.943555Z","iopub.status.idle":"2024-08-15T08:36:38.085309Z","shell.execute_reply.started":"2024-08-15T08:36:37.943522Z","shell.execute_reply":"2024-08-15T08:36:38.084400Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"pos_weights, neg_weights","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:36:51.111672Z","iopub.execute_input":"2024-08-15T08:36:51.112025Z","iopub.status.idle":"2024-08-15T08:36:51.118350Z","shell.execute_reply.started":"2024-08-15T08:36:51.111999Z","shell.execute_reply":"2024-08-15T08:36:51.117369Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(1.1622137404580153, 0.877521613832853)"},"metadata":{}}]},{"cell_type":"markdown","source":"然后，我们计算列文本的最大长度：","metadata":{}},{"cell_type":"code","source":"# Number of Characters\nmax_char = data['train'].to_pandas()['text'].str.len().max()\n# Number of Words\nmax_words = data['train'].to_pandas()['text'].str.split().str.len().max()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:36:58.414844Z","iopub.execute_input":"2024-08-15T08:36:58.415940Z","iopub.status.idle":"2024-08-15T08:36:58.512710Z","shell.execute_reply.started":"2024-08-15T08:36:58.415898Z","shell.execute_reply":"2024-08-15T08:36:58.511953Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"max_char, max_words","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:37:13.623764Z","iopub.execute_input":"2024-08-15T08:37:13.624374Z","iopub.status.idle":"2024-08-15T08:37:13.630193Z","shell.execute_reply.started":"2024-08-15T08:37:13.624343Z","shell.execute_reply":"2024-08-15T08:37:13.629285Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"(152, 31)"},"metadata":{}}]},{"cell_type":"markdown","source":"让我们看一下训练数据的一行示例：","metadata":{}},{"cell_type":"code","source":"data['train'][0]","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:37:32.254270Z","iopub.execute_input":"2024-08-15T08:37:32.254630Z","iopub.status.idle":"2024-08-15T08:37:32.262237Z","shell.execute_reply.started":"2024-08-15T08:37:32.254595Z","shell.execute_reply":"2024-08-15T08:37:32.261271Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"{'id': 5285,\n 'keyword': 'fear',\n 'location': 'Thibodaux, LA',\n 'text': 'my worst fear. https://t.co/iH8UDz8mq3',\n 'target': 0}"},"metadata":{}}]},{"cell_type":"markdown","source":"该数据包括关键字、位置和推文文本。为了简单起见，我们选择text特征作为唯一的输入LLM。","metadata":{}},{"cell_type":"markdown","source":"定义 RoBERTa 数据加载器：","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\nroberta_tokenizer = AutoTokenizer.from_pretrained(roberta_checkpoint, add_prefix_space=True)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:37:51.928952Z","iopub.execute_input":"2024-08-15T08:37:51.929736Z","iopub.status.idle":"2024-08-15T08:37:57.145624Z","shell.execute_reply.started":"2024-08-15T08:37:51.929701Z","shell.execute_reply":"2024-08-15T08:37:57.144808Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f6b04210c584b90b4acb850153b7da9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4fa55ff7ac0641fbb37d52fa5a94e3fb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d82de10a312a4972b85f541700e5536b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa6acfbbe2084fd5b94cced7bc0c160e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58574277ff54438c88c9c84e3c230fa7"}},"metadata":{}}]},{"cell_type":"markdown","source":"定义用于转换数据帧的一行的预处理函数：","metadata":{}},{"cell_type":"code","source":"def roberta_preprocessing_function(examples):\n    return roberta_tokenizer(examples['text'], truncation=True, max_length=MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:38:07.585248Z","iopub.execute_input":"2024-08-15T08:38:07.585768Z","iopub.status.idle":"2024-08-15T08:38:07.590421Z","shell.execute_reply.started":"2024-08-15T08:38:07.585741Z","shell.execute_reply":"2024-08-15T08:38:07.589399Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"通过将预处理函数应用于训练数据集的第一个示例，我们得到了标记化输入（ input_ids ）和注意掩码：","metadata":{}},{"cell_type":"code","source":"roberta_preprocessing_function(data['train'][0])","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:38:14.846582Z","iopub.execute_input":"2024-08-15T08:38:14.847338Z","iopub.status.idle":"2024-08-15T08:38:14.856957Z","shell.execute_reply.started":"2024-08-15T08:38:14.847301Z","shell.execute_reply":"2024-08-15T08:38:14.856042Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"{'input_ids': [0, 127, 2373, 2490, 4, 1205, 640, 90, 4, 876, 73, 118, 725, 398, 13083, 329, 398, 119, 1343, 246, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"},"metadata":{}}]},{"cell_type":"markdown","source":"现在，让我们将预处理函数应用于整个数据集：","metadata":{}},{"cell_type":"code","source":"col_to_delete = ['id', 'keyword','location', 'text']\n# Apply the preprocessing function and remove the undesired columns\nroberta_tokenized_datasets = data.map(roberta_preprocessing_function, batched=True, remove_columns=col_to_delete)\n# Rename the target to label as for HugginFace standards\nroberta_tokenized_datasets = roberta_tokenized_datasets.rename_column(\"target\", \"label\")\n# Set to torch format\nroberta_tokenized_datasets.set_format(\"torch\")","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:38:23.346827Z","iopub.execute_input":"2024-08-15T08:38:23.347208Z","iopub.status.idle":"2024-08-15T08:38:24.424583Z","shell.execute_reply.started":"2024-08-15T08:38:23.347178Z","shell.execute_reply":"2024-08-15T08:38:24.423710Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6090 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a385cf7f53fe4eafa976cf4b43afa335"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1523 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b63fa116c37d4281b0a4e1f5d5a064a3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3263 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1c3c5d5c58048c2af1b6b2300bd0a73"}},"metadata":{}}]},{"cell_type":"markdown","source":"我们可以查看我们的标记化训练数据集：","metadata":{}},{"cell_type":"code","source":"roberta_tokenized_datasets['train'][0]","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:38:31.268434Z","iopub.execute_input":"2024-08-15T08:38:31.269280Z","iopub.status.idle":"2024-08-15T08:38:31.314977Z","shell.execute_reply.started":"2024-08-15T08:38:31.269249Z","shell.execute_reply":"2024-08-15T08:38:31.314137Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'label': tensor(0),\n 'input_ids': tensor([    0,   127,  2373,  2490,     4,  1205,   640,    90,     4,   876,\n            73,   118,   725,   398, 13083,   329,   398,   119,  1343,   246,\n             2]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}"},"metadata":{}}]},{"cell_type":"markdown","source":"为了生成训练批次，我们还需要将给定批次的行填充到批次中找到的最大长度。为此，我们将使用DataCollatorWithPadding类：","metadata":{}},{"cell_type":"code","source":"# Data collator for padding a batch of examples to the maximum length seen in the batch\nfrom transformers import DataCollatorWithPadding\n\nroberta_data_collator = DataCollatorWithPadding(tokenizer=roberta_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:38:39.162291Z","iopub.execute_input":"2024-08-15T08:38:39.163196Z","iopub.status.idle":"2024-08-15T08:38:50.120622Z","shell.execute_reply.started":"2024-08-15T08:38:39.163166Z","shell.execute_reply":"2024-08-15T08:38:50.119851Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"2024-08-15 08:38:41.113197: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-15 08:38:41.113329: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-15 08:38:41.227380: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"我们使用 Hugging Face AutoModelForSequenceClassification类加载带有序列分类头的预训练 RoBERTa 模型：","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nroberta_model = AutoModelForSequenceClassification.from_pretrained(roberta_checkpoint, num_labels=2)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:39:08.182532Z","iopub.execute_input":"2024-08-15T08:39:08.183215Z","iopub.status.idle":"2024-08-15T08:39:14.064659Z","shell.execute_reply.started":"2024-08-15T08:39:08.183183Z","shell.execute_reply":"2024-08-15T08:39:14.063845Z"},"trusted":true},"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff4ef4c2005f462da55f4b16badcf878"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install peft -q","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:39:54.495678Z","iopub.execute_input":"2024-08-15T08:39:54.496477Z","iopub.status.idle":"2024-08-15T08:40:06.767478Z","shell.execute_reply.started":"2024-08-15T08:39:54.496441Z","shell.execute_reply":"2024-08-15T08:40:06.766338Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"我们导入 LoRa 配置并为 RoBERTa 分类器设置一些参数：","metadata":{}},{"cell_type":"code","source":"from peft import get_peft_model, LoraConfig, TaskType\n\nroberta_peft_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS, r=2, lora_alpha=16, lora_dropout=0.1, bias=\"none\",\n)\n\nroberta_model = get_peft_model(roberta_model, roberta_peft_config)\nroberta_model.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:40:11.778751Z","iopub.execute_input":"2024-08-15T08:40:11.779158Z","iopub.status.idle":"2024-08-15T08:40:11.932528Z","shell.execute_reply.started":"2024-08-15T08:40:11.779122Z","shell.execute_reply":"2024-08-15T08:40:11.931626Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"trainable params: 1,248,258 || all params: 356,610,052 || trainable%: 0.3500\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install evaluate -q","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:41:46.216670Z","iopub.execute_input":"2024-08-15T08:41:46.217350Z","iopub.status.idle":"2024-08-15T08:41:58.571668Z","shell.execute_reply.started":"2024-08-15T08:41:46.217314Z","shell.execute_reply":"2024-08-15T08:41:58.570620Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"我们定义用于比较三个模型的性能指标：F1 分数、召回率、精确度和准确度：","metadata":{}},{"cell_type":"code","source":"import evaluate\nimport numpy as np\n\ndef compute_metrics(eval_pred):\n    # All metrics are already predefined in the HF `evaluate` package\n    precision_metric = evaluate.load(\"precision\")\n    recall_metric = evaluate.load(\"recall\")\n    f1_metric= evaluate.load(\"f1\")\n    accuracy_metric = evaluate.load(\"accuracy\")\n\n    logits, labels = eval_pred # eval_pred is the tuple of predictions and labels returned by the model\n    predictions = np.argmax(logits, axis=-1)\n    precision = precision_metric.compute(predictions=predictions, references=labels)[\"precision\"]\n    recall = recall_metric.compute(predictions=predictions, references=labels)[\"recall\"]\n    f1 = f1_metric.compute(predictions=predictions, references=labels)[\"f1\"]\n    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n    # The trainer is expecting a dictionary where the keys are the metrics names and the values are the scores. \n    return {\"precision\": precision, \"recall\": recall, \"f1-score\": f1, 'accuracy': accuracy}","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:42:12.363706Z","iopub.execute_input":"2024-08-15T08:42:12.364119Z","iopub.status.idle":"2024-08-15T08:42:13.322995Z","shell.execute_reply.started":"2024-08-15T08:42:12.364060Z","shell.execute_reply":"2024-08-15T08:42:13.322029Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"正如本文开头提到的，我们的正类和负类之间的分布不平衡。我们需要用加权交叉熵损失来训练我们的模型来解决这个问题。 Trainer类不支持提供自定义损失，因为它期望直接从模型的输出中获取损失。因此，我们需要定义自定义的WeightedCELossTrainer来重写compute_loss方法，以根据模型的预测和输入标签计算加权交叉熵损失：","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\n\nclass WeightedCELossTrainer(Trainer):\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels = inputs.pop(\"labels\")\n        # Get model's predictions\n        outputs = model(**inputs)\n        logits = outputs.get(\"logits\")\n        # Compute custom loss\n        loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor([neg_weights, pos_weights], device=model.device, dtype=logits.dtype))\n        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n        return (loss, outputs) if return_outputs else loss","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:42:20.983107Z","iopub.execute_input":"2024-08-15T08:42:20.983463Z","iopub.status.idle":"2024-08-15T08:42:21.494302Z","shell.execute_reply.started":"2024-08-15T08:42:20.983436Z","shell.execute_reply":"2024-08-15T08:42:21.493324Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"将模型转移到 GPU 设备上进行训练：","metadata":{}},{"cell_type":"code","source":"roberta_model = roberta_model.cuda()\n#roberta_model.device()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:43:46.094704Z","iopub.execute_input":"2024-08-15T08:43:46.095303Z","iopub.status.idle":"2024-08-15T08:43:46.110389Z","shell.execute_reply.started":"2024-08-15T08:43:46.095269Z","shell.execute_reply":"2024-08-15T08:43:46.109502Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"设置训练参数：","metadata":{}},{"cell_type":"code","source":"from transformers import TrainingArguments\n\nlr = 1e-4\nbatch_size = 8\nnum_epochs = 5\n\ntraining_args = TrainingArguments(\n    output_dir=\"roberta-large-lora-token-classification\",\n    learning_rate=lr,\n    lr_scheduler_type= \"constant\",\n    warmup_ratio= 0.1,\n    max_grad_norm= 0.3,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=num_epochs,\n    weight_decay=0.001,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    load_best_model_at_end=True,\n    report_to=\"wandb\",\n    fp16=False,\n    gradient_checkpointing=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:43:52.695036Z","iopub.execute_input":"2024-08-15T08:43:52.695421Z","iopub.status.idle":"2024-08-15T08:43:52.725478Z","shell.execute_reply.started":"2024-08-15T08:43:52.695391Z","shell.execute_reply":"2024-08-15T08:43:52.724571Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"最后，我们通过提供模型、训练参数和标记化数据集来定义 RoBERTa 训练器：","metadata":{}},{"cell_type":"code","source":"roberta_trainer = WeightedCELossTrainer(\n    model=roberta_model,\n    args=training_args,\n    train_dataset=roberta_tokenized_datasets['train'],\n    eval_dataset=roberta_tokenized_datasets[\"val\"],\n    data_collator=roberta_data_collator,\n    compute_metrics=compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:44:06.976507Z","iopub.execute_input":"2024-08-15T08:44:06.976877Z","iopub.status.idle":"2024-08-15T08:44:07.905771Z","shell.execute_reply.started":"2024-08-15T08:44:06.976849Z","shell.execute_reply":"2024-08-15T08:44:07.904797Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"开始训练：","metadata":{}},{"cell_type":"code","source":"import torch\n\nroberta_trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-15T08:47:35.981235Z","iopub.execute_input":"2024-08-15T08:47:35.981601Z","iopub.status.idle":"2024-08-15T08:53:39.923252Z","shell.execute_reply.started":"2024-08-15T08:47:35.981571Z","shell.execute_reply":"2024-08-15T08:53:39.922324Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3810' max='3810' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3810/3810 06:03, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1-score</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.656100</td>\n      <td>0.557806</td>\n      <td>0.664411</td>\n      <td>0.754224</td>\n      <td>0.706475</td>\n      <td>0.732108</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.572800</td>\n      <td>0.595131</td>\n      <td>0.538108</td>\n      <td>0.900154</td>\n      <td>0.673563</td>\n      <td>0.627052</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.557700</td>\n      <td>0.498145</td>\n      <td>0.691460</td>\n      <td>0.771121</td>\n      <td>0.729121</td>\n      <td>0.755089</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.553100</td>\n      <td>0.479321</td>\n      <td>0.820976</td>\n      <td>0.697389</td>\n      <td>0.754153</td>\n      <td>0.805647</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.540000</td>\n      <td>0.475383</td>\n      <td>0.848077</td>\n      <td>0.677419</td>\n      <td>0.753202</td>\n      <td>0.810243</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23558af44add41238f0bc7f1fe14bf4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/7.36k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da498f85fdb6431ead4471c6a5ec5d87"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/6.77k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7ead1c8c8634d52a115985e56cd1c94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f4f68b8c9fe441e788f82fa780ab49ec"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3810, training_loss=0.5647373109351932, metrics={'train_runtime': 363.5465, 'train_samples_per_second': 83.758, 'train_steps_per_second': 10.48, 'total_flos': 2795794693370352.0, 'train_loss': 0.5647373109351932, 'epoch': 5.0})"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}