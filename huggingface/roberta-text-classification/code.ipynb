{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下载数据集：https://github.com/SophonPlus/ChineseNlpCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 11987\n",
      "Total number of samples after removing NaN: 11987\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data import random_split\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('data/waimai_10k.csv')\n",
    "data.head()\n",
    "print(f\"Total number of samples: {len(data)}\")\n",
    "data = data.dropna()\n",
    "\n",
    "print(f\"Total number of samples after removing NaN: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('很快，好吃，味道足，量大', 1)\n",
      "11987\n"
     ]
    }
   ],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self,data):\n",
    "        super().__init__()\n",
    "        self.data = data.dropna()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.data.iloc[index]['review'],self.data.iloc[index]['label']\n",
    "\n",
    "# 对Mydataset进行实例化\n",
    "dataset = MyDataset(data)\n",
    "print(dataset[0])\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples: 11987\n",
      "Number of training samples: 9590\n",
      "Number of test samples: 2397\n",
      "('粉太淡没味，没上次好吃了，也不辣。', 0)\n"
     ]
    }
   ],
   "source": [
    "trainset, testset = random_split(dataset, [0.8, 0.2])\n",
    "\n",
    "print(f\"Total number of samples: {len(dataset)}\")\n",
    "print(f\"Number of training samples: {len(trainset)}\")\n",
    "print(f\"Number of test samples: {len(testset)}\")\n",
    "print(trainset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1962, 1391,  ...,    0,    0,    0],\n",
       "        [ 101, 1343,  782,  ...,    0,    0,    0],\n",
       "        [ 101, 3862, 5976,  ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 2523, 2571,  ...,    0,    0,    0],\n",
       "        [ 101, 7824, 7676,  ...,    0,    0,    0],\n",
       "        [ 101,  679, 3221,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0],\n",
       "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0],\n",
       "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/rbt3\")\n",
    "\n",
    "def collate_func(batch):\n",
    "    texts, labels = [], []\n",
    "    for item in batch:\n",
    "        texts.append(item[0])\n",
    "        labels.append(item[1])\n",
    "    \n",
    "    inputs = tokenizer(texts, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    inputs[\"labels\"] = torch.tensor(labels)\n",
    "    return inputs\n",
    "\n",
    "## 创建DataLoader\n",
    "train_dataloader = DataLoader(trainset, batch_size=16, shuffle=True, collate_fn=collate_func)\n",
    "test_dataloader = DataLoader(testset, batch_size=16, shuffle=False, collate_fn=collate_func)\n",
    "\n",
    "## 检查train_dataloader中的数据\n",
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\miniconda3\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/rbt3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"hfl/rbt3\") \n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "    \n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for batch in test_dataloader:\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k:v.cuda() for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            correct += (predictions == batch['labels']).sum().item()\n",
    "            \n",
    "    return correct / len(test_dataloader.dataset) * 100 # 返回准确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch = 1, log_step = 50): \n",
    "    global_step = 0\n",
    "    for ep in range(epoch):\n",
    "        model.train()\n",
    "        for batch in train_dataloader:\n",
    "            if torch.cuda.is_available():\n",
    "                batch = {k:v.cuda() for k, v in batch.items()} \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if (global_step+1) % log_step == 0:\n",
    "                print(f'Epoch: {ep+1}, global_step: {global_step+1},loss: {loss.item()}')\n",
    "            global_step += 1\n",
    "        acc = evaluate()\n",
    "        print(f'Epoch: {ep+1}, Accuracy: {acc:.2f}%') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, global_step: 50,loss: 0.5368702411651611\n",
      "Epoch: 1, global_step: 100,loss: 0.2037513554096222\n",
      "Epoch: 1, global_step: 150,loss: 0.2375202625989914\n",
      "Epoch: 1, global_step: 200,loss: 0.4101874530315399\n",
      "Epoch: 1, global_step: 250,loss: 0.28052493929862976\n",
      "Epoch: 1, global_step: 300,loss: 0.37337055802345276\n",
      "Epoch: 1, global_step: 350,loss: 0.10706482827663422\n",
      "Epoch: 1, global_step: 400,loss: 0.078570157289505\n",
      "Epoch: 1, global_step: 450,loss: 0.6404622793197632\n",
      "Epoch: 1, global_step: 500,loss: 0.23093605041503906\n",
      "Epoch: 1, global_step: 550,loss: 0.30611029267311096\n",
      "Epoch: 1, global_step: 600,loss: 0.4223825931549072\n",
      "Epoch: 1, Accuracy: 90.57%\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7320, 0.2680]], device='cuda:0')\n",
      "Sentence: 这家饭馆真贴心啊，知道我吃不饱还特地在里面防蟑螂！\n",
      "Predicted class: negative\n"
     ]
    }
   ],
   "source": [
    "sentence1 = \"这家饭馆真贴心啊，知道我吃不饱还特地在里面防蟑螂！\"\n",
    "sentence2 = \"难吃的和shit一样，恶心！\"\n",
    "\n",
    "model.eval()\n",
    "\n",
    "id2_label = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "with torch.inference_mode():\n",
    "    inputs = tokenizer(sentence1, return_tensors=\"pt\")\n",
    "    inputs = {k:v.cuda() for k, v in inputs.items()}\n",
    "    logits = model(**inputs).logits\n",
    "    softmax_ = torch.nn.Softmax(dim=1)(logits)\n",
    "    print(softmax_)\n",
    "    predicted_class = torch.argmax(logits, dim=-1)\n",
    "    print(f\"Sentence: {sentence1}\")\n",
    "    print(f\"Predicted class: {id2_label.get(predicted_class.item())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./saved_models\\rbt3_save.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "output_dir = \"./saved_models\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "model_save_path = os.path.join(output_dir, f\"rbt3_save.pt\")\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/rbt3 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7320, 0.2680]], device='cuda:0')\n",
      "Sentence: 这家饭馆真贴心啊，知道我吃不饱还特地在里面防蟑螂！\n",
      "Predicted class: negative\n"
     ]
    }
   ],
   "source": [
    "# 加载模型架构，但不加载权重\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"hfl/rbt3\")\n",
    "# 加载保存的模型状态字典\n",
    "model_save_path = \"./saved_models/rbt3_save.pt\"\n",
    "state_dict = torch.load(model_save_path)\n",
    "\n",
    "# 将状态字典加载到模型实例中\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "# 移动模型到GPU（如果可用）\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "# 切换模型到评估模式\n",
    "model.eval()\n",
    "\n",
    "# 推理过程\n",
    "sentence1 = \"这家饭馆真贴心啊，知道我吃不饱还特地在里面防蟑螂！\"\n",
    "sentence2 = \"难吃的和shit一样，恶心！\"\n",
    "\n",
    "id2_label = {0: \"negative\", 1: \"positive\"}\n",
    "\n",
    "with torch.inference_mode():\n",
    "    inputs = tokenizer(sentence1, return_tensors=\"pt\")\n",
    "    inputs = {k:v.cuda() for k, v in inputs.items()}\n",
    "    logits = model(**inputs).logits\n",
    "    softmax_ = torch.nn.Softmax(dim=1)(logits)\n",
    "    print(softmax_)\n",
    "    predicted_class = torch.argmax(logits, dim=-1)\n",
    "    print(f\"Sentence: {sentence1}\")\n",
    "    print(f\"Predicted class: {id2_label.get(predicted_class.item())}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
