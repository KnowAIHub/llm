{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d85a817-ac9a-4b3f-bf02-4da37d40aff6",
   "metadata": {},
   "source": [
    "## vLLM 本地大模型部署和推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bae45dc1-c7ca-4afe-a781-6fbdb031f18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-01 22:45:06 llm_engine.py:226] Initializing an LLM engine (v0.6.1.dev238+ge2c6e0a82) with config: model='./Qwen2___5-3B-Instruct', speculative_config=None, tokenizer='./Qwen2___5-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=./Qwen2___5-3B-Instruct, use_v2_block_manager=False, num_scheduler_steps=1, multi_step_stream_outputs=False, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 10-01 22:45:08 model_runner.py:1014] Starting to load model ./Qwen2___5-3B-Instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "491563cfc85343e7b2caa0e8967953f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-01 22:45:12 model_runner.py:1025] Loading model weights took 5.7915 GB\n",
      "INFO 10-01 22:45:14 gpu_executor.py:122] # GPU blocks: 48781, # CPU blocks: 7281\n",
      "INFO 10-01 22:45:17 model_runner.py:1329] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 10-01 22:45:17 model_runner.py:1333] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 10-01 22:45:33 model_runner.py:1456] Graph capturing finished in 16 secs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.47s/it, est. speed input: 17.07 toks/s, output: 101.72 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt提示词: '<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n天空为什么是蓝色的？<|im_end|>\\n<|im_start|>assistant\\n', 大模型推理输出: '这是由于大气散射的原因。天空呈现出蓝色，主要是因为太阳光中的短波光（如蓝光和紫光）更容易被大气中的气体分子（主要是氮气和氧气）散射。尽管所有颜色的光都被散射了，但蓝光由于波长较短，更容易被散射到各个方向，因此我们看到的天空就是蓝色的。\\n\\n在日出和日落时分，太阳接近地平线，光线需要穿过更多的大气层才能到达观察者的眼睛，这个过程中蓝光和紫光都被散射掉了，剩下的主要是红光、橙光和黄光，这就是为什么日出和日落时天空会呈现红色、橙色或黄色的原因。'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Qwen2-vLLM-Local.py\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    " \n",
    "# 设置环境变量\n",
    "os.environ['VLLM_TARGET_DEVICE'] = 'cuda'\n",
    " \n",
    "# 模型ID：我们下载的模型权重文件目录\n",
    "model_dir = './Qwen2___5-3B-Instruct'\n",
    " \n",
    "# Tokenizer初始化\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_dir,\n",
    "    local_files_only=True,\n",
    ")\n",
    " \n",
    "# Prompt提示词\n",
    "messages = [\n",
    "    {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "    {'role': 'user', 'content': '天空为什么是蓝色的？'}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    " \n",
    "# 初始化大语言模型\n",
    "llm = LLM(\n",
    "    model=model_dir,\n",
    "    tensor_parallel_size=1,  # CPU无需张量并行\n",
    "    device='cuda',\n",
    ")\n",
    " \n",
    "# 超参数：最多512个Token\n",
    "sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=512)\n",
    " \n",
    "# 模型推理输出\n",
    "outputs = llm.generate([text], sampling_params)\n",
    " \n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    " \n",
    "    print(f'Prompt提示词: {prompt!r}, 大模型推理输出: {generated_text!r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb18b0a4-71da-4634-8fbd-418f2392fb72",
   "metadata": {},
   "source": [
    "## 发布 API 服务和调用推理\n",
    "\n",
    "本地部署推理只能在一台服务器完成，我们也通过vLLM把本地大模型部署成 OpenAI API 服务："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdc1a17-f313-4465-80aa-125238b0f62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m vllm.entrypoints.openai.api_server --model ./Qwen2___5-3B-Instruct\n",
    "# 修改端口\n",
    "python -m vllm.entrypoints.openai.api_server --model ./Qwen2___5-3B-Instruct --port 8000 --host 0.0.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceef021e-411f-464d-80eb-15e3887963fd",
   "metadata": {},
   "source": [
    "API 服务部署成功之后，可以通过 CURL 命令验证服务："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad746858-cde1-40fd-9fe9-17292405bdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen2-vLLM-CURL.py\n",
    "curl http://localhost:8000/v1/chat/completions -H \"Content-Type: application/json\" -d '{\n",
    "  \"model\": \"./Qwen2___5-3B-Instruct\",\n",
    "  \"messages\": [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"天空为什么是蓝色的？\"}\n",
    "  ],\n",
    "  \"temperature\": 0.7,\n",
    "  \"top_p\": 0.8,\n",
    "  \"repetition_penalty\": 1.05,\n",
    "  \"max_tokens\": 512\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c121987b-9a14-4ba0-b332-9d1f1856b1fd",
   "metadata": {},
   "source": [
    "或者，我们可以通过 Python 客户端调用API 访问服务：\n",
    "\n",
    "若没有安装openai依赖包，需要提前安装一下：`pip install openai`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229f1fc0-4242-45d8-8808-cb47c4a581d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen2-vLLM-OpenAI.py\n",
    "from openai import OpenAI\n",
    " \n",
    "# OpenAI初始化\n",
    "client = OpenAI(\n",
    "    api_key='EMPTY',\n",
    "    base_url='http://localhost:8000/v1',\n",
    ")\n",
    " \n",
    "chat_response = client.chat.completions.create(\n",
    "    model='./Qwen2___5-3B-Instruct',\n",
    "    messages=[\n",
    "        {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "        {'role': 'user', 'content': '天空为什么是蓝色的？'},\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    top_p=0.8,\n",
    "    max_tokens=512,\n",
    ")\n",
    " \n",
    "print('Qwen2推理结果:', chat_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf8a35ca-3de0-440f-b0e5-b75ec8868818",
   "metadata": {},
   "source": [
    "我们还可以通过 WebUI 访问我们部署的 API 服务："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dde9eb4-47eb-4d0b-82f4-7af332d835b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen2-vLLM-WebUI.py\n",
    "import argparse\n",
    "import json\n",
    " \n",
    "import gradio as gr\n",
    "import requests\n",
    " \n",
    " \n",
    "def http_bot(prompt):\n",
    "    headers = {\"User-Agent\": \"vLLM Client\"}\n",
    "    pload = {\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": True,\n",
    "        \"max_tokens\": 128,\n",
    "    }\n",
    "    response = requests.post(args.model_url,\n",
    "                             headers=headers,\n",
    "                             json=pload,\n",
    "                             stream=True)\n",
    " \n",
    "    for chunk in response.iter_lines(chunk_size=8192,\n",
    "                                     decode_unicode=False,\n",
    "                                     delimiter=b\"\\0\"):\n",
    "        if chunk:\n",
    "            data = json.loads(chunk.decode(\"utf-8\"))\n",
    "            output = data[\"text\"][0]\n",
    "            yield output\n",
    " \n",
    " \n",
    "def build_demo():\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# vLLM text completion demo\\n\")\n",
    "        inputbox = gr.Textbox(label=\"Input\",\n",
    "                              placeholder=\"Enter text and press ENTER\")\n",
    "        outputbox = gr.Textbox(label=\"Output\",\n",
    "                               placeholder=\"Generated result from the model\")\n",
    "        inputbox.submit(http_bot, [inputbox], [outputbox])\n",
    "    return demo\n",
    " \n",
    " \n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--host\", type=str, default=None)\n",
    "    parser.add_argument(\"--port\", type=int, default=8001)\n",
    "    parser.add_argument(\"--model-url\",\n",
    "                        type=str,\n",
    "                        default=\"http://0.0.0.0:8000/generate\")\n",
    "    args = parser.parse_args()\n",
    " \n",
    "    demo = build_demo()\n",
    "    demo.queue().launch(server_name=args.host,\n",
    "                        server_port=args.port,\n",
    "                        share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6d91bc-6fde-4b7e-bc6c-e0f9297757dd",
   "metadata": {},
   "source": [
    "## GPU 多卡部署和推理大模型\n",
    "\n",
    "我们可以通过`tensor_parallel_size`参数启用 GPU 多卡分布式并行推理能力，提高大模型推理的吞吐量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6a8681-314f-452b-8318-907cc0bc068e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Qwen2-vLLM-Local.py\n",
    "import os\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    " \n",
    "# 设置环境变量\n",
    "os.environ['VLLM_TARGET_DEVICE'] = 'cuda'\n",
    " \n",
    "# 模型ID：我们下载的模型权重文件目录\n",
    "model_dir = './Qwen2___5-3B-Instruct'\n",
    " \n",
    "# Tokenizer初始化\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_dir,\n",
    "    local_files_only=True,\n",
    ")\n",
    " \n",
    "# Prompt提示词\n",
    "messages = [\n",
    "    {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
    "    {'role': 'user', 'content': '天空为什么是蓝色的？'}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    " \n",
    "# 初始化大语言模型\n",
    "llm = LLM(\n",
    "    model=model_dir,\n",
    "    tensor_parallel_size=4,  # 4卡张量并行\n",
    "    device='cuda',\n",
    ")\n",
    " \n",
    "# 超参数：最多512个Token\n",
    "sampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=512)\n",
    " \n",
    "# 模型推理输出\n",
    "outputs = llm.generate([text], sampling_params)\n",
    " \n",
    "for output in outputs:\n",
    "    prompt = output.prompt\n",
    "    generated_text = output.outputs[0].text\n",
    " \n",
    "    print(f'Prompt提示词: {prompt!r}, 大模型推理输出: {generated_text!r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf681af5-9f5d-41b0-9931-33248034f0ef",
   "metadata": {},
   "source": [
    "同样的，也可以通过 `--tensor-parallel-size` 参数部署并发布 API 服务："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37e5e9c-c834-47a1-9dff-5c3ed8de32de",
   "metadata": {},
   "outputs": [],
   "source": [
    "python -m vllm.entrypoints.api_server --model ./Qwen2___5-3B-Instruct --tensor-parallel-size 4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
