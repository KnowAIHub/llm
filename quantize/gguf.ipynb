{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdf87363-4885-437a-8140-3d0624a9a325",
   "metadata": {},
   "source": [
    "## 安装需要的包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03d7469-7414-449e-9726-141b88471917",
   "metadata": {},
   "outputs": [],
   "source": [
    "git clone https://github.com/ggerganov/llama.cpp\n",
    "cd llama.cpp\n",
    "make -j 8 llama-cli llama-quantize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c0b297-2413-4191-90ef-bdd2da0a388d",
   "metadata": {},
   "source": [
    "## 准备您自己的 GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fc077b-901f-421a-bab4-00acb7ad75ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FP16\n",
    "python convert_hf_to_gguf.py ../Qwen2-1.5B-Instruct/ --outfile ../Qwen2-1.5b-instruct-f16.gguf\n",
    "\n",
    "# FP32\n",
    "python convert_hf_to_gguf.py ../Qwen2-1.5B-Instruct/ --outtype f32 --outfile ../Qwen2-1.5b-instruct-f32.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a93c83d-f021-4ca7-bcc3-13d246a17eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对话模式\n",
    "./llama-cli -m ../Qwen2-1.5b-instruct-f16.gguf \\\n",
    "    -co -cnv -p \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\" \\\n",
    "    -fa -ngl 80 -n 512\n",
    "\n",
    "# -m 或 –model:\n",
    "# 显然，这是模型路径。\n",
    "\n",
    "# -co 或 –color:\n",
    "# 为输出着色以区分提示词、用户输入和生成的文本。提示文本为深黄色；用户文本为绿色；生成的文本为白色；错误文本为红色。\n",
    "\n",
    "# -cnv 或 –conversation:\n",
    "# 在对话模式下运行。程序将相应地应用聊天模板。\n",
    "\n",
    "# -p 或 –prompt:\n",
    "# 在对话模式下，它作为系统提示。\n",
    "\n",
    "# -fa 或 –flash-attn:\n",
    "# 如果程序编译时支持 GPU，则启用Flash Attention注意力实现。\n",
    "\n",
    "# -ngl 或 –n-gpu-layers:\n",
    "# 如果程序编译时支持 GPU，则将这么多层分配给 GPU 进行计算。\n",
    "\n",
    "# -n 或 –predict:\n",
    "# 要预测的token数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52982143-9a85-4f77-8165-7e157ca2ffe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 互动模式\n",
    "./llama-cli -m ../Qwen2-1.5b-instruct-f16.gguf \\\n",
    "    -co -sp -i -if -p \"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n\" \\\n",
    "    --in-prefix \"<|im_start|>user\\n\" --in-suffix \"<|im_end|>\\n<|im_start|>assistant\\n\" \\\n",
    "    -fa -ngl 80 -n 512\n",
    "\n",
    "# 我们在这里使用了一些新的选项：\n",
    "\n",
    "# -sp 或 –special:\n",
    "# 显示特殊token。\n",
    "\n",
    "# -i 或 –interactive:\n",
    "# 进入互动模式。你可以中断模型生成并添加新文本。\n",
    "\n",
    "# -i 或 –interactive-first:\n",
    "# 立即等待用户输入。否则，模型将立即运行并根据提示生成文本。\n",
    "\n",
    "# -p 或 –prompt:\n",
    "# 在互动模式下，这是模型续写用的上文。\n",
    "\n",
    "# --in-prefix:\n",
    "# 用户输入附加的前缀字符串。\n",
    "\n",
    "# --in-suffix:\n",
    "# 用户输入附加的后缀字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c808af-cb71-4082-8e62-b98a789ddc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 非交互模式\n",
    "./llama-cli -m ../Qwen2-1.5b-instruct-f16.gguf \\\n",
    "    -co -sp -p \"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\ngive me a short introduction to LLMs.<|im_end|>\\n<|im_start|>assistant\\n\" \\\n",
    "    -fa -ngl 80 -n 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792a548b-e774-48b6-ba07-bad577671a55",
   "metadata": {},
   "source": [
    "## 无校准量化GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3128978e-1e98-4511-8d65-11afc1ee1684",
   "metadata": {},
   "outputs": [],
   "source": [
    "./llama-quantize ../Qwen2-1.5b-instruct-f16.gguf ../Qwen2-1.5b-instruct-q4_k_m.gguf Q4_K_M\n",
    "\n",
    "# 这里的Q4_K_M是其中一种量化方法，还有很多，你可以查看：https://github.com/ggerganov/llama.cpp/blob/master/examples/quantize/quantize.cpp\n",
    "# 但是很多证据表明，Q4_K_M 量化的最好:https://oobabooga.github.io/blog/posts/perplexities/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce20b89-b054-4c1e-a095-0f20a90c5fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "./llama-cli -m ../Qwen2-1.5b-instruct-q4_k_m.gguf \\\n",
    "    -co -cnv -p \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\" \\\n",
    "    -fa -ngl 80 -n 512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9cdcfe-fa77-40d8-9a7f-bfccdb6570e0",
   "metadata": {},
   "source": [
    "到此，你可以把量化的模型发布到 Ollama 社区了。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a653760-e7d8-49e6-a541-e602086ec42e",
   "metadata": {},
   "source": [
    "## 使用AWQ尺度量化GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b649ffb-4cfd-4bbe-9c82-fb6a68a51b5e",
   "metadata": {},
   "source": [
    "为了提高量化模型的质量，一种可能的解决方案是应用[AWQ尺度](https://github.com/casper-hansen/AutoAWQ/blob/main/docs/examples.md#gguf-export)，遵循这个脚本。首先，当你使用`autoawq`运行`model.quantize()`时，记得添加`export_compatible=True`，如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ac072d-7d23-4d2a-bdb3-4bd9a2522be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "model.quantize(\n",
    "    tokenizer,\n",
    "    quant_config=quant_config,\n",
    "    export_compatible=True\n",
    ")\n",
    "model.save_pretrained(quant_path)\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d7977f-b9eb-44d7-8f46-e56f062ef7f8",
   "metadata": {},
   "source": [
    "上述代码实际上不会量化权重。相反，它会根据数据集调整权重，使它们“更容易”量化。\n",
    "\n",
    "然后，当你运行`convert_hf_to_gguf.py`时，记得将模型路径替换为新模型的路径："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85fe864-632a-46b6-b443-aa2d445d658a",
   "metadata": {},
   "outputs": [],
   "source": [
    "python convert_hf_to_gguf.py <quant_path> --outfile qwen2-1.5b-instruct-f16-awq.gguf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd99f15c-21ac-4198-ab50-94a603db927c",
   "metadata": {},
   "source": [
    "最后，你可以像最后一个例子那样量化模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f4efd0-9f94-4f31-bd9e-a40e57bc3c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "./llama-quantize qwen2-1.5b-instruct-f16-awq.gguf ../qwen2-1.5b-instruct-q4_k_m-awq.gguf Q4_K_M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd3fdb6-e39b-4787-bcd1-12cd31dbf9d9",
   "metadata": {},
   "source": [
    "## 参考\n",
    "\n",
    "- https://qwen.readthedocs.io/\n",
    "- https://github.com/QwenLM/Qwen2.5/tree/main/docs/source/quantization\n",
    "- https://mlabonne.github.io/blog/posts/Quantize_Llama_2_models_using_ggml.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b1351a-0d31-4f93-8263-a3d1f1e591cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
