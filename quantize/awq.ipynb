{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64cf035f-3c8e-40b0-94df-9cf57ddaa91f",
   "metadata": {},
   "source": [
    "## 安装AWQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d079c1f-bbf5-4b28-999d-a76043611495",
   "metadata": {},
   "outputs": [],
   "source": [
    "git clone https://github.com/casper-hansen/AutoAWQ.git\n",
    "cd AutoAWQ\n",
    "pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aecaaea-4dd8-4f4c-9e4d-9d6e297758ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8901377-37f6-4ee6-9285-028ebf3bf8a3",
   "metadata": {},
   "source": [
    "- torch                          2.2.0+cu118\n",
    "- triton                         3.0.0\n",
    "- transformers                   4.45.2\n",
    "- auto_gptq                      0.7.0+cu118\n",
    "- ~autoawq                        0.2.6+cu118~\n",
    "- datasets                       3.0.1\n",
    "- optimum                        1.23.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8db4356-27a7-46f6-9a96-38b40e9bcf60",
   "metadata": {},
   "source": [
    "## AWQ量化微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccb13d7e-dd21-4bbc-a650-9f1c9c68c307",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from awq import AutoAWQForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = 'cuda:0'\n",
    "model_path = \"Qwen2-1.5B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad02a213-040f-4fa8-933a-551c9d6eaea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_config = { \"zero_point\": True, \"q_group_size\": 128, \"w_bit\": 4, \"version\": \"GEMM\" }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ed0f56-15f3-432e-836e-62831d949751",
   "metadata": {},
   "source": [
    "加载分词器，并根据配置quantize_config指定的量化位数来加载模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dafdc32f-00c9-478d-b619-df92a95800ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoAWQForCausalLM.from_pretrained(model_path, device_map=\"auto\", safetensors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c029bee9-4aac-47e3-a2e8-974d3f580c9d",
   "metadata": {},
   "source": [
    "AWQ 量化需要准备校准数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fe2b730-6265-4179-8648-9156525b70ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# 从Hugging Face加载数据集\n",
    "dataset = load_dataset(\"lamini/lamini_docs\")\n",
    "\n",
    "train_dataset = dataset['train']\n",
    "train_df = pd.DataFrame(train_dataset)\n",
    "questions_answers = train_df[['question', 'answer']]\n",
    "\n",
    "with open('gptq_data_chat_format.jsonl', 'w') as jsonl_file:\n",
    "    for index, example in questions_answers.iterrows():\n",
    "        formatted_data = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You're a helpful assistant\"}, \n",
    "                {\"role\": \"user\", \"content\": example['question']},\n",
    "                {\"role\": \"assistant\", \"content\": example['answer']}\n",
    "            ]\n",
    "        }\n",
    "        jsonl_file.write(json.dumps(formatted_data) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a35bd2ad-8c41-4087-842a-b7c8dec10b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'system', 'content': \"You're a helpful assistant\"},\n",
       " {'role': 'user',\n",
       "  'content': 'How can I evaluate the performance and quality of the generated text from Lamini models?'},\n",
       " {'role': 'assistant',\n",
       "  'content': \"There are several metrics that can be used to evaluate the performance and quality of generated text from Lamini models, including perplexity, BLEU score, and human evaluation. Perplexity measures how well the model predicts the next word in a sequence, while BLEU score measures the similarity between the generated text and a reference text. Human evaluation involves having human judges rate the quality of the generated text based on factors such as coherence, fluency, and relevance. It is recommended to use a combination of these metrics for a comprehensive evaluation of the model's performance.\"}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_jsonl(path):\n",
    "    conversations = []\n",
    "    with open(path, 'r') as file:\n",
    "        data = [json.loads(line) for line in file]\n",
    "        conversations = [dialog['messages'] for dialog in data]\n",
    "        return conversations\n",
    "\n",
    "eval_data_path = 'gptq_data_chat_format.jsonl'\n",
    "conversations = load_jsonl(eval_data_path)\n",
    "\n",
    "conversations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a402fd03-9974-40f5-8ffe-f9937666faea",
   "metadata": {},
   "source": [
    "定义一个预处理函数，将文本数据预处理为张量数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59b791e2-965a-42cc-8610-923c40f9d935",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset, max_len=2048):\n",
    "    data = []\n",
    "    for msg in dataset:\n",
    "        text = tokenizer.apply_chat_template(msg, tokenize=False, add_generation_prompt=False)\n",
    "        data.append(text.strip())\n",
    "    return data\n",
    "\n",
    "dataset = preprocess(conversations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c298863-229f-45ec-bc38-5253c251b15c",
   "metadata": {},
   "source": [
    "配置日志显示格式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9429ea5d-2ea5-4a8a-82fb-527bd8dc5905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s [%(name)s] %(message)s\", level=logging.INFO, datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629ae543-a3bc-48ac-944a-2a1366acde70",
   "metadata": {},
   "source": [
    "使用校准数据集来动态调整量化参数，使模型在量化时学习并适应数据分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57afe544-5463-4e10-b191-50a9d1f76bc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AWQ:   0%|          | 0/28 [00:00<?, ?it/s]The attention layers in this model are transitioning from computing the RoPE embeddings internally through `position_ids` (2D tensor with the indexes of the tokens), to using externally computed `position_embeddings` (Tuple of tensors, containing cos and sin). In v4.46 `position_ids` will be removed and `position_embeddings` will be mandatory.\n",
      "AWQ: 100%|██████████| 28/28 [04:18<00:00,  9.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 23s, sys: 23 s, total: 5min 46s\n",
      "Wall time: 4min 18s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.quantize(tokenizer, quant_config=quant_config, calib_data=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838e6079-5799-46eb-8f5d-3b2d96b4a1be",
   "metadata": {},
   "source": [
    "保存量化后的模型和分词器状态。\n",
    "\n",
    "- quant_path指定了量化模型的保存路径。\n",
    "- use_safetensors=True 参数表示使用安全张量格式（SafeTensors）进行保存，具有更好的安全性和性能。\n",
    "- tokenizer.save_pretrained为量化后的模型保存一份分词器配置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95f9dcc6-a98b-48f9-960a-45f7f8143043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note that `shard_checkpoint` is deprecated and will be removed in v4.44. We recommend you using split_torch_state_dict_into_shards from huggingface_hub library\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./Qwen2-1__5B-Instruct-gptq-int4/tokenizer_config.json',\n",
       " './Qwen2-1__5B-Instruct-gptq-int4/special_tokens_map.json',\n",
       " './Qwen2-1__5B-Instruct-gptq-int4/vocab.json',\n",
       " './Qwen2-1__5B-Instruct-gptq-int4/merges.txt',\n",
       " './Qwen2-1__5B-Instruct-gptq-int4/added_tokens.json',\n",
       " './Qwen2-1__5B-Instruct-gptq-int4/tokenizer.json')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_path = \"./Qwen2-1__5B-Instruct-AWQ\"\n",
    "model.save_quantized(quant_path, safetensors=True, shard_size=\"4GB\")\n",
    "tokenizer.save_pretrained(quant_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf033012-539c-4cf0-874a-061c18804bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "自然语言处理（Natural Language Processing，简称 NLP）是一门计算机科学和人工智能领域的分支学科，研究人如何与计算机进行交互，以及计算机如何理解人类的自然语言。NLP的目标是使机器能够“看懂”并理解人类的语言，从而实现文本识别、文本分析、文本生成等功能。\n",
      "\n",
      "NLP主要涉及两个方面：一是语法分析，即对句子结构进行解析；二是语义分析，即理解句子的意义。此外，NLP还包括语音识别、语音合成等技术，用于将文本转换为语音或从语音中提取信息。\n",
      "\n",
      "NLP的应用领域非常广泛，包括但不限于聊天机器人、智能客服、自动翻译、文本摘要、情感分析、垃圾邮件过滤、新闻摘要等。随着技术的发展，NLP正在改变我们的生活，帮助我们更好地理解和使用语言。\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"./Qwen2-1__5B-Instruct-AWQ\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, \n",
    "    device_map=\"auto\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"简单介绍一下什么是NLP？\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6eb6141-a23b-4a93-8ca8-05002955d523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Replacing layers...: 100%|██████████| 28/28 [00:08<00:00,  3.31it/s]\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
      "            553,  54364,  14817,     13,   1446,    525,    264,  10950,  17847,\n",
      "             13, 151645,    198, 151644,    872,    198, 100405, 109432, 106582,\n",
      "             45,  12567,  11319, 151645,    198, 151644,  77091,    198,  99795,\n",
      "         102064,  54542,   9909,  54281,  11434,  28125,     11,    451,  12567,\n",
      "           7552,  20412, 104455, 104111, 103799,   3837,  99652,  99556, 104564,\n",
      "         100007, 115167,  54542, 103971,  99795, 102064,   9909,  29524, 104105,\n",
      "           5373, 109023,   5373,   8903,  72881,  49567,  74276,     45,  12567,\n",
      "         104820,  20412,  32555, 104564, 100006, 101128,   5373, 104136,   5373,\n",
      "          43959,  33108,  54542, 103971,  99795, 102064,   3837, 101982, 101884,\n",
      "          17340,  32648, 108221,   1773,     45,  12567,  99361,  73670, 110645,\n",
      "         100694, 100650,   3837,  29524, 102182, 105395,   5373, 108704,  70538,\n",
      "           5373, 104934, 101042,   5373, 111436,  72448,   5373, 105292, 104354,\n",
      "          49567,   1773, 151645]], device='cuda:0')\n",
      "system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
      "user\n",
      "简单介绍一下什么是NLP？\n",
      "assistant\n",
      "自然语言处理（Natural Language Processing, NLP）是人工智能的一个分支，它研究计算机如何理解和处理人类自然语言（如英语、汉语、日语等）。NLP的目标是使计算机能够理解、解释、生成和处理人类自然语言，从而实现人机交互。NLP技术可以应用于许多领域，如机器翻译、文本分类、情感分析、问答系统、聊天机器人等。\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TextStreamer\n",
    "from awq import AutoAWQForCausalLM\n",
    "\n",
    "model_name = \"./Qwen2-1__5B-Instruct-AWQ\"\n",
    "\n",
    "model = AutoAWQForCausalLM.from_quantized(model_name, fuse_layers=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "prompt = \"简单介绍一下什么是NLP？\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt},\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").input_ids.cuda()\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs,\n",
    "    max_new_tokens=512,\n",
    ")\n",
    "\n",
    "print(generated_ids)\n",
    "# generated_ids = [\n",
    "#     output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "# ]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba09014",
   "metadata": {},
   "source": [
    "- https://qwen.readthedocs.io/zh-cn/latest/quantization/awq.html#quantize-your-own-model-with-autoawq\n",
    "- https://github.com/casper-hansen/AutoAWQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5877ada4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
