{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 导库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: bitsandbytes in e:\\miniconda3\\lib\\site-packages (0.43.3)\n",
      "Requirement already satisfied: torch in e:\\miniconda3\\lib\\site-packages (from bitsandbytes) (2.1.1+cu118)\n",
      "Requirement already satisfied: numpy in e:\\miniconda3\\lib\\site-packages (from bitsandbytes) (1.24.4)\n",
      "Requirement already satisfied: filelock in e:\\miniconda3\\lib\\site-packages (from torch->bitsandbytes) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in e:\\miniconda3\\lib\\site-packages (from torch->bitsandbytes) (4.12.2)\n",
      "Requirement already satisfied: sympy in e:\\miniconda3\\lib\\site-packages (from torch->bitsandbytes) (1.12)\n",
      "Requirement already satisfied: networkx in e:\\miniconda3\\lib\\site-packages (from torch->bitsandbytes) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in e:\\miniconda3\\lib\\site-packages (from torch->bitsandbytes) (3.1.2)\n",
      "Requirement already satisfied: fsspec in e:\\miniconda3\\lib\\site-packages (from torch->bitsandbytes) (2023.10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\miniconda3\\lib\\site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in e:\\miniconda3\\lib\\site-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at e:\\miniconda3\\lib\\site-packages\\whisper_live-0.0.11-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig, BitsAndBytesConfig\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "{'instruction': '保持健康的三个提示。', 'input': '', 'output': '以下是保持健康的三个提示：\\n\\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\\n\\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\\n\\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。'}\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json('data/data.json')\n",
    "ds = Dataset.from_pandas(df)\n",
    "print(len(ds))\n",
    "print(ds[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# 模型下载：https://huggingface.co/Qwen\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen2-0.5B-Instruct', use_fast=False, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 384    # 分词器会将一个中文字切分为多个token，因此需要放开一些最大长度，保证数据的完整性\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(f\"<|im_start|>system\\n你是一个有用的助手<|im_end|>\\n<|im_start|>user\\n{example['instruction'] + example['input']}<|im_end|>\\n<|im_start|>assistant\\n\", add_special_tokens=False)  # add_special_tokens 不在开头加 special_tokens\n",
    "    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]  # 因为eos token也是要关注所以补充为1\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]  \n",
    "    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c541190c7004ec6bd85106bb4b7d8b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_id = ds.map(process_func, remove_columns=ds.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[151644,\n",
       " 8948,\n",
       " 198,\n",
       " 56568,\n",
       " 101909,\n",
       " 115405,\n",
       " 110498,\n",
       " 151645,\n",
       " 198,\n",
       " 151644,\n",
       " 872,\n",
       " 198,\n",
       " 100662,\n",
       " 108136,\n",
       " 101124,\n",
       " 45139,\n",
       " 1773,\n",
       " 151645,\n",
       " 198,\n",
       " 151644,\n",
       " 77091,\n",
       " 198,\n",
       " 114566,\n",
       " 100662,\n",
       " 108136,\n",
       " 101124,\n",
       " 45139,\n",
       " 48443,\n",
       " 16,\n",
       " 13,\n",
       " 220,\n",
       " 100662,\n",
       " 101099,\n",
       " 99600,\n",
       " 1773,\n",
       " 101922,\n",
       " 99190,\n",
       " 102618,\n",
       " 106214,\n",
       " 101079,\n",
       " 3837,\n",
       " 29524,\n",
       " 111261,\n",
       " 5373,\n",
       " 107530,\n",
       " 57191,\n",
       " 107140,\n",
       " 3837,\n",
       " 26232,\n",
       " 101902,\n",
       " 114718,\n",
       " 99722,\n",
       " 3837,\n",
       " 101138,\n",
       " 105640,\n",
       " 101102,\n",
       " 90395,\n",
       " 105767,\n",
       " 101940,\n",
       " 107235,\n",
       " 3407,\n",
       " 17,\n",
       " 13,\n",
       " 4891,\n",
       " 251,\n",
       " 229,\n",
       " 99967,\n",
       " 104579,\n",
       " 1773,\n",
       " 101922,\n",
       " 105086,\n",
       " 104838,\n",
       " 9370,\n",
       " 104451,\n",
       " 5373,\n",
       " 104618,\n",
       " 5373,\n",
       " 35987,\n",
       " 100203,\n",
       " 52853,\n",
       " 33108,\n",
       " 105349,\n",
       " 104982,\n",
       " 99285,\n",
       " 9370,\n",
       " 107151,\n",
       " 102153,\n",
       " 3837,\n",
       " 101153,\n",
       " 44636,\n",
       " 100443,\n",
       " 5373,\n",
       " 44636,\n",
       " 105349,\n",
       " 33108,\n",
       " 101130,\n",
       " 101083,\n",
       " 3837,\n",
       " 23031,\n",
       " 100662,\n",
       " 108136,\n",
       " 104579,\n",
       " 100784,\n",
       " 3407,\n",
       " 18,\n",
       " 13,\n",
       " 10236,\n",
       " 251,\n",
       " 94,\n",
       " 101519,\n",
       " 103119,\n",
       " 1773,\n",
       " 105552,\n",
       " 113357,\n",
       " 99722,\n",
       " 107940,\n",
       " 3837,\n",
       " 113459,\n",
       " 101922,\n",
       " 50511,\n",
       " 101907,\n",
       " 220,\n",
       " 22,\n",
       " 12,\n",
       " 23,\n",
       " 58230,\n",
       " 237,\n",
       " 13343,\n",
       " 9370,\n",
       " 105552,\n",
       " 1773,\n",
       " 104205,\n",
       " 105552,\n",
       " 105767,\n",
       " 106104,\n",
       " 101950,\n",
       " 3837,\n",
       " 101902,\n",
       " 101099,\n",
       " 102005,\n",
       " 90395,\n",
       " 100627,\n",
       " 108260,\n",
       " 33108,\n",
       " 118836,\n",
       " 1773,\n",
       " 151643]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_id[0]['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|>system\\n你是一个有用的助手<|im_end|>\\n<|im_start|>user\\n保持健康的三个提示。<|im_end|>\\n<|im_start|>assistant\\n以下是保持健康的三个提示：\\n\\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\\n\\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\\n\\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。<|endoftext|>'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenized_id[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'以下是保持健康的三个提示：\\n\\n1. 保持身体活动。每天做适当的身体运动，如散步、跑步或游泳，能促进心血管健康，增强肌肉力量，并有助于减少体重。\\n\\n2. 均衡饮食。每天食用新鲜的蔬菜、水果、全谷物和脂肪含量低的蛋白质食物，避免高糖、高脂肪和加工食品，以保持健康的饮食习惯。\\n\\n3. 睡眠充足。睡眠对人体健康至关重要，成年人每天应保证 7-8 小时的睡眠。良好的睡眠有助于减轻压力，促进身体恢复，并提高注意力和记忆力。<|endoftext|>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(list(filter(lambda x: x != -100, tokenized_id[0][\"labels\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activate 4-bit precision base model loading\n",
    "use_4bit = True\n",
    "# Compute dtype for 4-bit base models\n",
    "bnb_4bit_compute_dtype = \"float16\"\n",
    "# Quantization type (fp4 or nf4)\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = False\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "     load_in_4bit=use_4bit,\n",
    "     bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "     bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n",
    "     bnb_4bit_use_double_quant=use_nested_quant,\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('Qwen2-0.5B-Instruct', device_map=\"auto\", quantization_config=bnb_config) # 以BF16精度加载，节省显存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 896)\n",
       "    (layers): ModuleList(\n",
       "      (0-23): 24 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=896, out_features=896, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=896, out_features=128, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=896, out_features=896, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=896, out_features=4864, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=4864, out_features=896, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm()\n",
       "        (post_attention_layernorm): Qwen2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=896, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enable_input_require_grads() # 开启梯度检查点，具体解释： https://blog.csdn.net/qq_30438779/article/details/135229610"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight\n",
      "model.layers.0.self_attn.q_proj.weight\n",
      "model.layers.0.self_attn.q_proj.bias\n",
      "model.layers.0.self_attn.k_proj.weight\n",
      "model.layers.0.self_attn.k_proj.bias\n",
      "model.layers.0.self_attn.v_proj.weight\n",
      "model.layers.0.self_attn.v_proj.bias\n",
      "model.layers.0.self_attn.o_proj.weight\n",
      "model.layers.0.mlp.gate_proj.weight\n",
      "model.layers.0.mlp.up_proj.weight\n",
      "model.layers.0.mlp.down_proj.weight\n",
      "model.layers.0.input_layernorm.weight\n",
      "model.layers.0.post_attention_layernorm.weight\n",
      "model.layers.1.self_attn.q_proj.weight\n",
      "model.layers.1.self_attn.q_proj.bias\n",
      "model.layers.1.self_attn.k_proj.weight\n",
      "model.layers.1.self_attn.k_proj.bias\n",
      "model.layers.1.self_attn.v_proj.weight\n",
      "model.layers.1.self_attn.v_proj.bias\n",
      "model.layers.1.self_attn.o_proj.weight\n",
      "model.layers.1.mlp.gate_proj.weight\n",
      "model.layers.1.mlp.up_proj.weight\n",
      "model.layers.1.mlp.down_proj.weight\n",
      "model.layers.1.input_layernorm.weight\n",
      "model.layers.1.post_attention_layernorm.weight\n",
      "model.layers.2.self_attn.q_proj.weight\n",
      "model.layers.2.self_attn.q_proj.bias\n",
      "model.layers.2.self_attn.k_proj.weight\n",
      "model.layers.2.self_attn.k_proj.bias\n",
      "model.layers.2.self_attn.v_proj.weight\n",
      "model.layers.2.self_attn.v_proj.bias\n",
      "model.layers.2.self_attn.o_proj.weight\n",
      "model.layers.2.mlp.gate_proj.weight\n",
      "model.layers.2.mlp.up_proj.weight\n",
      "model.layers.2.mlp.down_proj.weight\n",
      "model.layers.2.input_layernorm.weight\n",
      "model.layers.2.post_attention_layernorm.weight\n",
      "model.layers.3.self_attn.q_proj.weight\n",
      "model.layers.3.self_attn.q_proj.bias\n",
      "model.layers.3.self_attn.k_proj.weight\n",
      "model.layers.3.self_attn.k_proj.bias\n",
      "model.layers.3.self_attn.v_proj.weight\n",
      "model.layers.3.self_attn.v_proj.bias\n",
      "model.layers.3.self_attn.o_proj.weight\n",
      "model.layers.3.mlp.gate_proj.weight\n",
      "model.layers.3.mlp.up_proj.weight\n",
      "model.layers.3.mlp.down_proj.weight\n",
      "model.layers.3.input_layernorm.weight\n",
      "model.layers.3.post_attention_layernorm.weight\n",
      "model.layers.4.self_attn.q_proj.weight\n",
      "model.layers.4.self_attn.q_proj.bias\n",
      "model.layers.4.self_attn.k_proj.weight\n",
      "model.layers.4.self_attn.k_proj.bias\n",
      "model.layers.4.self_attn.v_proj.weight\n",
      "model.layers.4.self_attn.v_proj.bias\n",
      "model.layers.4.self_attn.o_proj.weight\n",
      "model.layers.4.mlp.gate_proj.weight\n",
      "model.layers.4.mlp.up_proj.weight\n",
      "model.layers.4.mlp.down_proj.weight\n",
      "model.layers.4.input_layernorm.weight\n",
      "model.layers.4.post_attention_layernorm.weight\n",
      "model.layers.5.self_attn.q_proj.weight\n",
      "model.layers.5.self_attn.q_proj.bias\n",
      "model.layers.5.self_attn.k_proj.weight\n",
      "model.layers.5.self_attn.k_proj.bias\n",
      "model.layers.5.self_attn.v_proj.weight\n",
      "model.layers.5.self_attn.v_proj.bias\n",
      "model.layers.5.self_attn.o_proj.weight\n",
      "model.layers.5.mlp.gate_proj.weight\n",
      "model.layers.5.mlp.up_proj.weight\n",
      "model.layers.5.mlp.down_proj.weight\n",
      "model.layers.5.input_layernorm.weight\n",
      "model.layers.5.post_attention_layernorm.weight\n",
      "model.layers.6.self_attn.q_proj.weight\n",
      "model.layers.6.self_attn.q_proj.bias\n",
      "model.layers.6.self_attn.k_proj.weight\n",
      "model.layers.6.self_attn.k_proj.bias\n",
      "model.layers.6.self_attn.v_proj.weight\n",
      "model.layers.6.self_attn.v_proj.bias\n",
      "model.layers.6.self_attn.o_proj.weight\n",
      "model.layers.6.mlp.gate_proj.weight\n",
      "model.layers.6.mlp.up_proj.weight\n",
      "model.layers.6.mlp.down_proj.weight\n",
      "model.layers.6.input_layernorm.weight\n",
      "model.layers.6.post_attention_layernorm.weight\n",
      "model.layers.7.self_attn.q_proj.weight\n",
      "model.layers.7.self_attn.q_proj.bias\n",
      "model.layers.7.self_attn.k_proj.weight\n",
      "model.layers.7.self_attn.k_proj.bias\n",
      "model.layers.7.self_attn.v_proj.weight\n",
      "model.layers.7.self_attn.v_proj.bias\n",
      "model.layers.7.self_attn.o_proj.weight\n",
      "model.layers.7.mlp.gate_proj.weight\n",
      "model.layers.7.mlp.up_proj.weight\n",
      "model.layers.7.mlp.down_proj.weight\n",
      "model.layers.7.input_layernorm.weight\n",
      "model.layers.7.post_attention_layernorm.weight\n",
      "model.layers.8.self_attn.q_proj.weight\n",
      "model.layers.8.self_attn.q_proj.bias\n",
      "model.layers.8.self_attn.k_proj.weight\n",
      "model.layers.8.self_attn.k_proj.bias\n",
      "model.layers.8.self_attn.v_proj.weight\n",
      "model.layers.8.self_attn.v_proj.bias\n",
      "model.layers.8.self_attn.o_proj.weight\n",
      "model.layers.8.mlp.gate_proj.weight\n",
      "model.layers.8.mlp.up_proj.weight\n",
      "model.layers.8.mlp.down_proj.weight\n",
      "model.layers.8.input_layernorm.weight\n",
      "model.layers.8.post_attention_layernorm.weight\n",
      "model.layers.9.self_attn.q_proj.weight\n",
      "model.layers.9.self_attn.q_proj.bias\n",
      "model.layers.9.self_attn.k_proj.weight\n",
      "model.layers.9.self_attn.k_proj.bias\n",
      "model.layers.9.self_attn.v_proj.weight\n",
      "model.layers.9.self_attn.v_proj.bias\n",
      "model.layers.9.self_attn.o_proj.weight\n",
      "model.layers.9.mlp.gate_proj.weight\n",
      "model.layers.9.mlp.up_proj.weight\n",
      "model.layers.9.mlp.down_proj.weight\n",
      "model.layers.9.input_layernorm.weight\n",
      "model.layers.9.post_attention_layernorm.weight\n",
      "model.layers.10.self_attn.q_proj.weight\n",
      "model.layers.10.self_attn.q_proj.bias\n",
      "model.layers.10.self_attn.k_proj.weight\n",
      "model.layers.10.self_attn.k_proj.bias\n",
      "model.layers.10.self_attn.v_proj.weight\n",
      "model.layers.10.self_attn.v_proj.bias\n",
      "model.layers.10.self_attn.o_proj.weight\n",
      "model.layers.10.mlp.gate_proj.weight\n",
      "model.layers.10.mlp.up_proj.weight\n",
      "model.layers.10.mlp.down_proj.weight\n",
      "model.layers.10.input_layernorm.weight\n",
      "model.layers.10.post_attention_layernorm.weight\n",
      "model.layers.11.self_attn.q_proj.weight\n",
      "model.layers.11.self_attn.q_proj.bias\n",
      "model.layers.11.self_attn.k_proj.weight\n",
      "model.layers.11.self_attn.k_proj.bias\n",
      "model.layers.11.self_attn.v_proj.weight\n",
      "model.layers.11.self_attn.v_proj.bias\n",
      "model.layers.11.self_attn.o_proj.weight\n",
      "model.layers.11.mlp.gate_proj.weight\n",
      "model.layers.11.mlp.up_proj.weight\n",
      "model.layers.11.mlp.down_proj.weight\n",
      "model.layers.11.input_layernorm.weight\n",
      "model.layers.11.post_attention_layernorm.weight\n",
      "model.layers.12.self_attn.q_proj.weight\n",
      "model.layers.12.self_attn.q_proj.bias\n",
      "model.layers.12.self_attn.k_proj.weight\n",
      "model.layers.12.self_attn.k_proj.bias\n",
      "model.layers.12.self_attn.v_proj.weight\n",
      "model.layers.12.self_attn.v_proj.bias\n",
      "model.layers.12.self_attn.o_proj.weight\n",
      "model.layers.12.mlp.gate_proj.weight\n",
      "model.layers.12.mlp.up_proj.weight\n",
      "model.layers.12.mlp.down_proj.weight\n",
      "model.layers.12.input_layernorm.weight\n",
      "model.layers.12.post_attention_layernorm.weight\n",
      "model.layers.13.self_attn.q_proj.weight\n",
      "model.layers.13.self_attn.q_proj.bias\n",
      "model.layers.13.self_attn.k_proj.weight\n",
      "model.layers.13.self_attn.k_proj.bias\n",
      "model.layers.13.self_attn.v_proj.weight\n",
      "model.layers.13.self_attn.v_proj.bias\n",
      "model.layers.13.self_attn.o_proj.weight\n",
      "model.layers.13.mlp.gate_proj.weight\n",
      "model.layers.13.mlp.up_proj.weight\n",
      "model.layers.13.mlp.down_proj.weight\n",
      "model.layers.13.input_layernorm.weight\n",
      "model.layers.13.post_attention_layernorm.weight\n",
      "model.layers.14.self_attn.q_proj.weight\n",
      "model.layers.14.self_attn.q_proj.bias\n",
      "model.layers.14.self_attn.k_proj.weight\n",
      "model.layers.14.self_attn.k_proj.bias\n",
      "model.layers.14.self_attn.v_proj.weight\n",
      "model.layers.14.self_attn.v_proj.bias\n",
      "model.layers.14.self_attn.o_proj.weight\n",
      "model.layers.14.mlp.gate_proj.weight\n",
      "model.layers.14.mlp.up_proj.weight\n",
      "model.layers.14.mlp.down_proj.weight\n",
      "model.layers.14.input_layernorm.weight\n",
      "model.layers.14.post_attention_layernorm.weight\n",
      "model.layers.15.self_attn.q_proj.weight\n",
      "model.layers.15.self_attn.q_proj.bias\n",
      "model.layers.15.self_attn.k_proj.weight\n",
      "model.layers.15.self_attn.k_proj.bias\n",
      "model.layers.15.self_attn.v_proj.weight\n",
      "model.layers.15.self_attn.v_proj.bias\n",
      "model.layers.15.self_attn.o_proj.weight\n",
      "model.layers.15.mlp.gate_proj.weight\n",
      "model.layers.15.mlp.up_proj.weight\n",
      "model.layers.15.mlp.down_proj.weight\n",
      "model.layers.15.input_layernorm.weight\n",
      "model.layers.15.post_attention_layernorm.weight\n",
      "model.layers.16.self_attn.q_proj.weight\n",
      "model.layers.16.self_attn.q_proj.bias\n",
      "model.layers.16.self_attn.k_proj.weight\n",
      "model.layers.16.self_attn.k_proj.bias\n",
      "model.layers.16.self_attn.v_proj.weight\n",
      "model.layers.16.self_attn.v_proj.bias\n",
      "model.layers.16.self_attn.o_proj.weight\n",
      "model.layers.16.mlp.gate_proj.weight\n",
      "model.layers.16.mlp.up_proj.weight\n",
      "model.layers.16.mlp.down_proj.weight\n",
      "model.layers.16.input_layernorm.weight\n",
      "model.layers.16.post_attention_layernorm.weight\n",
      "model.layers.17.self_attn.q_proj.weight\n",
      "model.layers.17.self_attn.q_proj.bias\n",
      "model.layers.17.self_attn.k_proj.weight\n",
      "model.layers.17.self_attn.k_proj.bias\n",
      "model.layers.17.self_attn.v_proj.weight\n",
      "model.layers.17.self_attn.v_proj.bias\n",
      "model.layers.17.self_attn.o_proj.weight\n",
      "model.layers.17.mlp.gate_proj.weight\n",
      "model.layers.17.mlp.up_proj.weight\n",
      "model.layers.17.mlp.down_proj.weight\n",
      "model.layers.17.input_layernorm.weight\n",
      "model.layers.17.post_attention_layernorm.weight\n",
      "model.layers.18.self_attn.q_proj.weight\n",
      "model.layers.18.self_attn.q_proj.bias\n",
      "model.layers.18.self_attn.k_proj.weight\n",
      "model.layers.18.self_attn.k_proj.bias\n",
      "model.layers.18.self_attn.v_proj.weight\n",
      "model.layers.18.self_attn.v_proj.bias\n",
      "model.layers.18.self_attn.o_proj.weight\n",
      "model.layers.18.mlp.gate_proj.weight\n",
      "model.layers.18.mlp.up_proj.weight\n",
      "model.layers.18.mlp.down_proj.weight\n",
      "model.layers.18.input_layernorm.weight\n",
      "model.layers.18.post_attention_layernorm.weight\n",
      "model.layers.19.self_attn.q_proj.weight\n",
      "model.layers.19.self_attn.q_proj.bias\n",
      "model.layers.19.self_attn.k_proj.weight\n",
      "model.layers.19.self_attn.k_proj.bias\n",
      "model.layers.19.self_attn.v_proj.weight\n",
      "model.layers.19.self_attn.v_proj.bias\n",
      "model.layers.19.self_attn.o_proj.weight\n",
      "model.layers.19.mlp.gate_proj.weight\n",
      "model.layers.19.mlp.up_proj.weight\n",
      "model.layers.19.mlp.down_proj.weight\n",
      "model.layers.19.input_layernorm.weight\n",
      "model.layers.19.post_attention_layernorm.weight\n",
      "model.layers.20.self_attn.q_proj.weight\n",
      "model.layers.20.self_attn.q_proj.bias\n",
      "model.layers.20.self_attn.k_proj.weight\n",
      "model.layers.20.self_attn.k_proj.bias\n",
      "model.layers.20.self_attn.v_proj.weight\n",
      "model.layers.20.self_attn.v_proj.bias\n",
      "model.layers.20.self_attn.o_proj.weight\n",
      "model.layers.20.mlp.gate_proj.weight\n",
      "model.layers.20.mlp.up_proj.weight\n",
      "model.layers.20.mlp.down_proj.weight\n",
      "model.layers.20.input_layernorm.weight\n",
      "model.layers.20.post_attention_layernorm.weight\n",
      "model.layers.21.self_attn.q_proj.weight\n",
      "model.layers.21.self_attn.q_proj.bias\n",
      "model.layers.21.self_attn.k_proj.weight\n",
      "model.layers.21.self_attn.k_proj.bias\n",
      "model.layers.21.self_attn.v_proj.weight\n",
      "model.layers.21.self_attn.v_proj.bias\n",
      "model.layers.21.self_attn.o_proj.weight\n",
      "model.layers.21.mlp.gate_proj.weight\n",
      "model.layers.21.mlp.up_proj.weight\n",
      "model.layers.21.mlp.down_proj.weight\n",
      "model.layers.21.input_layernorm.weight\n",
      "model.layers.21.post_attention_layernorm.weight\n",
      "model.layers.22.self_attn.q_proj.weight\n",
      "model.layers.22.self_attn.q_proj.bias\n",
      "model.layers.22.self_attn.k_proj.weight\n",
      "model.layers.22.self_attn.k_proj.bias\n",
      "model.layers.22.self_attn.v_proj.weight\n",
      "model.layers.22.self_attn.v_proj.bias\n",
      "model.layers.22.self_attn.o_proj.weight\n",
      "model.layers.22.mlp.gate_proj.weight\n",
      "model.layers.22.mlp.up_proj.weight\n",
      "model.layers.22.mlp.down_proj.weight\n",
      "model.layers.22.input_layernorm.weight\n",
      "model.layers.22.post_attention_layernorm.weight\n",
      "model.layers.23.self_attn.q_proj.weight\n",
      "model.layers.23.self_attn.q_proj.bias\n",
      "model.layers.23.self_attn.k_proj.weight\n",
      "model.layers.23.self_attn.k_proj.bias\n",
      "model.layers.23.self_attn.v_proj.weight\n",
      "model.layers.23.self_attn.v_proj.bias\n",
      "model.layers.23.self_attn.o_proj.weight\n",
      "model.layers.23.mlp.gate_proj.weight\n",
      "model.layers.23.mlp.up_proj.weight\n",
      "model.layers.23.mlp.down_proj.weight\n",
      "model.layers.23.input_layernorm.weight\n",
      "model.layers.23.post_attention_layernorm.weight\n",
      "model.norm.weight\n"
     ]
    }
   ],
   "source": [
    "# 查看所有的块和名称\n",
    "for name,param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"], # 选择合适的target_modules：https://github.com/huggingface/peft/blob/main/src/peft/utils/constants.py#L78\n",
    "    inference_mode=False, # 训练模式\n",
    "    r=8, # LoRA 秩大小\n",
    "    lora_alpha=32, # LoRA alaph，具体作用参见 LoRA 原理\n",
    "    lora_dropout=0.1 # Dropout 比例\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 540,672 || all params: 494,573,440 || trainable%: 0.1093\n"
     ]
    }
   ],
   "source": [
    "# 查看可训练的模型参数占比\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置训练参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir=\"save_checkpoint\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_steps=10,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=100,\n",
    "    learning_rate=1e-3,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"tensorboard\"\n",
    ")\n",
    "\n",
    "# 更多可设置参数：https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_id,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_PROJECT\"] = \"qwen2-qlora\"\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'qwen2-sft-qlora'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ef7504165044cc4bc485f1bb924dedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/187 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "e:\\miniconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9754, 'grad_norm': 0.8251539468765259, 'learning_rate': 0.000946524064171123, 'epoch': 0.05}\n",
      "{'loss': 1.9106, 'grad_norm': 0.796172559261322, 'learning_rate': 0.000893048128342246, 'epoch': 0.11}\n",
      "{'loss': 1.7789, 'grad_norm': 0.7549453377723694, 'learning_rate': 0.000839572192513369, 'epoch': 0.16}\n",
      "{'loss': 1.8517, 'grad_norm': 0.6903822422027588, 'learning_rate': 0.000786096256684492, 'epoch': 0.21}\n",
      "{'loss': 1.898, 'grad_norm': 0.8567588329315186, 'learning_rate': 0.000732620320855615, 'epoch': 0.27}\n",
      "{'loss': 1.8581, 'grad_norm': 0.6746519207954407, 'learning_rate': 0.000679144385026738, 'epoch': 0.32}\n",
      "{'loss': 1.8805, 'grad_norm': 0.9380040764808655, 'learning_rate': 0.0006256684491978609, 'epoch': 0.37}\n",
      "{'loss': 1.9323, 'grad_norm': 0.747184157371521, 'learning_rate': 0.000572192513368984, 'epoch': 0.43}\n",
      "{'loss': 1.8087, 'grad_norm': 0.6952503323554993, 'learning_rate': 0.0005187165775401069, 'epoch': 0.48}\n",
      "{'loss': 1.8998, 'grad_norm': 0.8202732801437378, 'learning_rate': 0.00046524064171123, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\miniconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8931, 'grad_norm': 0.898560106754303, 'learning_rate': 0.0004117647058823529, 'epoch': 0.59}\n",
      "{'loss': 1.8978, 'grad_norm': 0.7028713822364807, 'learning_rate': 0.0003582887700534759, 'epoch': 0.64}\n",
      "{'loss': 1.8418, 'grad_norm': 0.678516149520874, 'learning_rate': 0.0003048128342245989, 'epoch': 0.69}\n",
      "{'loss': 1.9235, 'grad_norm': 0.7228291034698486, 'learning_rate': 0.0002513368983957219, 'epoch': 0.75}\n",
      "{'loss': 1.8747, 'grad_norm': 0.7668856978416443, 'learning_rate': 0.00019786096256684495, 'epoch': 0.8}\n",
      "{'loss': 1.8752, 'grad_norm': 0.8265697360038757, 'learning_rate': 0.0001443850267379679, 'epoch': 0.85}\n",
      "{'loss': 1.9551, 'grad_norm': 0.7666983008384705, 'learning_rate': 9.090909090909092e-05, 'epoch': 0.91}\n",
      "{'loss': 1.9415, 'grad_norm': 0.9352073669433594, 'learning_rate': 3.74331550802139e-05, 'epoch': 0.96}\n",
      "{'train_runtime': 327.0775, 'train_samples_per_second': 9.172, 'train_steps_per_second': 0.572, 'train_loss': 1.884782337249919, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=187, training_loss=1.884782337249919, metrics={'train_runtime': 327.0775, 'train_samples_per_second': 9.172, 'train_steps_per_second': 0.572, 'total_flos': 1641397289816064.0, 'train_loss': 1.884782337249919, 'epoch': 0.9973333333333333})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 开始训练\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我是阿里云开发的超大规模语言模型，我叫通义千问。\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "\n",
    "mode_path = 'Qwen2-0.5B-Instruct'\n",
    "lora_path = 'save_checkpoint/checkpoint-100' # 这里改称你的 lora 输出对应 checkpoint 地址\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(mode_path, trust_remote_code=True)\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(mode_path, device_map=\"auto\",torch_dtype=torch.bfloat16, trust_remote_code=True).eval()\n",
    "# 加载LoRA权重\n",
    "model = PeftModel.from_pretrained(model, model_id=lora_path)\n",
    "\n",
    "prompt = \"你是谁？\"\n",
    "inputs = tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": \"你是一个有用的助手\"},{\"role\": \"user\", \"content\": prompt}],\n",
    "                                       add_generation_prompt=True,\n",
    "                                       tokenize=True,\n",
    "                                       return_tensors=\"pt\",\n",
    "                                       return_dict=True\n",
    "                                       ).to('cuda')\n",
    "\n",
    "\n",
    "gen_kwargs = {\"max_length\": 2500, \"do_sample\": True, \"top_k\": 1}\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, **gen_kwargs)\n",
    "    outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 合并权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the base model from Qwen2-0.5B-Instruct\n",
      "Loading the LoRA adapter from save_checkpoint/checkpoint-100\n",
      "Applying the LoRA\n",
      "Saving the model weights to merge_checkpoint_bin/pytorch_model.bin\n",
      "Saving the tokenizer to merge_checkpoint_bin\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "\n",
    "def apply_lora(model_name_or_path, output_path, lora_path, model_format=\"safetensors\"):\n",
    "    print(f\"Loading the base model from {model_name_or_path}\")\n",
    "    base_tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False, trust_remote_code=True)\n",
    "    base = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"cuda:0\", torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "    # base.generation_config = GenerationConfig.from_pretrained(model_name_or_path)\n",
    "\n",
    "    print(f\"Loading the LoRA adapter from {lora_path}\")\n",
    " \n",
    "    lora_model = PeftModel.from_pretrained(\n",
    "        base,\n",
    "        lora_path,\n",
    "        torch_dtype=torch.float16,\n",
    "    )\n",
    " \n",
    "    print(\"Applying the LoRA\")\n",
    "    model = lora_model.merge_and_unload()\n",
    "    \n",
    "    if model_format==\"safetensors\":\n",
    "        print(f\"Saving the target model to {output_path}\")\n",
    "        model.save_pretrained(output_path)\n",
    "        base_tokenizer.save_pretrained(output_path)\n",
    "    if model_format==\"bin\":\n",
    "        # Save the model weights to a .bin file\n",
    "        bin_file_path = f\"{output_path}/pytorch_model.bin\"\n",
    "        print(f\"Saving the model weights to {bin_file_path}\")\n",
    "        torch.save(model.state_dict(), bin_file_path)\n",
    "\n",
    "        print(f\"Saving the tokenizer to {output_path}\")\n",
    "        base_tokenizer.save_pretrained(output_path)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    lora_path = 'save_checkpoint/checkpoint-100'\n",
    "    model_path = 'Qwen2-0.5B-Instruct'\n",
    "    output = 'merge_checkpoint'\n",
    "    model_format = 'safetensors'\n",
    "\n",
    "    apply_lora(model_path,output,lora_path,model_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推理微调模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"A large language model is an artificial intelligence system that can generate human-like text and respond to questions in natural language. These models are trained on vast amounts of data, which includes texts from various sources, such as social media, news articles, and scientific papers.\\nThe primary goal of a large language model is to generate human-like responses to a wide range of questions. It can be used for tasks such as answering trivia questions, providing answers to complex questions, and generating creative content.\\nOne of the key advantages of large language models is their ability to process vast amounts of information quickly and accurately. They can generate text at speeds up to several million words per second, making them ideal for many applications where speed is critical.\\nIn addition to their ability to generate human-like responses, large language models have also been shown to have valuable insights into a variety of topics, including language and communication. They can help businesses and organizations better understand their customers' needs and preferences, and make more informed decisions based on this information.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"merge_checkpoint\",\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"merge_checkpoint\")\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    model_inputs.input_ids,\n",
    "    max_new_tokens=512\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastAPI部署"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "部署代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Request\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "import uvicorn\n",
    "import json\n",
    "import datetime\n",
    "import torch\n",
    "\n",
    "# 设置设备参数\n",
    "DEVICE = \"cuda\"  # 使用CUDA\n",
    "DEVICE_ID = \"0\"  # CUDA设备ID，如果未设置则为空\n",
    "CUDA_DEVICE = f\"{DEVICE}:{DEVICE_ID}\" if DEVICE_ID else DEVICE  # 组合CUDA设备信息\n",
    "\n",
    "# 清理GPU内存函数\n",
    "def torch_gc():\n",
    "    if torch.cuda.is_available():  # 检查是否可用CUDA\n",
    "        with torch.cuda.device(CUDA_DEVICE):  # 指定CUDA设备\n",
    "            torch.cuda.empty_cache()  # 清空CUDA缓存\n",
    "            torch.cuda.ipc_collect()  # 收集CUDA内存碎片\n",
    "\n",
    "# 创建FastAPI应用\n",
    "app = FastAPI()\n",
    "\n",
    "# 处理POST请求的端点\n",
    "@app.post(\"/\")\n",
    "async def create_item(request: Request):\n",
    "    global model, tokenizer  # 声明全局变量以便在函数内部使用模型和分词器\n",
    "    json_post_raw = await request.json()  # 获取POST请求的JSON数据\n",
    "    json_post = json.dumps(json_post_raw)  # 将JSON数据转换为字符串\n",
    "    json_post_list = json.loads(json_post)  # 将字符串转换为Python对象\n",
    "    prompt = json_post_list.get('prompt')  # 获取请求中的提示\n",
    "\n",
    "    messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    # 调用模型进行对话生成\n",
    "    input_ids = tokenizer.apply_chat_template(messages,tokenize=False,add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([input_ids], return_tensors=\"pt\").to('cuda')\n",
    "    generated_ids = model.generate(model_inputs.input_ids,max_new_tokens=512)\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    now = datetime.datetime.now()  # 获取当前时间\n",
    "    time = now.strftime(\"%Y-%m-%d %H:%M:%S\")  # 格式化时间为字符串\n",
    "    # 构建响应JSON\n",
    "    answer = {\n",
    "        \"response\": response,\n",
    "        \"status\": 200,\n",
    "        \"time\": time\n",
    "    }\n",
    "    # 构建日志信息\n",
    "    log = \"[\" + time + \"] \" + '\", prompt:\"' + prompt + '\", response:\"' + repr(response) + '\"'\n",
    "    print(log)  # 打印日志\n",
    "    torch_gc()  # 执行GPU内存清理\n",
    "    return answer  # 返回响应\n",
    "\n",
    "# 主函数入口\n",
    "if __name__ == '__main__':\n",
    "    # 加载预训练的分词器和模型\n",
    "    model_name_or_path = \"merge_checkpoint\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=False)\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "    # 启动FastAPI应用\n",
    "    # 用6006端口可以将autodl的端口映射到本地，从而在本地使用api\n",
    "    uvicorn.run(app, host='127.0.0.1', port=6006, workers=1)  # 在指定端口和主机上启动应用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请求代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你好！有什么我可以帮助你的吗？\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def get_completion(prompt):\n",
    "    headers = {'Content-Type': 'application/json'}\n",
    "    data = {\"prompt\": prompt}\n",
    "    response = requests.post(url='http://127.0.0.1:6006', headers=headers, data=json.dumps(data))\n",
    "    return response.json()['response']\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(get_completion('你好'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：https://cloud.tencent.com/developer/article/2331992"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
