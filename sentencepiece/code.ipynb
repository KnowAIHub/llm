{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前，大语言模型呈爆发式的增长，其中，基于llama家族的模型占据了半壁江山。而原始的llama模型对中文的支持不太友好，接下来本文将讲解如何去扩充vocab里面的词以对中文进行token化。\n",
    "\n",
    "一般的，目前比较主流的是使用sentencepiece训练中文词库。安装指令也很简单：`pip install sentencepiece`。然后，我们准备好语料，这里我们使用的语料是斗破苍穹小说。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data.txt\", \"r\", encoding=\"utf-8\") as fp:\n",
    "    data = fp.read().strip().split(\"\\n\")\n",
    "sentences = []\n",
    "\n",
    "for d in data:\n",
    "    d = d.strip()\n",
    "    if \"===\" in d or len(d) == 0 or d == \"《斗破苍穹》来自:\":\n",
    "        continue\n",
    "    sentences.append(d)\n",
    "\n",
    "with open(\"corpus.txt\", \"w\", encoding=\"utf-8\") as fp:\n",
    "    fp.write(\"\\n\".join(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: sentencepiece in e:\\miniconda3\\lib\\site-packages (0.1.99)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at e:\\miniconda3\\lib\\site-packages\\whisper_live-0.0.11-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练，这里面有几个参数要注意一下，model_type分词算法选择bpe，split_digits为True，byte_fallback为True，和LLaMa 保持一致，max_sentence_length设置的大一点，更多参数解释可以查看：https://zhuanlan.zhihu.com/p/655281268 和 https://zhuanlan.zhihu.com/p/639144223"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "spm.SentencePieceTrainer.train(\n",
    "    input='corpus.txt',\n",
    "    input_format='text',\n",
    "    model_prefix='tokenizer',\n",
    "    vocab_size=10000,\n",
    "    character_coverage=0.9995,\n",
    "    model_type=\"bpe\",\n",
    "    num_threads=32,\n",
    "    split_digits=True,\n",
    "    byte_fallback=True,\n",
    "    max_sentence_length=24000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行上述训练过程，大概需要30S左右，会在当前目录下生成三个文件，tokenizer.model，tokenizer.vocab。看一下模型的分词效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', '<0x54>', 'h', 'e', '▁', 'e', 'x', 'c', 'e', 'l', 'l', 'e', 'n', 'c', 'e', '▁', 'o', 'f', '▁', 'a', '▁', 't', 'r', 'a', 'n', 's', 'l', 'a', 't', 'i', 'o', 'n', '▁', 'c', 'a', 'n', '▁', 'o', 'n', 'l', 'y', '▁', 'b', 'e', '▁', 'j', 'u', 'd', 'g', 'e', 'd', '▁', 'b', 'y', '▁', 'n', 'o', 't', 'i', 'n', 'g']\n",
      "分词长度: 61\n",
      "['▁', '<0xE9>', '<0xBA>', '<0x92>', '麟', ',', '是', '中', '国', '古', '代', '神', '话', '中', '的一种', '<0xE7>', '<0x91>', '<0x9E>', '兽']\n",
      "分词长度: 19\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "sp_bpe = spm.SentencePieceProcessor()\n",
    "\n",
    "sp_bpe.load('tokenizer.model')\n",
    "\n",
    "print(sp_bpe.encode_as_pieces('The excellence of a translation can only be judged by noting'))\n",
    "print('分词长度:', len(sp_bpe.encode_as_pieces('The excellence of a translation can only be judged by noting')))\n",
    "print(sp_bpe.encode_as_pieces('麒麟，是中国古代神话中的一种瑞兽'))\n",
    "print('分词长度:', len(sp_bpe.encode_as_pieces('麒麟，是中国古代神话中的一种瑞兽')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，因为训练语料几乎都是中文的，对中文的分词效果是好于英文的，中文常见的一些词都变成了一个token，而英文被分的很碎。接下里把这个词表和原生LLaMa的词表进行合并。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama2的词表大小为32000, 刚训练的模型的词表大小为10000\n",
      "['<s>', '</s>', '<unk>']\n",
      "[1, 2, 0]\n",
      "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"]=\"python\"\n",
    "from transformers import LlamaTokenizer\n",
    "from sentencepiece import sentencepiece_model_pb2 as sp_pb2_model\n",
    "import sentencepiece as spm\n",
    "\n",
    "# 位置\n",
    "llama_tokenizer_dir = \"llama2-7b-hf\" # llama2模型\n",
    "chinese_sp_model_file =\"tokenizer.model\" # 刚才训练的模型\n",
    "\n",
    "# 加载\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained(llama_tokenizer_dir)\n",
    "chinese_sp_model = spm.SentencePieceProcessor()\n",
    "chinese_sp_model.Load(chinese_sp_model_file)\n",
    "llama_spm = sp_pb2_model.ModelProto()\n",
    "llama_spm.ParseFromString(llama_tokenizer.sp_model.serialized_model_proto())\n",
    "chinese_spm = sp_pb2_model.ModelProto()\n",
    "chinese_spm.ParseFromString(chinese_sp_model.serialized_model_proto())\n",
    "\n",
    "# 打印两个词表的大小和原llama的特殊token\n",
    "print(f'llama2的词表大小为{len(llama_tokenizer)}, 刚训练的模型的词表大小为{len(chinese_sp_model)}')\n",
    "print(llama_tokenizer.all_special_tokens) # 特殊token\n",
    "print(llama_tokenizer.all_special_ids) # 特殊token对应的id\n",
    "print(llama_tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始往llama词表里添加，这里你也可以直接加入你想要加入词表的词，或者是领域内的特殊词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "添加词表前，词表大小为:32000\n",
      "新合并词表的大小为: 41013\n"
     ]
    }
   ],
   "source": [
    "llama_spm_tokens_set=set(p.piece for p in llama_spm.pieces)\n",
    "\n",
    "print(f\"添加词表前，词表大小为:{len(llama_spm_tokens_set)}\")\n",
    "for p in chinese_spm.pieces:\n",
    "    piece = p.piece\n",
    "    if piece not in llama_spm_tokens_set:\n",
    "        new_p = sp_pb2_model.ModelProto().SentencePiece()\n",
    "        new_p.piece = piece\n",
    "        new_p.score = 0\n",
    "        llama_spm.pieces.append(new_p)\n",
    "\n",
    "print(f\"新合并词表的大小为: {len(llama_spm.pieces)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "41013-32000=9013，可以大小9013跟我们训练的10000词不相等，这是因为合并过程会默认进行去重操作，去重后的新合并的词表大小为9013。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese-LLaMA tokenizer has been saved to merged_tokenizer_hf_test\n"
     ]
    }
   ],
   "source": [
    "# 保存合并后的模型\n",
    "output_sp_dir = 'merged_tokenizer_sp_test'\n",
    "output_hf_dir = 'merged_tokenizer_hf_test'\n",
    "\n",
    "os.makedirs(output_sp_dir,exist_ok=True)\n",
    "os.makedirs(output_hf_dir,exist_ok=True)\n",
    "with open(output_sp_dir+'/chinese_llama.model', 'wb') as f:\n",
    "    f.write(llama_spm.SerializeToString())\n",
    "tokenizer = LlamaTokenizer(vocab_file=output_sp_dir+'/chinese_llama.model')\n",
    "\n",
    "tokenizer.save_pretrained(output_hf_dir)\n",
    "print(f\"Chinese-LLaMA tokenizer has been saved to {output_hf_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始文本: The excellence of a translation can only be judged by noting\n",
      "llama进行token词表分割:['▁The', '▁excell', 'ence', '▁of', '▁a', '▁translation', '▁can', '▁only', '▁be', '▁jud', 'ged', '▁by', '▁not', 'ing']\n",
      "llama进行token词表长度为:14\n",
      "新合并的token模型词表分割:['The', '▁excell', 'ence', '▁of', '▁a', '▁translation', '▁can', '▁only', '▁be', '▁jud', 'ged', '▁by', '▁not', 'ing']\n",
      "新合并的token模型词表长度为:14\n"
     ]
    }
   ],
   "source": [
    "# 看一下效果\n",
    "llama_tokenizer = LlamaTokenizer.from_pretrained(llama_tokenizer_dir)\n",
    "chinese_llama_tokenizer = LlamaTokenizer.from_pretrained(output_hf_dir)\n",
    "\n",
    "text = \"The excellence of a translation can only be judged by noting\"\n",
    "print(\"原始文本:\",text)\n",
    "print(f\"llama进行token词表分割:{llama_tokenizer.tokenize(text)}\")\n",
    "print(f\"llama进行token词表长度为:{len(llama_tokenizer.tokenize(text))}\")\n",
    "print(f\"新合并的token模型词表分割:{chinese_llama_tokenizer.tokenize(text)}\")\n",
    "print(f\"新合并的token模型词表长度为:{len(chinese_llama_tokenizer.tokenize(text))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到在英文上是没有变化的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test text:\n",
      " 麒麟，是中国古代神话中的一种瑞兽\n",
      "原始文本: 麒麟，是中国古代神话中的一种瑞兽\n",
      "llama进行token词表分割:['▁', '<0xE9>', '<0xBA>', '<0x92>', '<0xE9>', '<0xBA>', '<0x9F>', '，', '是', '中', '国', '古', '代', '神', '话', '中', '的', '一', '种', '<0xE7>', '<0x91>', '<0x9E>', '<0xE5>', '<0x85>', '<0xBD>']\n",
      "llama进行token词表长度为:25\n",
      "新合并的token模型词表分割:['<0xE9>', '<0xBA>', '<0x92>', '麟', '，', '是', '中', '国', '古', '代', '神', '话', '中的', '一种', '<0xE7>', '<0x91>', '<0x9E>', '兽']\n",
      "新合并的token模型词表长度为:18\n"
     ]
    }
   ],
   "source": [
    "text = \"麒麟，是中国古代神话中的一种瑞兽\"\n",
    "print(\"Test text:\\n\",text)\n",
    "print(\"原始文本:\",text)\n",
    "print(f\"llama进行token词表分割:{llama_tokenizer.tokenize(text)}\")\n",
    "print(f\"llama进行token词表长度为:{len(llama_tokenizer.tokenize(text))}\")\n",
    "print(f\"新合并的token模型词表分割:{chinese_llama_tokenizer.tokenize(text)}\")\n",
    "print(f\"新合并的token模型词表长度为:{len(chinese_llama_tokenizer.tokenize(text))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此，我们完成了LLaMa中文词表的扩充，扩充垂直领域词表也是如此，要准备垂直领域的训练语料，最好和通用领域的训练语料混合一下"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
