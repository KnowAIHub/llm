{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基于SentencePiece扩充LLaMa的词表，但是扩充了词表后的下一步该怎么操作呢？如何将新增的token在模型的embedding层和lm_head层初始化呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "model_name = \"../sentencepiece/llama2-7b-hf\" # llama2模型的位置\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "new_tokenizer = AutoTokenizer.from_pretrained(\"../sentencepiece/merged_tokenizer_hf_test\") # 新训练的分词器的位置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们加载完模型和分词器以及新增的分词器后，看一下模型的结构：\n",
    "```python\n",
    "model\n",
    "LlamaForCausalLM(\n",
    "  (model): LlamaModel(\n",
    "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
    "    (layers): ModuleList(\n",
    "      (0-31): 32 x LlamaDecoderLayer(\n",
    "        (self_attn): LlamaAttention(\n",
    "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
    "          (rotary_emb): LlamaRotaryEmbedding()\n",
    "        )\n",
    "        (mlp): LlamaMLP(\n",
    "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
    "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
    "          (act_fn): SiLUActivation()\n",
    "        )\n",
    "        (input_layernorm): LlamaRMSNorm()\n",
    "        (post_attention_layernorm): LlamaRMSNorm()\n",
    "      )\n",
    "    )\n",
    "    (norm): LlamaRMSNorm()\n",
    "  )\n",
    "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原来LLaMa的词表是32000，所以Embedding层的大小为(32000, 4096)，即词表里的每一个token对应一个1*4096的Embedding向量，上篇文章扩充后的词表大小为40114，所以这里有两种方式：\n",
    "- 随机扩充\n",
    "- 均值扩充（用的比较多）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 随机填充\n",
    "\n",
    "将扩充的token对应的向量随机初始化，实现方式如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取原先的embedding\n",
    "embeddings = model.get_input_embeddings()\n",
    "print(embeddings)\n",
    "# Embedding(32000, 4096, padding_idx=0)\n",
    "\n",
    "print(embeddings(torch.LongTensor([31999])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resize_token_embeddings(41013)\n",
    "new_embeddings = model.get_input_embeddings()\n",
    "print(new_embeddings)\n",
    "# Embedding(41013, 4096)\n",
    "\n",
    "print(new_embeddings(torch.LongTensor([31999])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述可以看到，Embedding层从32000扩展为了40114，而且前32000个token的Embedding是没有发生变化的，只有新增的token是随机初始化的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 均值扩充\n",
    "\n",
    "新增token的Embedding用原来token的Embedding的均值来表示，比如比如“你好”在原来的词表里为“你”：[-0.0396, -0.0217, -0.0092, ..., -0.0032, -0.0103, 0.0068]；“好”：[-0.0104, -0.0145, -0.0142, ..., -0.0048, 0.0042, -0.0014]，则新增的“你好”则为其均值：[-0.0250, -0.0181, -0.0117, ..., -0.0040, -0.0030, 0.0027]，以此方式扩充："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新增的token和在原来token相对应的字典\n",
    "token_mapping = {}\n",
    "for i in range(32000, len(new_tokenizer)):\n",
    "    # 使用 tokenizer 的 convert_ids_to_tokens 方法将索引转换为对应的 token\n",
    "    token = new_tokenizer.convert_ids_to_tokens(i)\n",
    "    # 原来的token\n",
    "    input_ids = tokenizer(token, return_tensors=\"pt\").input_ids[0]\n",
    "    if input_ids[1] == 29871:\n",
    "        new_input_ids = input_ids[2:]\n",
    "    else:\n",
    "        new_input_ids = input_ids[1:]        \n",
    "    token_mapping[i] = new_input_ids\n",
    "\n",
    "# 原始输入embedding\n",
    "embeddings = model.get_input_embeddings()\n",
    "# 新完全初始化的embedding\n",
    "new_vocab_size = len(new_tokenizer)\n",
    "embedding_dim = 4096\n",
    "new_embedding = torch.nn.Embedding(new_vocab_size, embedding_dim)\n",
    "\n",
    "# 将现有Embedding层的权重赋值给新的Embedding层的前32000行\n",
    "num_to_copy = min(new_vocab_size, len(embeddings.weight))\n",
    "new_embedding.weight.data[:num_to_copy, :] = embeddings.weight.data[:num_to_copy, :]\n",
    "\n",
    "# 开始新增\n",
    "for new_token, original_tokens in token_mapping.items():\n",
    "    original_embeddings = embeddings(original_tokens)\n",
    "    mean_embedding = torch.mean(original_embeddings, dim=0)\n",
    "    new_embedding.weight.data[new_token] = mean_embedding\n",
    "\n",
    "# 更换嵌入层\n",
    "model.set_input_embeddings(new_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同理模型的最后一层lm_head: Linear(in_features=4096, out_features=32000, bias=False)，参数也是一个32000*4096的矩阵，方法和上述是一致的，我们来看看均值扩充的方式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 32000\n",
    "new_output_size = 41013\n",
    "lm_head = model.lm_head\n",
    "# 新的lm_head\n",
    "new_lm_head = torch.nn.Linear(in_features=4096, out_features=new_output_size, bias=False)\n",
    "# 前32000个向量不变\n",
    "new_lm_head.weight.data[:output_size, :] = lm_head.weight.data[:output_size, :]\n",
    "\n",
    "# 新增\n",
    "for new_token, original_tokens in token_mapping.items():\n",
    "    original = 0\n",
    "    for i in original_tokens:\n",
    "        original += lm_head.weight.data[i]\n",
    "    mean_para = original / len(original_tokens)\n",
    "    new_lm_head.weight.data[new_token] = mean_para\n",
    "\n",
    "# 替换模型原来的lm_head\n",
    "model.lm_head = new_lm_head\n",
    "model.config.vocab_size = 41013\n",
    "\n",
    "# 最后完成了embedding和lm_head替换后，保存模型\n",
    "model.save_pretrained(\"llama-2-7b-extent\", max_shard_size=\"8GB\")\n",
    "# 保存为bin格式\n",
    "#model.save_pretrained(\"llama-2-7b-extent\", max_shard_size=\"8GB\", safe_serialization=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述就是扩充词表后，需要对模型的embedding和lm_head做的操作，目前业界一般用的都是均值的方式。我们可以算一下新增了多少个参数：（41013-32000）* 4096 * 2 = 73,834,496，7千多万个参数，还只是扩充9千个词，如果扩充的词表数量达到5万左右，那新增参数就是1亿多，这个参数数量还是不少的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "初始化了之后，还得预训练，也得更新这个embedding矩阵，性价比比较高的是把训练词表的语料训练一遍，然后做SFT"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
