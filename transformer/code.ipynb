{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在现在的大模型时代，核心还得是Transformer，Transformer是由谷歌于2017年提出，最初用于机器翻译的神经网络模型，在此衍生出了一系列的模型，BERT、T5、GPT、GLM、BLOOM、LLaMa等等从小模型到大模型都少不了他。Transformer最经典的模型结构图还得是论文中原汁原味的图\n",
    "\n",
    "![](https://pic2.zhimg.com/80/v2-99d78d31305c01d06f81c86af3b42b15_1440w.webp)\n",
    "\n",
    "关于这张图的讲解在其他的回答中有很多，简单总结下，左边是编码器(Encoder)，右边是解码器(Decoder)，N× 表示进行了 N 次堆叠。主要可分成以下几个模块：\n",
    "\n",
    "- 嵌入表示层(图中粉红Input Embedding和白色小圈Positional Encoding部分)；\n",
    "- 注意力层(图中淡黄Attention部分)；\n",
    "- 前馈层(图中淡蓝Feed Forward部分)；\n",
    "- 残差连接和层归一化(图中黄色Add&Norm部分)；\n",
    "\n",
    "接下来分别对每个模块进行简述以及代码实现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 嵌入表示层\n",
    "\n",
    "主要由两部分组成即Embedding层和位置编码层。先看下Embedding层的实现，Embedding层的主要作用就是将文本序列通过分词器（tokenizer）分出的token，转换为其相对应的向量表示。假设：\"我喜欢\"，分为\"我|喜欢\"，对应的token索引是[13,156]，然后通过Embedding层转换成对应的向量，有一篇专门讲tokenization的文章详见：https://www.zhihu.com/question/64984731/answer/3183726323\n",
    "\n",
    "下面是Embedding层的代码实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "\n",
    "class TokenEmbedding(nn.Embedding):\n",
    "    \"\"\"\n",
    "    使用torch.nn的Embedding模块\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        \"\"\"\n",
    "        TokenEmbedding类\n",
    "\n",
    "        :param vocab_size: 词汇表的大小\n",
    "        :param d_model: 模型的维度\n",
    "        :padding的索引为1，即token索引为1时，Embedding补0\n",
    "        \"\"\"\n",
    "        super(TokenEmbedding, self).__init__(vocab_size, d_model, padding_idx=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一下参数，假设词汇表大小为1000，模型维度是512，则Embedding层的参数如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模块中的参数数量为: 512000\n"
     ]
    }
   ],
   "source": [
    "tok_emb = TokenEmbedding(1000, 512)\n",
    "num_params = sum(p.numel() for p in tok_emb.parameters())\n",
    "print(\"模块中的参数数量为:\", num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下能否够正常使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res: tensor([[[-0.3398,  0.6714,  0.5205,  ...,  2.0852, -0.2745, -3.4193],\n",
      "         [ 0.9891,  1.1674, -0.0548,  ..., -0.1365, -0.6469, -0.1807],\n",
      "         [ 0.4388, -0.3126, -0.8564,  ..., -1.1363, -2.1417,  1.0565]],\n",
      "\n",
      "        [[ 1.8572,  0.5508,  1.3253,  ..., -0.1493, -0.3859, -0.9570],\n",
      "         [ 0.4711,  0.1042,  1.3682,  ..., -1.3991, -0.1269, -0.9717],\n",
      "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "res.shape: torch.Size([2, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "# x是batch_size为2, seq_len为3，索引为1的会被padding为0\n",
    "x = torch.LongTensor([[6, 5, 4], [3, 2, 1]])\n",
    "res = tok_emb(x)\n",
    "print(\"res:\", res)\n",
    "print(\"res.shape:\", res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来看位置编码层，如果只有Embedding层是不能让模型知道token之间的相对位置关系的，所以在进入编码器建模其上下文语义之前，编码其相对位置信息是一个很重要的操作，Transformer使用的是正余弦函数来编码其位置信息，目前大模型时代使用最多的是苏神的旋转位置编码（RoPE），在此不多赘述了。\n",
    "\n",
    "![](https://pic3.zhimg.com/80/v2-4eaf534112eb133c2f17ac17bd66a076_1440w.webp)\n",
    "\n",
    "上图中pos 表示单词所在的位置，2i 和 2i + 1 表示位置编码向量中的对应维度，d 则对应位置编码的总维度。这种位置编码有两个优点：1.第 pos + n 个位置的编码是第 pos 个位置的编码的线性组合，包含了相对位置信息；2.正余弦函数的范围是在 [-1,+1]，相加后的总嵌入改变不会太大。下面是位置编码层代码实现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    计算正余弦位置编码。\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, max_len):\n",
    "        \"\"\"\n",
    "        正余弦位置编码类\n",
    "\n",
    "        :param d_model: 模型的维度\n",
    "        :param max_len: 最大序列长度\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # 初始化位置编码矩阵\n",
    "        self.encoding = torch.zeros(max_len, d_model)\n",
    "        self.encoding.requires_grad = False  # 不需要计算梯度\n",
    "\n",
    "        pos = torch.arange(0, max_len)\n",
    "        pos = pos.float().unsqueeze(dim=1)\n",
    "\n",
    "        # 'i'表示d_model的索引（例如，嵌入大小=50，'i' = [0,50]）\n",
    "        # “step=2”表示将'i'乘以二（与2 * i相同）\n",
    "        _2i = torch.arange(0, d_model, step=2).float()\n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # self.encoding\n",
    "        # [max_len = 512, d_model = 512]\n",
    "\n",
    "        batch_size, seq_len = x.size()\n",
    "        # [batch_size = 8, seq_len = 30]\n",
    "\n",
    "        return self.encoding[:seq_len, :]\n",
    "        # [seq_len = 30, d_model = 512]\n",
    "        # 将与 tok_emb 相加：[8, 30, 512]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "看一下参数，假设最大序列长度为512，模型维度也是512，则位置编码层的参数如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模块中的参数数量为: 0\n"
     ]
    }
   ],
   "source": [
    "pe = PositionalEncoding(512,512)\n",
    "num_params = sum(p.numel() for p in pe.parameters())\n",
    "print(\"模块中的参数数量为:\", num_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "会发现结果结果打印出来是0，因为这个参数矩阵不需要训练，直接生成的，大小是max_len\\*d_model = 512\\*512的一个矩阵。看一下使用，如果对这个位置编码有疑惑可以将函数中每一步的中间结果打印出来更容易理解："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res: tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  ...,  1.0000e+00,\n",
      "          0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  8.2186e-01,  ...,  1.0000e+00,\n",
      "          1.0366e-04,  1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  9.3641e-01,  ...,  1.0000e+00,\n",
      "          2.0733e-04,  1.0000e+00]])\n",
      "res.shape: torch.Size([3, 512])\n"
     ]
    }
   ],
   "source": [
    "# x是batch_size为2, seq_len为3\n",
    "x = torch.LongTensor([[6, 5, 4], [3, 2, 1]])\n",
    "res = pe.forward(x)\n",
    "print(\"res:\", res)\n",
    "# 返回的形状是[seq_len = 3, d_model = 512]\n",
    "print(\"res.shape:\", res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "把Embedding层和位置编码层集成在一起即嵌入表示层："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    token embedding + positional encoding\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, max_len, drop_prob):\n",
    "        \"\"\"\n",
    "        包含Embedding和位置编码的类\n",
    "\n",
    "        :param vocab_size: 词汇表大小\n",
    "        :param d_model: 模型的维度\n",
    "        :param max_len: 最大序列长度\n",
    "        :param drop_prob: dropout 正则化概率，防止过拟合\n",
    "        \"\"\"\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, d_model)\n",
    "        self.pos_emb = PositionalEncoding(d_model, max_len)\n",
    "        self.drop_out = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        tok_emb = self.tok_emb(x)\n",
    "        pos_emb = self.pos_emb(x)       \n",
    "        return self.drop_out(tok_emb + pos_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个类的参数量还是Embedding层的参数量，看一下使用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "res: tensor([[[-1.7675e+00,  6.2947e-01,  0.0000e+00,  ...,  1.7656e+00,\n",
      "          -3.7920e-02,  1.2085e+00],\n",
      "         [ 3.6604e-01,  1.2264e-01, -7.2386e-02,  ...,  0.0000e+00,\n",
      "          -1.1619e+00, -1.3416e+00],\n",
      "         [ 5.6828e-01, -5.5072e-01,  2.2809e+00,  ...,  5.6782e-01,\n",
      "           9.9775e-01,  3.7725e+00]],\n",
      "\n",
      "        [[ 5.6805e-01, -1.3158e-01, -4.7994e-01,  ...,  1.7758e+00,\n",
      "           2.3991e-01, -9.5818e-02],\n",
      "         [ 9.1609e-01,  2.7782e+00, -6.7694e-02,  ..., -1.1187e+00,\n",
      "          -7.0974e-01,  2.0479e+00],\n",
      "         [ 1.0103e+00, -4.6239e-01,  1.0405e+00,  ...,  1.1111e+00,\n",
      "           2.3036e-04,  1.1111e+00]]], grad_fn=<MulBackward0>)\n",
      "res.shape: torch.Size([2, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "te = TransformerEmbedding(1000,512,512,0.1)\n",
    "# x是batch_size为2, seq_len为3\n",
    "x = torch.LongTensor([[6, 5, 4], [3, 2, 1]])\n",
    "res = te.forward(x)\n",
    "print(\"res:\", res)\n",
    "# 返回的形状是[batch_size = 2, seq_len = 3, d_model = 512]\n",
    "print(\"res.shape:\", res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的结果需要注意一下：我们输入的维度是[batch_size = 2, seq_len = 3]，Embedding层返回的结果的维度是[batch_size = 2, seq_len = 3, d_model = 512]，位置编码层返回的结果的维度是[seq_len = 3, d_model = 512]。因为我们只需要前三个的位置编码信息，所以最后tok_emb + pos_emb维度是不一样的，但为什么可以相加呢？因为触发了torch的广播机制，两条数据其实每一条都加上了位置编码的矩阵，感兴趣的可以试一下。至此嵌入表示模块的代码已经完成了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 注意力层\n",
    "\n",
    "注意力机制在NLP领域其实很早就提出了，一句通俗的话来理解就是“你正在做什么，就将注意力集中在这上面”，关于Transformer的多头自注意力机制最重要就是要理解Q，K，V这三个向量的含义，但要把三个向量讲明白，是需要长篇大论的，有些回答的图解讲的不错，这里就从代码的角度来简述一下。\n",
    "\n",
    "假设文本：\"我喜欢\"，分为\"我|喜欢\"，经过Embedding层和位置编码后变成嵌入向量形状是：[batch_size = 1, seq_len = 2, d_model = 512]，这时候Q矩阵就是这个嵌入向量\\*W_q参数矩阵，同理K矩阵是嵌入向量\\*W_k参数矩阵，V矩阵是嵌入向量\\*W_v参数矩阵。\n",
    "\n",
    "这三个参数矩阵W_q、W_k、W_v都是可以训练的，经过计算后Q，K，V的形状还是[batch_size = 1, seq_len = 2, d_model = 512]，然后带入到下面的公式中进行计算。这个注意力公式我觉得核心的理解就是“**一个矩阵乘以它自己的转置再乘以它自身**”，这样一通算下来还不够关注自己吗，这就是注意力机制的精髓。\n",
    "\n",
    "[](https://pic1.zhimg.com/80/v2-f926f359e8e1e035a3c9345d75b63890_1440w.webp)\n",
    "\n",
    "但注意上面是单头的情况，那什么是多头（Multi-Head）呢，就是把模型的维度d_model=512进行拆分成多分，假设头数为2，那么会把原来的Q，K，V都会拆成两份，但是只拆维度，两个Q的形状都是[batch_size = 1, seq_len = 2, d_model = 256]，但第一个Q取模型维度的前256的值，K和V同理。然后分别带入下面公式计算注意力。为什么使用多头呢？一个简单解释：在不同的子空间中分别计算并得到不同的上下文相关的表示，通俗来讲在多个角度来关注自己。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaleDotProductAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    计算单个点积注意力\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ScaleDotProductAttention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None, e=1e-12):\n",
    "        # 输入是一个4维的张量\n",
    "        # [batch_size, head, length, d_tensor]\n",
    "        batch_size, head, length, d_tensor = k.size()\n",
    "\n",
    "        # 1.用Key的转置与Query计算点积\n",
    "        k_t = k.transpose(2, 3)  # transpose\n",
    "        score = (q @ k_t) / math.sqrt(d_tensor)  # scaled dot product\n",
    "\n",
    "        # 2.进行掩码，encoder不需要进行掩码，decoder需要进行掩码\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask == 0, -10000)  \n",
    "\n",
    "        # 3.通过softmax使分数范围在[0, 1]之间\n",
    "        score = self.softmax(score)\n",
    "\n",
    "        # 4.再与Value相乘        \n",
    "        v = score @ v\n",
    "\n",
    "        return v, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面是单独实现论文中计算注意力机制的代码，下面是整个多头注意力实现的代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.attention = ScaleDotProductAttention()\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_concat = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # 1.点积相应的矩阵\n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
    "\n",
    "        # 2.根据头数进行维度拆分\n",
    "        q, k, v = self.split(q), self.split(k), self.split(v)\n",
    "\n",
    "        # 3.进行计算\n",
    "        out, attention = self.attention(q, k, v, mask=mask)\n",
    "\n",
    "        # 4.把拆分的多头再拼起来\n",
    "        out = self.concat(out)\n",
    "        out = self.w_concat(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def split(self, tensor):\n",
    "        \"\"\"\n",
    "        根据头数进行维度拆分\n",
    "\n",
    "        :param tensor: [batch_size, length, d_model]\n",
    "        :return: [batch_size, head, length, d_tensor]\n",
    "        \"\"\"\n",
    "        batch_size, length, d_model = tensor.size()\n",
    "\n",
    "        d_tensor = d_model // self.n_head\n",
    "        tensor = tensor.view(batch_size, length, self.n_head, d_tensor).transpose(1, 2)\n",
    "\n",
    "        return tensor\n",
    "\n",
    "    def concat(self, tensor):\n",
    "        \"\"\"\n",
    "        把拆分的多头再拼起来\n",
    "\n",
    "        :param tensor: [batch_size, head, length, d_tensor]\n",
    "        :return: [batch_size, length, d_model]\n",
    "        \"\"\"\n",
    "        batch_size, head, length, d_tensor = tensor.size()\n",
    "        d_model = head * d_tensor\n",
    "\n",
    "        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 前馈层\n",
    "\n",
    "接受上面注意力模块的输出作为输入，其实简单点来说就是一个带有Relu激活函数的两层全连接层，进行非线性变化，公式如下：\n",
    "\n",
    "![](https://pic1.zhimg.com/80/v2-b289a6748d91ae1f5c125ead95fc6880_1440w.webp)\n",
    "\n",
    "代码实现也是比较简单的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里需要注意的是，这两个全连接层的维度，是要比模型的维度大的，为什么要大呢？其实在大模型时代的lora和这个也是异曲同工之妙，我觉得原因很朴实，因为参数更多，学得更多。原文中模型维度为512，全连接层维度是2048。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 残差连接和层归一化\n",
    "\n",
    "残差连接就是把未经过某层的原矩阵加上经过该层的矩阵，等于最终所需矩阵，最早是用在视觉领域，主要的效果是避免网络过深导致梯度消失，是由恺明大神在ResNet论文中提出的，简单公式描述就是：$x_{i+1}$ = F($x_i$) + $x_i$，$x_i$为原矩阵，$x_{i+1}$为最终所需矩阵。\n",
    "\n",
    "层归一化简单来说就是计算数据的均值和方差，然后使其平移缩放到标准分布中，主要的效果是**避免训练时数值不稳定，收敛速度慢**。\n",
    "\n",
    "![](https://pic2.zhimg.com/80/v2-0e76168dd965c4cac4d5b8977bd83d41_1440w.webp)\n",
    "\n",
    "公式中 μ 和 σ 分别表示均值和方差，α 和 b 是可学习的参数。实现的代码也比较简单："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-12):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.alpha = nn.Parameter(torch.ones(d_model))\n",
    "        self.bias = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, unbiased=False, keepdim=True)\n",
    "\n",
    "        out = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        out = self.alpha * out + self.bias\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder\n",
    "\n",
    "上面是各个模块实现的代码，接下来把各个模块拼起来，先拼单独的Encoder层，就是架构图左侧的部分，建议翻到前面对着图看会更清晰："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm1 = LayerNorm(d_model=d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = LayerNorm(d_model=d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        # 1.计算注意力\n",
    "        _x = x\n",
    "        x = self.attention(q=x, k=x, v=x, mask=src_mask)\n",
    "\n",
    "        # 2.残差连接和层归一化\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + _x)\n",
    "\n",
    "        # 3.前馈层\n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "\n",
    "        # 4.最后一次残差连接和层归一化\n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x + _x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是整个Encoder部分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, enc_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob):\n",
    "        super().__init__()\n",
    "        # Embedding部分\n",
    "        self.emb = TransformerEmbedding(d_model=d_model,\n",
    "                                        max_len=max_len,\n",
    "                                        vocab_size=enc_voc_size,\n",
    "                                        drop_prob=drop_prob)\n",
    "\n",
    "        # Encoder部分\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model=d_model,\n",
    "                                                  ffn_hidden=ffn_hidden,\n",
    "                                                  n_head=n_head,\n",
    "                                                  drop_prob=drop_prob)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        x = self.emb(x)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, src_mask)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder\n",
    "\n",
    "以上就是Encoder部分，现在开始拼Decoder层，看架构图右侧的部分，这里需要注意几点，\n",
    "\n",
    "第一点是：Decoder部分是两个注意力块，而且这两个还不一样，第一个注意力块是需要mask进行掩码的，而上面Encoder是不需要的，简单的解释就是：Transformer最初设计适用于机器翻译的，Encoder主要用于编码源语言序列的信息，而这个序列是属于已知的，只需要考虑如何融合上下文语义信息；而Decoder是负责生成目标语言序列的，这一生成过程是自回归的，即对于每一个token的生成过程，仅有当前token之前的目标语言序列是可以被看到的， 因此这一额外增加的掩码是用来掩盖后续的文本信息，以防模型在训练阶段直接看到后续的文本序列进而无法得到有效地训练。GPT系列主要用的这个思想，所以才有了现在的大模型时代。\n",
    "\n",
    "第二点是：Decoder部分的第二个注意力块，输入的Q是上一步的输出，K和V是Encoder部分的输出，也就是这里把Encoder和Decoder连接了起来也可以叫做交叉注意力，看代码会更清晰明了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm1 = LayerNorm(d_model=d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.enc_dec_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm2 = LayerNorm(d_model=d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm3 = LayerNorm(d_model=d_model)\n",
    "        self.dropout3 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, dec, enc, trg_mask, src_mask):    \n",
    "        # 1.对应上面说的第一点\n",
    "        _x = dec\n",
    "        x = self.self_attention(q=dec, k=dec, v=dec, mask=trg_mask)\n",
    "\n",
    "        # 2.残差连接和层归一化\n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + _x)\n",
    "\n",
    "        if enc is not None:\n",
    "            # 3.对应上面说的第二点\n",
    "            _x = x\n",
    "            x = self.enc_dec_attention(q=x, k=enc, v=enc, mask=src_mask)\n",
    "\n",
    "            # 4.残差连接和层归一化\n",
    "            x = self.dropout2(x)\n",
    "            x = self.norm2(x + _x)\n",
    "\n",
    "        # 5.前馈层\n",
    "        _x = x\n",
    "        x = self.ffn(x)\n",
    "\n",
    "        # 6.残差连接和层归一化\n",
    "        x = self.dropout3(x)\n",
    "        x = self.norm3(x + _x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是整个Decoder部分："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob):\n",
    "        super().__init__()\n",
    "        self.emb = TransformerEmbedding(d_model=d_model,\n",
    "                                        drop_prob=drop_prob,\n",
    "                                        max_len=max_len,\n",
    "                                        vocab_size=dec_voc_size)\n",
    "\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model=d_model,\n",
    "                                                  ffn_hidden=ffn_hidden,\n",
    "                                                  n_head=n_head,\n",
    "                                                  drop_prob=drop_prob)\n",
    "                                     for _ in range(n_layers)])\n",
    "\n",
    "        self.linear = nn.Linear(d_model, dec_voc_size)\n",
    "\n",
    "    def forward(self, trg, src, trg_mask, src_mask):\n",
    "        trg = self.emb(trg)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            trg = layer(trg, src, trg_mask, src_mask)\n",
    "\n",
    "        # 最后经过一个全连接层\n",
    "        output = self.linear(trg)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transformer\n",
    "\n",
    "接下来构建整个Transformer的结构，对着结构图实现起来就比较简单了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_pad_idx, trg_pad_idx, trg_sos_idx, enc_voc_size, dec_voc_size, d_model, n_head, max_len,\n",
    "                 ffn_hidden, n_layers, drop_prob):\n",
    "        super().__init__()\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.trg_sos_idx = trg_sos_idx\n",
    "        \n",
    "        self.encoder = Encoder(d_model=d_model,\n",
    "                               n_head=n_head,\n",
    "                               max_len=max_len,\n",
    "                               ffn_hidden=ffn_hidden,\n",
    "                               enc_voc_size=enc_voc_size,\n",
    "                               drop_prob=drop_prob,\n",
    "                               n_layers=n_layers)\n",
    "\n",
    "        self.decoder = Decoder(d_model=d_model,\n",
    "                               n_head=n_head,\n",
    "                               max_len=max_len,\n",
    "                               ffn_hidden=ffn_hidden,\n",
    "                               dec_voc_size=dec_voc_size,\n",
    "                               drop_prob=drop_prob,\n",
    "                               n_layers=n_layers)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        output = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
    "        return output\n",
    "\n",
    "    def make_src_mask(self, src):\n",
    "        \"\"\"\n",
    "        创建源序列（src）的掩码, 将pad补零的位置设为False\n",
    "        \"\"\"\n",
    "        src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        return src_mask\n",
    "\n",
    "    def make_trg_mask(self, trg):\n",
    "        \"\"\"\n",
    "        创建目标序列（trg）的掩码, 1.pad补零的位置设为False；\n",
    "        2.创建一个下三角矩阵，这个矩阵的对角线及以下的为为True，其余位置为False\n",
    "        表示在训练时模型只能依赖于当前和过去的信息，不能依赖未来的信息\n",
    "        \"\"\"\n",
    "        trg_pad_mask = (trg != self.trg_pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_sub_mask = torch.tril(torch.ones(trg_len, trg_len)).type(torch.ByteTensor).to(self.device)\n",
    "        trg_mask = trg_pad_mask & trg_sub_mask\n",
    "        return trg_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上就是整个Transformer的结构，初始化一下看看模型的结构和参数，模型参数就参照原文了："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 86,147,496 trainable parameters\n",
      "Transformer(\n",
      "  (encoder): Encoder(\n",
      "    (emb): TransformerEmbedding(\n",
      "      (tok_emb): TokenEmbedding(32000, 512, padding_idx=1)\n",
      "      (pos_emb): PositionalEncoding()\n",
      "      (drop_out): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x EncoderLayer(\n",
      "        (attention): MultiHeadAttention(\n",
      "          (attention): ScaleDotProductAttention(\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm()\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (relu): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (norm2): LayerNorm()\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (emb): TransformerEmbedding(\n",
      "      (tok_emb): TokenEmbedding(25000, 512, padding_idx=1)\n",
      "      (pos_emb): PositionalEncoding()\n",
      "      (drop_out): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x DecoderLayer(\n",
      "        (self_attention): MultiHeadAttention(\n",
      "          (attention): ScaleDotProductAttention(\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm1): LayerNorm()\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (enc_dec_attention): MultiHeadAttention(\n",
      "          (attention): ScaleDotProductAttention(\n",
      "            (softmax): Softmax(dim=-1)\n",
      "          )\n",
      "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_concat): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (norm2): LayerNorm()\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (ffn): PositionwiseFeedForward(\n",
      "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (relu): ReLU()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (norm3): LayerNorm()\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (linear): Linear(in_features=512, out_features=25000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 原文base模型参数\n",
    "max_len = 256\n",
    "d_model = 512\n",
    "n_layers = 6\n",
    "n_heads = 8\n",
    "ffn_hidden = 2048\n",
    "drop_prob = 0.1\n",
    "\n",
    "# 分词，词表的一些参数，和通过数据集训练的tokenizer是相关的，这里就简单给一下\n",
    "# 原文使用的数据集是WMT14 EN-DE，enc_voc_size为32000，dec_voc_size为25000，\n",
    "# 这个训练出的词表数值不一样，最终模型的参数也不一样，因为这两个参数会影响Embedding层的参数\n",
    "src_pad_idx = 1\n",
    "trg_pad_idx = 1\n",
    "trg_sos_idx = 2\n",
    "enc_voc_size = 32000\n",
    "dec_voc_size = 25000\n",
    "\n",
    "model = Transformer(src_pad_idx=src_pad_idx,\n",
    "                    trg_pad_idx=trg_pad_idx,\n",
    "                    trg_sos_idx=trg_sos_idx,\n",
    "                    d_model=d_model,\n",
    "                    enc_voc_size=enc_voc_size,\n",
    "                    dec_voc_size=dec_voc_size,\n",
    "                    max_len=max_len,\n",
    "                    ffn_hidden=ffn_hidden,\n",
    "                    n_head=n_heads,\n",
    "                    n_layers=n_layers,\n",
    "                    drop_prob=drop_prob)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# 模型的参数\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')\n",
    "\n",
    "# 模型的结构\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面就是整个Transformer的实践，想要开始训练还缺一个数据集去训练tokenizer和词表。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
