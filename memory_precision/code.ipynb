{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关于不同存储精度问题可以查看[《LLM大模型之精度问题（FP16，FP32，BF16）详解与实践》](https://zhuanlan.zhihu.com/p/657886517) 。这里我们主要研究两个问题：\n",
    "1. 模型不同精度下显存占用情况；\n",
    "2. 模型不同精度之间如何转换；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以显卡NVIDIA 4050 6G，模型用Qwen2-0.5B为例，这个模型的保存的精度通过查看模型文件的congfig.json可以看到是\"torch_dtype\": \"float16\"。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers version: 4.41.0\n",
      "torch version: 2.1.1+cu118\n",
      "可用 GPU 数量: 1\n",
      "\n",
      "GPU 0 的详细信息:\n",
      "名称: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "计算能力: 8.9\n",
      "内存总量 (GB): 6.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# 打印版本号\n",
    "print(\"transformers version:\", transformers.__version__)\n",
    "print(\"torch version:\", torch.__version__)\n",
    "\n",
    "# 检查系统中是否有可用的 GPU\n",
    "if torch.cuda.is_available():\n",
    "    # 获取可用的 GPU 设备数量\n",
    "    num_devices = torch.cuda.device_count()\n",
    "    print(\"可用 GPU 数量:\", num_devices)\n",
    "\n",
    "    # 遍历所有可用的 GPU 设备并打印详细信息\n",
    "    for i in range(num_devices):\n",
    "        device = torch.cuda.get_device_properties(i)\n",
    "        print(f\"\\nGPU {i} 的详细信息:\")\n",
    "        print(\"名称:\", device.name)\n",
    "        print(\"计算能力:\", f\"{device.major}.{device.minor}\")\n",
    "        print(\"内存总量 (GB):\", round(device.total_memory / (1024**3), 1))\n",
    "else:\n",
    "    print(\"没有可用的 GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in the model: 494032768\n"
     ]
    }
   ],
   "source": [
    "# 加载模型\n",
    "model_name = \"Qwen2-0.5B-Instruct\" # 你模型存放的位置\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda:0\", torch_dtype=torch.float16)\n",
    "# 打印参数\n",
    "total_parameters = model.num_parameters()\n",
    "print(\"Total parameters in the model:\", total_parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以float16进行加载，也就是每个参数16个bit，2个byte，计算一下这么多参数应该占多少显存："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total memory occupied by the model in MB: 0.9202077388763428\n"
     ]
    }
   ],
   "source": [
    "# 计算每个参数的大小（以字节为单位）\n",
    "size_per_parameter_bytes = 2\n",
    "# 计算模型在显存中的总空间（以字节为单位）\n",
    "total_memory_bytes = total_parameters * size_per_parameter_bytes\n",
    "# 将字节转换为更常见的单位（GB）\n",
    "total_memory_gb = total_memory_bytes / (1024**3)\n",
    "\n",
    "print(\"Total memory occupied by the model in MB:\", total_memory_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着看一下，现在的显存占用："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory allocated by the model in GB: 1.115407943725586\n"
     ]
    }
   ],
   "source": [
    "# 计算模型的显存占用\n",
    "memory_allocated = torch.cuda.memory_allocated(device='cuda:0')\n",
    "# 将字节转换为更常见的单位（GB）\n",
    "memory_allocated_gb = memory_allocated / (1024**3)\n",
    "\n",
    "print(\"Memory allocated by the model in GB:\", memory_allocated_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到这数值是接近一致的（稍微差一点的原因：框架torch以及transformers本身、GPU的本身缓存等等）。以上是在float16下加载的，同理看一下在float32下加载的情况："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters in the model: 494032768\n",
      "Total memory occupied by the model in MB: 0.9202077388763428\n",
      "Memory allocated by the model in GB: 2.2383761405944824\n"
     ]
    }
   ],
   "source": [
    "# 加载模型float32\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda:0\", torch_dtype=torch.float32)\n",
    "\n",
    "# 打印参数\n",
    "total_parameters = model.num_parameters()\n",
    "print(\"Total parameters in the model:\", total_parameters)\n",
    "\n",
    "# 计算每个参数的大小（以字节为单位）\n",
    "size_per_parameter_bytes = 2\n",
    "# 计算模型在显存中的总空间（以字节为单位）\n",
    "total_memory_bytes = total_parameters * size_per_parameter_bytes\n",
    "# 将字节转换为更常见的单位（GB）\n",
    "total_memory_gb = total_memory_bytes / (1024**3)\n",
    "print(\"Total memory occupied by the model in MB:\", total_memory_gb)\n",
    "\n",
    "# 计算模型的显存占用\n",
    "memory_allocated = torch.cuda.memory_allocated(device='cuda:0')\n",
    "# 将字节转换为更常见的单位（GB）\n",
    "memory_allocated_gb = memory_allocated / (1024**3)\n",
    "print(\"Memory allocated by the model in GB:\", memory_allocated_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：https://zhuanlan.zhihu.com/p/658343628"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
